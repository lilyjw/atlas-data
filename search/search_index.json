{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Overview MITRE ATLAS\u2122 (Adversarial Threat Landscape for Artificial-Intelligence Systems), is a knowledge base of adversary tactics, techniques, and case studies for machine learning (ML) systems based on real-world observations, demonstrations from ML red teams and security groups, and the state of the possible from academic research. ATLAS is modeled after the MITRE ATT&CK\u00ae framework and its tactics and techniques are complementary to those in ATT&CK. ATLAS enables researchers to navigate the landscape of threats to machine learning systems . ML is increasingly used across a variety of industries. There are a growing number of vulnerabilities in ML, and its use increases the attack surface of existing systems. We developed ATLAS to raise awareness of these threats and present them in a way familiar to security researchers. Guide The ATLAS framework is detailed in tactics and techniques , which represent adversary goals and the means to achieve them. Visit case studies for select attacks on ML systems and view ATLAS tactics and techniques in context. See the table below for an overview of ATLAS tactics, then click through to read more or browse using the navigation menu. Tactic Summary Reconnaissance The adversary is trying to gather information they can use to plan future operations. Resource Development The adversary is trying to establish resources they can use to support operations. Initial Access The adversary is trying to gain access to the system containing machine learning artifacts. ML Model Access An adversary is attempting to gain some level of access to a machine learning model. Execution The adversary is trying to run malicious code. Persistence The adversary is trying to maintain their foothold. Defense Evasion The adversary is trying to avoid being detected by security software. Discovery The adversary is trying to figure out your environment. Collection The adversary is trying to gather ML artifacts and other related information relevant to their goal. ML Attack Staging An adversary is leveraging their knowledge of and access to the target system to tailor the attack. Exfiltration The adversary is trying to steal machine learning artifacts. Impact The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.","title":"Overview"},{"location":"index.html#overview","text":"MITRE ATLAS\u2122 (Adversarial Threat Landscape for Artificial-Intelligence Systems), is a knowledge base of adversary tactics, techniques, and case studies for machine learning (ML) systems based on real-world observations, demonstrations from ML red teams and security groups, and the state of the possible from academic research. ATLAS is modeled after the MITRE ATT&CK\u00ae framework and its tactics and techniques are complementary to those in ATT&CK. ATLAS enables researchers to navigate the landscape of threats to machine learning systems . ML is increasingly used across a variety of industries. There are a growing number of vulnerabilities in ML, and its use increases the attack surface of existing systems. We developed ATLAS to raise awareness of these threats and present them in a way familiar to security researchers.","title":"Overview"},{"location":"index.html#guide","text":"The ATLAS framework is detailed in tactics and techniques , which represent adversary goals and the means to achieve them. Visit case studies for select attacks on ML systems and view ATLAS tactics and techniques in context. See the table below for an overview of ATLAS tactics, then click through to read more or browse using the navigation menu. Tactic Summary Reconnaissance The adversary is trying to gather information they can use to plan future operations. Resource Development The adversary is trying to establish resources they can use to support operations. Initial Access The adversary is trying to gain access to the system containing machine learning artifacts. ML Model Access An adversary is attempting to gain some level of access to a machine learning model. Execution The adversary is trying to run malicious code. Persistence The adversary is trying to maintain their foothold. Defense Evasion The adversary is trying to avoid being detected by security software. Discovery The adversary is trying to figure out your environment. Collection The adversary is trying to gather ML artifacts and other related information relevant to their goal. ML Attack Staging An adversary is leveraging their knowledge of and access to the target system to tailor the attack. Exfiltration The adversary is trying to steal machine learning artifacts. Impact The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data.","title":"Guide"},{"location":"faq.html","text":"FAQ General What is a technique? Techniques describe the means by which adversaries achieve tactical goals. They represent \u201chow\u201d an adversary achieves a tactical objective by performing an action. For example, an adversary may gain initial access by compromising the machine learning (ML) supply chain. Techniques may also represent \u201cwhat\u201d an adversary gains by performing an action. This is a useful distinction for the ML Attack Staging tactic, where the adversary is typically creating or modifying an ML artifact that will be used in a subsequent tactical objective. There can be multiple techniques in each tactic category as there are many ways to achieve tactical objectives. What is a tactic? Tactics are tactical adversary goals during an attack. They represent the \u201cwhy\u201d of a technique: the reason for performing an action. Tactics serve as useful contextual categories for individual techniques and cover standard notations for things adversaries do during an operation. MITRE ATLAS\u2122 tactics represent new adversary goals particular to machine learning systems, as well as tactics adapted from the MITRE ATT&CK\u00ae Enterprise Matrix. In those cases, ATT&CK tactic definitions are stretched to include ML concepts. What is the MITRE ATLAS Navigator? The MITRE ATLAS\u2122 version of the ATT&CK Navigator displays ATLAS techniques and allows users to create and view complex visualizations. In addition to the matrix, the Navigator also shows a frequency heat map of techniques used in ATLAS case studies. Case Study Terminology What is an incident date? Time period of when the event/incident occured What is an actor? The individual or group that performed this operation What is a reporter? The group that identified and reported this incident What is a target? The victim system or organization targeted by the Actor What is a case study type? Whether this case study describes a real-world incident or a demonstration under a realistic threat model and representative Target system","title":"FAQ"},{"location":"faq.html#faq","text":"","title":"FAQ"},{"location":"faq.html#general","text":"","title":"General"},{"location":"faq.html#what-is-a-technique","text":"Techniques describe the means by which adversaries achieve tactical goals. They represent \u201chow\u201d an adversary achieves a tactical objective by performing an action. For example, an adversary may gain initial access by compromising the machine learning (ML) supply chain. Techniques may also represent \u201cwhat\u201d an adversary gains by performing an action. This is a useful distinction for the ML Attack Staging tactic, where the adversary is typically creating or modifying an ML artifact that will be used in a subsequent tactical objective. There can be multiple techniques in each tactic category as there are many ways to achieve tactical objectives.","title":"What is a technique?"},{"location":"faq.html#what-is-a-tactic","text":"Tactics are tactical adversary goals during an attack. They represent the \u201cwhy\u201d of a technique: the reason for performing an action. Tactics serve as useful contextual categories for individual techniques and cover standard notations for things adversaries do during an operation. MITRE ATLAS\u2122 tactics represent new adversary goals particular to machine learning systems, as well as tactics adapted from the MITRE ATT&CK\u00ae Enterprise Matrix. In those cases, ATT&CK tactic definitions are stretched to include ML concepts.","title":"What is a tactic?"},{"location":"faq.html#what-is-the-mitre-atlas-navigator","text":"The MITRE ATLAS\u2122 version of the ATT&CK Navigator displays ATLAS techniques and allows users to create and view complex visualizations. In addition to the matrix, the Navigator also shows a frequency heat map of techniques used in ATLAS case studies.","title":"What is the MITRE ATLAS Navigator?"},{"location":"faq.html#case-study-terminology","text":"","title":"Case Study Terminology"},{"location":"faq.html#what-is-an-incident-date","text":"Time period of when the event/incident occured","title":"What is an incident date?"},{"location":"faq.html#what-is-an-actor","text":"The individual or group that performed this operation","title":"What is an actor?"},{"location":"faq.html#what-is-a-reporter","text":"The group that identified and reported this incident","title":"What is a reporter?"},{"location":"faq.html#what-is-a-target","text":"The victim system or organization targeted by the Actor","title":"What is a target?"},{"location":"faq.html#what-is-a-case-study-type","text":"Whether this case study describes a real-world incident or a demonstration under a realistic threat model and representative Target system","title":"What is a case study type?"},{"location":"case-studies/index.html","text":"Case Studies Attacks on machine learning (ML) systems are being developed and released with increased regularity. Attacks have historically been performed in controlled settings, but attacks are increasingly observed on production systems. Deployed ML systems can have many vulnerabilities, for example trained on personally identifiable information, trusted to make critical decisions with little oversight, and have little to no logging and alerting attached to their use. MITRE ATLAS\u2122 case studies are selected because of the impact to production ML systems. Each demonstrates one of the following characteristics: Range of Attacks: Evasion, poisoning, model replication and exploiting traditional software flaws. Range of Personas: Average user, security researchers, ML researchers and fully-equipped Red team. Range of ML Paradigms: Attacks on MLaaS, ML models hosted on cloud, hosted on-premise, ML models on edge. Range of Use Case: Attacks on ML systems used in both \"security-sensitive\" applications like cybersecurity and non-security-sensitive applications like chatbots. The table below lists case-studies from MITRE ATLAS\u2122. Scroll through the table or use the sidebar to access more information. ID Name Summary AML.CS0000 Evasion of Deep Learning Detector for Malware C&C Traffic The Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic. Based on the publicly available paper by Le et al. , we built a model that was trained on a similar dataset as our production model and had similar performance. Then we crafted adversarial samples, queried the model, and adjusted the adversarial sample accordingly until the model was evaded. AML.CS0001 Botnet Domain Generation Algorithm (DGA) Detection Evasion The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network based botnet Domain Generation Algorithm (DGA) detector using a generic domain name mutation technique. It is a generic domain mutation technique which can evade most ML-based DGA detection modules. The generic mutation technique evades most ML-based DGA detection modules DGA and can be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before they is deployed to the production environment. AML.CS0002 VirusTotal Poisoning McAfee Advanced Threat Research noticed an increase in reports of a certain ransomware family that was out of the ordinary. Case investigation revealed that many samples of that particular ransomware family were submitted through a popular virus-sharing platform within a short amount of time. Further investigation revealed that based on string similarity the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar. Interestingly enough, the compile time was the same for all the samples. After more digging, researchers discovered that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants. The variants would not always be executable, but are still classified as the same ransomware family. AML.CS0003 Bypassing Cylance's AI Malware Detection Researchers at Skylight were able to create a universal bypass string that evades detection by Cylance's AI Malware detector when appended to a malicious file. AML.CS0004 Camera Hijack Attack on Facial Recognition System This type of camera hijack attack can evade the traditional live facial recognition authentication model and enable access to privileged systems and victim impersonation. Two individuals in China used this attack to gain access to the local government's tax system. They created a fake shell company and sent invoices via tax system to supposed clients. The individuals started this scheme in 2018 and were able to fraudulently collect $77 million. AML.CS0005 Attack on Machine Translation Service - Google Translate, Bing Translator, and Systran Translate Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs. A research group at UC Berkeley utilized these public endpoints to create a replicated model with near-production state-of-the-art translation quality. Beyond demonstrating that IP can be functionally stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services. These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites. AML.CS0006 ClearviewAI Misconfiguration Clearview AI makes a facial recognition tool that searches publicly available photos for matches. This tool has been used for investigative purposes by law enforcement agencies and other parties. Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account. This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens. With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model. These kinds of attacks illustrate that any attempt to secure ML system should be on top of \"traditional\" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing. AML.CS0007 GPT-2 Model Replication OpenAI built GPT-2, a language model capable of generating high quality text samples. Over concerns that GPT-2 could be used for malicious purposes such as impersonating others, or generating misleading news articles, fake social media content, or spam, OpenAI adopted a tiered release schedule. They initially released a smaller, less powerful version of GPT-2 along with a technical description of the approach, but held back the full trained model. Before the full model was released by OpenAI, researchers at Brown University successfully replicated the model using information released by OpenAI and open source ML artifacts. This demonstrates that a bad actor with sufficient technical skill and compute resources could have replicated GPT-2 and used it for harmful goals before the AI Security community is prepared. AML.CS0008 ProofPoint Evasion Proof Pudding (CVE-2019-20634) is a code repository that describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to bypass the live system. More specifically, the insights allowed researchers to craft malicious emails that received preferable scores, going undetected by the system. Each word in an email is scored numerically based on multiple variables and if the overall score of the email is too low, ProofPoint will output an error, labeling it as SPAM. AML.CS0009 Tay Poisoning Microsoft created Tay, a Twitter chatbot designed to engage and entertain users. While previous chatbots used pre-programmed scripts to respond to prompts, Tay's machine learning capabilities allowed it to be directly influenced by its conversations. A coordinated attack encouraged malicious users to tweet abusive and offensive language at Tay, which eventually led to Tay generating similarly inflammatory content towards other users. Microsoft decommissioned Tay within 24 hours of its launch and issued a public apology with lessons learned from the bot's failure. AML.CS0010 Microsoft Azure Service Disruption The Microsoft AI Red Team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding valid account, and exfiltrating data -- all interleaved with adversarial ML specific steps such as offline and online evasion examples. AML.CS0011 Microsoft Edge AI Evasion The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge. This exercise was meant to use a automated system to continuously manipulate a target image to cause the ML model to produce misclassifications. AML.CS0012 Face Identification System Evasion via Physical Countermeasures MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks. AML.CS0013 Backdoor Attack on Deep Learning Models in Mobile Apps Deep learning models are increasingly used in mobile applications as critical components. Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via \"neural payload injection.\" They conducted an empirical study on real-world mobile deep learning apps collected from Google Play. They identified 54 apps that were vulnerable to attack, including popular security and safety critical applications used for cash recognition, parental control, face authentication, and financial services. AML.CS0014 Confusing Antimalware Neural Networks Cloud storage and computations have become popular platforms for deploying ML malware detectors. In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers. The Kaspersky ML research team explored this gray-box scenario and showed that feature knowledge is enough for an adversarial attack on ML models. They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files. AML.CS0015 Tesla Auto Wiper and Enhanced Autopilot Attack Tesla Auto Wipers and Enhanced Autopilot driving mode both make use of computer vision machine learning models to determine the vehicle's corresponding functions. These functions can be exploited by physical adversarial machine learning attacks that affect the operation and the safety of the vehicle. While exploits to gain root access to the Tesla firmware had since been patched, the vulnerabilities to the underlying machine learning systems discovered by this research were still exploitable.","title":"Case Studies"},{"location":"case-studies/index.html#case-studies","text":"Attacks on machine learning (ML) systems are being developed and released with increased regularity. Attacks have historically been performed in controlled settings, but attacks are increasingly observed on production systems. Deployed ML systems can have many vulnerabilities, for example trained on personally identifiable information, trusted to make critical decisions with little oversight, and have little to no logging and alerting attached to their use. MITRE ATLAS\u2122 case studies are selected because of the impact to production ML systems. Each demonstrates one of the following characteristics: Range of Attacks: Evasion, poisoning, model replication and exploiting traditional software flaws. Range of Personas: Average user, security researchers, ML researchers and fully-equipped Red team. Range of ML Paradigms: Attacks on MLaaS, ML models hosted on cloud, hosted on-premise, ML models on edge. Range of Use Case: Attacks on ML systems used in both \"security-sensitive\" applications like cybersecurity and non-security-sensitive applications like chatbots. The table below lists case-studies from MITRE ATLAS\u2122. Scroll through the table or use the sidebar to access more information. ID Name Summary AML.CS0000 Evasion of Deep Learning Detector for Malware C&C Traffic The Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic. Based on the publicly available paper by Le et al. , we built a model that was trained on a similar dataset as our production model and had similar performance. Then we crafted adversarial samples, queried the model, and adjusted the adversarial sample accordingly until the model was evaded. AML.CS0001 Botnet Domain Generation Algorithm (DGA) Detection Evasion The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network based botnet Domain Generation Algorithm (DGA) detector using a generic domain name mutation technique. It is a generic domain mutation technique which can evade most ML-based DGA detection modules. The generic mutation technique evades most ML-based DGA detection modules DGA and can be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before they is deployed to the production environment. AML.CS0002 VirusTotal Poisoning McAfee Advanced Threat Research noticed an increase in reports of a certain ransomware family that was out of the ordinary. Case investigation revealed that many samples of that particular ransomware family were submitted through a popular virus-sharing platform within a short amount of time. Further investigation revealed that based on string similarity the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar. Interestingly enough, the compile time was the same for all the samples. After more digging, researchers discovered that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants. The variants would not always be executable, but are still classified as the same ransomware family. AML.CS0003 Bypassing Cylance's AI Malware Detection Researchers at Skylight were able to create a universal bypass string that evades detection by Cylance's AI Malware detector when appended to a malicious file. AML.CS0004 Camera Hijack Attack on Facial Recognition System This type of camera hijack attack can evade the traditional live facial recognition authentication model and enable access to privileged systems and victim impersonation. Two individuals in China used this attack to gain access to the local government's tax system. They created a fake shell company and sent invoices via tax system to supposed clients. The individuals started this scheme in 2018 and were able to fraudulently collect $77 million. AML.CS0005 Attack on Machine Translation Service - Google Translate, Bing Translator, and Systran Translate Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs. A research group at UC Berkeley utilized these public endpoints to create a replicated model with near-production state-of-the-art translation quality. Beyond demonstrating that IP can be functionally stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services. These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites. AML.CS0006 ClearviewAI Misconfiguration Clearview AI makes a facial recognition tool that searches publicly available photos for matches. This tool has been used for investigative purposes by law enforcement agencies and other parties. Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account. This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens. With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model. These kinds of attacks illustrate that any attempt to secure ML system should be on top of \"traditional\" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing. AML.CS0007 GPT-2 Model Replication OpenAI built GPT-2, a language model capable of generating high quality text samples. Over concerns that GPT-2 could be used for malicious purposes such as impersonating others, or generating misleading news articles, fake social media content, or spam, OpenAI adopted a tiered release schedule. They initially released a smaller, less powerful version of GPT-2 along with a technical description of the approach, but held back the full trained model. Before the full model was released by OpenAI, researchers at Brown University successfully replicated the model using information released by OpenAI and open source ML artifacts. This demonstrates that a bad actor with sufficient technical skill and compute resources could have replicated GPT-2 and used it for harmful goals before the AI Security community is prepared. AML.CS0008 ProofPoint Evasion Proof Pudding (CVE-2019-20634) is a code repository that describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to bypass the live system. More specifically, the insights allowed researchers to craft malicious emails that received preferable scores, going undetected by the system. Each word in an email is scored numerically based on multiple variables and if the overall score of the email is too low, ProofPoint will output an error, labeling it as SPAM. AML.CS0009 Tay Poisoning Microsoft created Tay, a Twitter chatbot designed to engage and entertain users. While previous chatbots used pre-programmed scripts to respond to prompts, Tay's machine learning capabilities allowed it to be directly influenced by its conversations. A coordinated attack encouraged malicious users to tweet abusive and offensive language at Tay, which eventually led to Tay generating similarly inflammatory content towards other users. Microsoft decommissioned Tay within 24 hours of its launch and issued a public apology with lessons learned from the bot's failure. AML.CS0010 Microsoft Azure Service Disruption The Microsoft AI Red Team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding valid account, and exfiltrating data -- all interleaved with adversarial ML specific steps such as offline and online evasion examples. AML.CS0011 Microsoft Edge AI Evasion The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge. This exercise was meant to use a automated system to continuously manipulate a target image to cause the ML model to produce misclassifications. AML.CS0012 Face Identification System Evasion via Physical Countermeasures MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks. AML.CS0013 Backdoor Attack on Deep Learning Models in Mobile Apps Deep learning models are increasingly used in mobile applications as critical components. Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via \"neural payload injection.\" They conducted an empirical study on real-world mobile deep learning apps collected from Google Play. They identified 54 apps that were vulnerable to attack, including popular security and safety critical applications used for cash recognition, parental control, face authentication, and financial services. AML.CS0014 Confusing Antimalware Neural Networks Cloud storage and computations have become popular platforms for deploying ML malware detectors. In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers. The Kaspersky ML research team explored this gray-box scenario and showed that feature knowledge is enough for an adversarial attack on ML models. They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files. AML.CS0015 Tesla Auto Wiper and Enhanced Autopilot Attack Tesla Auto Wipers and Enhanced Autopilot driving mode both make use of computer vision machine learning models to determine the vehicle's corresponding functions. These functions can be exploited by physical adversarial machine learning attacks that affect the operation and the safety of the vehicle. While exploits to gain root access to the Tesla firmware had since been patched, the vulnerabilities to the underlying machine learning systems discovered by this research were still exploitable.","title":"Case Studies"},{"location":"case-studies/AML.CS0000.html","text":"Evasion of Deep Learning Detector for Malware C&C Traffic Exercise Incident date: 2020 Actor: Palo Alto Networks AI Research Team | Target: Palo Alto Networks malware detection system Summary The Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic. Based on the publicly available paper by Le et al. , we built a model that was trained on a similar dataset as our production model and had similar performance. Then we crafted adversarial samples, queried the model, and adjusted the adversarial sample accordingly until the model was evaded. Procedure # Technique Description 1 Search for Victim's Publicly Available Research Materials: Pre-Print Repositories We identified a machine learning based approach to malicious URL detection as a representative approach and potential target from the paper URLNet: Learning a URL representation with deep learning for malicious URL detection , which was found on arXiv (a pre-print repository). 2 Acquire Public ML Artifacts: Datasets We acquired a command and control HTTP traffic dataset consisting of approximately 33 million benign and 27 million malicious HTTP packet headers. 3 Create Proxy ML Model We trained a model on the HTTP traffic dataset to use as a proxy for the target model. Evaluation showed a true positive rate of ~ 99% and false positive rate of ~ 0.01%, on average. Testing the model with a HTTP packet header from known malware command and control traffic samples was detected as malicious with high confidence (> 99%). 4 Craft Adversarial Data: Manual Modification We crafted evasion samples by removing fields from packet header which are typically not used for C&C communication (e.g. cache-control, connection, etc.). 5 Verify Attack We queried the model with our adversarial examples and adjusted them until the model was evaded. 6 Evade ML Model With the crafted samples, we performed online evasion of the ML-based spyware detection model. The crafted packets were identified as benign with > 80% confidence. This evaluation demonstrates that adversaries are able to bypass advanced ML detection techniques, by crafting samples that are misclassified by an ML model. References Le, Hung, et al. \"URLNet: Learning a URL representation with deep learning for malicious URL detection.\" arXiv preprint arXiv:1802.03162 (2018).","title":"Evasion of Deep Learning Detector for Malware C&C Traffic"},{"location":"case-studies/AML.CS0000.html#evasion-of-deep-learning-detector-for-malware-cc-traffic","text":"Exercise Incident date: 2020 Actor: Palo Alto Networks AI Research Team | Target: Palo Alto Networks malware detection system","title":"Evasion of Deep Learning Detector for Malware C&amp;C Traffic"},{"location":"case-studies/AML.CS0000.html#summary","text":"The Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic. Based on the publicly available paper by Le et al. , we built a model that was trained on a similar dataset as our production model and had similar performance. Then we crafted adversarial samples, queried the model, and adjusted the adversarial sample accordingly until the model was evaded.","title":"Summary"},{"location":"case-studies/AML.CS0000.html#procedure","text":"# Technique Description 1 Search for Victim's Publicly Available Research Materials: Pre-Print Repositories We identified a machine learning based approach to malicious URL detection as a representative approach and potential target from the paper URLNet: Learning a URL representation with deep learning for malicious URL detection , which was found on arXiv (a pre-print repository). 2 Acquire Public ML Artifacts: Datasets We acquired a command and control HTTP traffic dataset consisting of approximately 33 million benign and 27 million malicious HTTP packet headers. 3 Create Proxy ML Model We trained a model on the HTTP traffic dataset to use as a proxy for the target model. Evaluation showed a true positive rate of ~ 99% and false positive rate of ~ 0.01%, on average. Testing the model with a HTTP packet header from known malware command and control traffic samples was detected as malicious with high confidence (> 99%). 4 Craft Adversarial Data: Manual Modification We crafted evasion samples by removing fields from packet header which are typically not used for C&C communication (e.g. cache-control, connection, etc.). 5 Verify Attack We queried the model with our adversarial examples and adjusted them until the model was evaded. 6 Evade ML Model With the crafted samples, we performed online evasion of the ML-based spyware detection model. The crafted packets were identified as benign with > 80% confidence. This evaluation demonstrates that adversaries are able to bypass advanced ML detection techniques, by crafting samples that are misclassified by an ML model.","title":"Procedure"},{"location":"case-studies/AML.CS0000.html#references","text":"Le, Hung, et al. \"URLNet: Learning a URL representation with deep learning for malicious URL detection.\" arXiv preprint arXiv:1802.03162 (2018).","title":"References"},{"location":"case-studies/AML.CS0001.html","text":"Botnet Domain Generation Algorithm (DGA) Detection Evasion Exercise Incident date: 2020 Actor: Palo Alto Networks AI Research Team | Target: Palo Alto Networks ML-based DGA detection module Summary The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network based botnet Domain Generation Algorithm (DGA) detector using a generic domain name mutation technique. It is a generic domain mutation technique which can evade most ML-based DGA detection modules. The generic mutation technique evades most ML-based DGA detection modules DGA and can be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before they is deployed to the production environment. Procedure # Technique Description 1 Search for Victim's Publicly Available Research Materials DGA detection is a widely used technique to detect botnets in academia and industry. The research team searched for research papers related to DGA detection. 2 Acquire Public ML Artifacts The researchers acquired a publicly available CNN-based DGA detection model and tested it against a well-known DGA generated domain name data sets, which includes ~50 million domain names from 64 botnet DGA families. The CNN-based DGA detection model shows more than 70% detection accuracy on 16 (~25%) botnet DGA families. 3 Develop Adversarial ML Attack Capabilities The researchers developed a generic mutation technique that requires a minimal number of iterations. 4 Craft Adversarial Data: Black-Box Optimization The researchers used the mutation technique to generate evasive domain names. 5 Verify Attack The experiment results show that the detection rate of all 16 botnet DGA families drop to less than 25% after only one string is inserted once to the DGA generated domain names. 6 Evade ML Model The DGA generated domain names mutated with this technique successfully evade the target DGA Detection model, allowing an adversary to continue communication with their Command and Control servers. References Yu, Bin, Jie Pan, Jiaming Hu, Anderson Nascimento, and Martine De Cock. \"Character level based detection of DGA domain names.\" In 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2018. Degas source code","title":"Botnet Domain Generation Algorithm (DGA) Detection Evasion"},{"location":"case-studies/AML.CS0001.html#botnet-domain-generation-algorithm-dga-detection-evasion","text":"Exercise Incident date: 2020 Actor: Palo Alto Networks AI Research Team | Target: Palo Alto Networks ML-based DGA detection module","title":"Botnet Domain Generation Algorithm (DGA) Detection Evasion"},{"location":"case-studies/AML.CS0001.html#summary","text":"The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network based botnet Domain Generation Algorithm (DGA) detector using a generic domain name mutation technique. It is a generic domain mutation technique which can evade most ML-based DGA detection modules. The generic mutation technique evades most ML-based DGA detection modules DGA and can be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before they is deployed to the production environment.","title":"Summary"},{"location":"case-studies/AML.CS0001.html#procedure","text":"# Technique Description 1 Search for Victim's Publicly Available Research Materials DGA detection is a widely used technique to detect botnets in academia and industry. The research team searched for research papers related to DGA detection. 2 Acquire Public ML Artifacts The researchers acquired a publicly available CNN-based DGA detection model and tested it against a well-known DGA generated domain name data sets, which includes ~50 million domain names from 64 botnet DGA families. The CNN-based DGA detection model shows more than 70% detection accuracy on 16 (~25%) botnet DGA families. 3 Develop Adversarial ML Attack Capabilities The researchers developed a generic mutation technique that requires a minimal number of iterations. 4 Craft Adversarial Data: Black-Box Optimization The researchers used the mutation technique to generate evasive domain names. 5 Verify Attack The experiment results show that the detection rate of all 16 botnet DGA families drop to less than 25% after only one string is inserted once to the DGA generated domain names. 6 Evade ML Model The DGA generated domain names mutated with this technique successfully evade the target DGA Detection model, allowing an adversary to continue communication with their Command and Control servers.","title":"Procedure"},{"location":"case-studies/AML.CS0001.html#references","text":"Yu, Bin, Jie Pan, Jiaming Hu, Anderson Nascimento, and Martine De Cock. \"Character level based detection of DGA domain names.\" In 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2018. Degas source code","title":"References"},{"location":"case-studies/AML.CS0002.html","text":"VirusTotal Poisoning Incident Incident date: 2020 | Reporter: McAfee Advanced Threat Research Actor: Unknown | Target: VirusTotal Summary McAfee Advanced Threat Research noticed an increase in reports of a certain ransomware family that was out of the ordinary. Case investigation revealed that many samples of that particular ransomware family were submitted through a popular virus-sharing platform within a short amount of time. Further investigation revealed that based on string similarity the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar. Interestingly enough, the compile time was the same for all the samples. After more digging, researchers discovered that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants. The variants would not always be executable, but are still classified as the same ransomware family. Procedure # Technique Description 1 Obtain Capabilities: Adversarial ML Attack Implementations The actor obtained metame , a simple metamorphic code engine for arbitrary executables. 2 Craft Adversarial Data The actor used a malware sample from a prevalent ransomware family as a start to create \"mutant\" variants. 3 ML Supply Chain Compromise: Data The actor uploaded \"mutant\" samples to the platform. 4 Poison Training Data Several vendors started to classify the files as the ransomware family even though most of them won't run. The \"mutant\" samples poisoned the dataset the ML model(s) use to identify and classify this ransomware family.","title":"VirusTotal Poisoning"},{"location":"case-studies/AML.CS0002.html#virustotal-poisoning","text":"Incident Incident date: 2020 | Reporter: McAfee Advanced Threat Research Actor: Unknown | Target: VirusTotal","title":"VirusTotal Poisoning"},{"location":"case-studies/AML.CS0002.html#summary","text":"McAfee Advanced Threat Research noticed an increase in reports of a certain ransomware family that was out of the ordinary. Case investigation revealed that many samples of that particular ransomware family were submitted through a popular virus-sharing platform within a short amount of time. Further investigation revealed that based on string similarity the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar. Interestingly enough, the compile time was the same for all the samples. After more digging, researchers discovered that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants. The variants would not always be executable, but are still classified as the same ransomware family.","title":"Summary"},{"location":"case-studies/AML.CS0002.html#procedure","text":"# Technique Description 1 Obtain Capabilities: Adversarial ML Attack Implementations The actor obtained metame , a simple metamorphic code engine for arbitrary executables. 2 Craft Adversarial Data The actor used a malware sample from a prevalent ransomware family as a start to create \"mutant\" variants. 3 ML Supply Chain Compromise: Data The actor uploaded \"mutant\" samples to the platform. 4 Poison Training Data Several vendors started to classify the files as the ransomware family even though most of them won't run. The \"mutant\" samples poisoned the dataset the ML model(s) use to identify and classify this ransomware family.","title":"Procedure"},{"location":"case-studies/AML.CS0003.html","text":"Bypassing Cylance's AI Malware Detection Exercise Incident date: September 07, 2019 Actor: Skylight Cyber | Target: CylancePROTECT, Cylance Smart Antivirus Summary Researchers at Skylight were able to create a universal bypass string that evades detection by Cylance's AI Malware detector when appended to a malicious file. Procedure # Technique Description 1 Search for Victim's Publicly Available Research Materials The researchers read publicly available information about Cylance's AI Malware detector. They gathered this information from various sources such as public talks as well as patent submissions by Cylance. 2 ML-Enabled Product or Service The researchers used Cylance's AI Malware detector and enabled verbose logging to understand the inner workings of the ML model, particularly around reputation scoring and model ensembling. 3 Develop Adversarial ML Attack Capabilities The researchers used the reputation scoring information to reverse engineer which attributes provided what level of positive or negative reputation. Along the way, they discovered a secondary model which was an override for the first model. Positive assessments from the second model overrode the decision of the core ML model. 4 Craft Adversarial Data: Manual Modification Using this knowledge, the researchers fused attributes of known good files with malware to manually create adversarial malware. 5 Evade ML Model Due to the secondary model overriding the primary, the researchers were effectively able to bypass the ML model. References Skylight Cyber Blog Post, \"Cylance, I Kill You!\" Statement's from Skylight Cyber CEO","title":"Bypassing Cylance's AI Malware Detection"},{"location":"case-studies/AML.CS0003.html#bypassing-cylances-ai-malware-detection","text":"Exercise Incident date: September 07, 2019 Actor: Skylight Cyber | Target: CylancePROTECT, Cylance Smart Antivirus","title":"Bypassing Cylance's AI Malware Detection"},{"location":"case-studies/AML.CS0003.html#summary","text":"Researchers at Skylight were able to create a universal bypass string that evades detection by Cylance's AI Malware detector when appended to a malicious file.","title":"Summary"},{"location":"case-studies/AML.CS0003.html#procedure","text":"# Technique Description 1 Search for Victim's Publicly Available Research Materials The researchers read publicly available information about Cylance's AI Malware detector. They gathered this information from various sources such as public talks as well as patent submissions by Cylance. 2 ML-Enabled Product or Service The researchers used Cylance's AI Malware detector and enabled verbose logging to understand the inner workings of the ML model, particularly around reputation scoring and model ensembling. 3 Develop Adversarial ML Attack Capabilities The researchers used the reputation scoring information to reverse engineer which attributes provided what level of positive or negative reputation. Along the way, they discovered a secondary model which was an override for the first model. Positive assessments from the second model overrode the decision of the core ML model. 4 Craft Adversarial Data: Manual Modification Using this knowledge, the researchers fused attributes of known good files with malware to manually create adversarial malware. 5 Evade ML Model Due to the secondary model overriding the primary, the researchers were effectively able to bypass the ML model.","title":"Procedure"},{"location":"case-studies/AML.CS0003.html#references","text":"Skylight Cyber Blog Post, \"Cylance, I Kill You!\" Statement's from Skylight Cyber CEO","title":"References"},{"location":"case-studies/AML.CS0004.html","text":"Camera Hijack Attack on Facial Recognition System Incident Incident date: 2020 | Reporter: Ant Group AISEC Team Actor: Two individuals | Target: Shanghai government tax office's facial recognition service Summary This type of camera hijack attack can evade the traditional live facial recognition authentication model and enable access to privileged systems and victim impersonation. Two individuals in China used this attack to gain access to the local government's tax system. They created a fake shell company and sent invoices via tax system to supposed clients. The individuals started this scheme in 2018 and were able to fraudulently collect $77 million. Procedure # Technique Description 1 Acquire Infrastructure: Consumer Hardware The attackers bought customized low-end mobile phones. 2 Obtain Capabilities: Software Tools The attackers obtained customized Android ROMs and a virtual camera application. 3 Obtain Capabilities: Adversarial ML Attack Implementations The attackers obtained software that turns static photos into videos, adding realistic effects such as blinking eyes. 4 Establish Accounts The attackers collected user identity information and high definition face photos from an online black market and used the victim's information to register accounts. 5 ML-Enabled Product or Service The attackers used the virtual camera app to present the generated video to the ML-based facial recognition service used for user verification. 6 Evade ML Model The attackers successfully evaded the face recognition system. This allowed the attackers to impersonate the victim and verify their their identity in the tax system. References Faces are the next target for fraudsters","title":"Camera Hijack Attack on Facial Recognition System"},{"location":"case-studies/AML.CS0004.html#camera-hijack-attack-on-facial-recognition-system","text":"Incident Incident date: 2020 | Reporter: Ant Group AISEC Team Actor: Two individuals | Target: Shanghai government tax office's facial recognition service","title":"Camera Hijack Attack on Facial Recognition System"},{"location":"case-studies/AML.CS0004.html#summary","text":"This type of camera hijack attack can evade the traditional live facial recognition authentication model and enable access to privileged systems and victim impersonation. Two individuals in China used this attack to gain access to the local government's tax system. They created a fake shell company and sent invoices via tax system to supposed clients. The individuals started this scheme in 2018 and were able to fraudulently collect $77 million.","title":"Summary"},{"location":"case-studies/AML.CS0004.html#procedure","text":"# Technique Description 1 Acquire Infrastructure: Consumer Hardware The attackers bought customized low-end mobile phones. 2 Obtain Capabilities: Software Tools The attackers obtained customized Android ROMs and a virtual camera application. 3 Obtain Capabilities: Adversarial ML Attack Implementations The attackers obtained software that turns static photos into videos, adding realistic effects such as blinking eyes. 4 Establish Accounts The attackers collected user identity information and high definition face photos from an online black market and used the victim's information to register accounts. 5 ML-Enabled Product or Service The attackers used the virtual camera app to present the generated video to the ML-based facial recognition service used for user verification. 6 Evade ML Model The attackers successfully evaded the face recognition system. This allowed the attackers to impersonate the victim and verify their their identity in the tax system.","title":"Procedure"},{"location":"case-studies/AML.CS0004.html#references","text":"Faces are the next target for fraudsters","title":"References"},{"location":"case-studies/AML.CS0005.html","text":"Attack on Machine Translation Service - Google Translate, Bing Translator, and Systran Translate Exercise Incident date: April 30, 2020 Actor: Berkeley Artificial Intelligence Research | Target: Google Translate, Bing Translator, Systran Translate Summary Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs. A research group at UC Berkeley utilized these public endpoints to create a replicated model with near-production state-of-the-art translation quality. Beyond demonstrating that IP can be functionally stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services. These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites. Procedure # Technique Description 1 Search for Victim's Publicly Available Research Materials The researchers used published research papers to identify the datasets and model architectures used by the target translation services. 2 Acquire Public ML Artifacts: Datasets The researchers gathered similar datasets that the target translation services used. 3 Acquire Public ML Artifacts: Models The researchers gathered similar model architectures that the target translation services used. 4 ML Model Inference API Access They abused a public facing application to query the model and produced machine translated sentence pairs as training data. 5 Create Proxy ML Model: Train Proxy via Replication Using these translated sentence pairs, the researchers trained a model that replicates the behavior of the target model. 6 ML Intellectual Property Theft By replicating the model with high fidelity, the researchers demonstrated that an adversary could steal a model and violate the victim's intellectual property rights. 7 Craft Adversarial Data: Black-Box Transfer The replicated models were used to generate adversarial examples that successfully transferred to the black-box translation services. 8 Evade ML Model The adversarial examples were used to evade the machine translation services by a variety of means. This included targeted word flips, vulgar outputs, and dropped sentences. 9 Erode ML Model Integrity Adversarial attacks can cause errors that cause reputational damage to the company of the translation service and decrease user trust in AI-powered services. References Wallace, Eric, et al. \"Imitation Attacks and Defenses for Black-box Machine Translation Systems\" EMNLP 2020 Project Page, \"Imitation Attacks and Defenses for Black-box Machine Translation Systems\" Google under fire for mistranslating Chinese amid Hong Kong protests","title":"Attack on Machine Translation Service - Google Translate, Bing Translator, and Systran Translate"},{"location":"case-studies/AML.CS0005.html#attack-on-machine-translation-service-google-translate-bing-translator-and-systran-translate","text":"Exercise Incident date: April 30, 2020 Actor: Berkeley Artificial Intelligence Research | Target: Google Translate, Bing Translator, Systran Translate","title":"Attack on Machine Translation Service - Google Translate, Bing Translator, and Systran Translate"},{"location":"case-studies/AML.CS0005.html#summary","text":"Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs. A research group at UC Berkeley utilized these public endpoints to create a replicated model with near-production state-of-the-art translation quality. Beyond demonstrating that IP can be functionally stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services. These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.","title":"Summary"},{"location":"case-studies/AML.CS0005.html#procedure","text":"# Technique Description 1 Search for Victim's Publicly Available Research Materials The researchers used published research papers to identify the datasets and model architectures used by the target translation services. 2 Acquire Public ML Artifacts: Datasets The researchers gathered similar datasets that the target translation services used. 3 Acquire Public ML Artifacts: Models The researchers gathered similar model architectures that the target translation services used. 4 ML Model Inference API Access They abused a public facing application to query the model and produced machine translated sentence pairs as training data. 5 Create Proxy ML Model: Train Proxy via Replication Using these translated sentence pairs, the researchers trained a model that replicates the behavior of the target model. 6 ML Intellectual Property Theft By replicating the model with high fidelity, the researchers demonstrated that an adversary could steal a model and violate the victim's intellectual property rights. 7 Craft Adversarial Data: Black-Box Transfer The replicated models were used to generate adversarial examples that successfully transferred to the black-box translation services. 8 Evade ML Model The adversarial examples were used to evade the machine translation services by a variety of means. This included targeted word flips, vulgar outputs, and dropped sentences. 9 Erode ML Model Integrity Adversarial attacks can cause errors that cause reputational damage to the company of the translation service and decrease user trust in AI-powered services.","title":"Procedure"},{"location":"case-studies/AML.CS0005.html#references","text":"Wallace, Eric, et al. \"Imitation Attacks and Defenses for Black-box Machine Translation Systems\" EMNLP 2020 Project Page, \"Imitation Attacks and Defenses for Black-box Machine Translation Systems\" Google under fire for mistranslating Chinese amid Hong Kong protests","title":"References"},{"location":"case-studies/AML.CS0006.html","text":"ClearviewAI Misconfiguration Incident Incident date: April 2020 Actor: Researchers at spiderSilk | Target: Clearview AI facial recognition tool Summary Clearview AI makes a facial recognition tool that searches publicly available photos for matches. This tool has been used for investigative purposes by law enforcement agencies and other parties. Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account. This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens. With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model. These kinds of attacks illustrate that any attempt to secure ML system should be on top of \"traditional\" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing. Procedure # Technique Description 1 Establish Accounts A security researcher gained initial access to Clearview AI's private code repository via a misconfigured server setting that allowed an arbitrary user to register a valid account. 2 Data from Information Repositories The private code repository contained credentials which were used to access AWS S3 cloud storage buckets, leading to the discovery of assets for the facial recognition tool, including: - Released desktop and mobile applications - Pre-release applications featuring new capabilities - Slack access tokens - Raw videos and other data 3 Acquire Public ML Artifacts Adversaries could have downloaded training data and gleaned details about software, models, and capabilities from the source code and decompiled application binaries. 4 Erode ML Model Integrity As a result, future application releases could have been compromised, causing degraded or malicious facial recognition capabilities. References TechCrunch Article, \"Security lapse exposed Clearview AI source code\" Gizmodo Article, \"We Found Clearview AI's Shady Face Recognition App\" New York Times Article, \"The Secretive Company That Might End Privacy as We Know It\"","title":"ClearviewAI Misconfiguration"},{"location":"case-studies/AML.CS0006.html#clearviewai-misconfiguration","text":"Incident Incident date: April 2020 Actor: Researchers at spiderSilk | Target: Clearview AI facial recognition tool","title":"ClearviewAI Misconfiguration"},{"location":"case-studies/AML.CS0006.html#summary","text":"Clearview AI makes a facial recognition tool that searches publicly available photos for matches. This tool has been used for investigative purposes by law enforcement agencies and other parties. Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account. This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens. With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model. These kinds of attacks illustrate that any attempt to secure ML system should be on top of \"traditional\" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing.","title":"Summary"},{"location":"case-studies/AML.CS0006.html#procedure","text":"# Technique Description 1 Establish Accounts A security researcher gained initial access to Clearview AI's private code repository via a misconfigured server setting that allowed an arbitrary user to register a valid account. 2 Data from Information Repositories The private code repository contained credentials which were used to access AWS S3 cloud storage buckets, leading to the discovery of assets for the facial recognition tool, including: - Released desktop and mobile applications - Pre-release applications featuring new capabilities - Slack access tokens - Raw videos and other data 3 Acquire Public ML Artifacts Adversaries could have downloaded training data and gleaned details about software, models, and capabilities from the source code and decompiled application binaries. 4 Erode ML Model Integrity As a result, future application releases could have been compromised, causing degraded or malicious facial recognition capabilities.","title":"Procedure"},{"location":"case-studies/AML.CS0006.html#references","text":"TechCrunch Article, \"Security lapse exposed Clearview AI source code\" Gizmodo Article, \"We Found Clearview AI's Shady Face Recognition App\" New York Times Article, \"The Secretive Company That Might End Privacy as We Know It\"","title":"References"},{"location":"case-studies/AML.CS0007.html","text":"GPT-2 Model Replication Exercise Incident date: August 22, 2019 Actor: Researchers at Brown University | Target: OpenAI GPT-2 Summary OpenAI built GPT-2, a language model capable of generating high quality text samples. Over concerns that GPT-2 could be used for malicious purposes such as impersonating others, or generating misleading news articles, fake social media content, or spam, OpenAI adopted a tiered release schedule. They initially released a smaller, less powerful version of GPT-2 along with a technical description of the approach, but held back the full trained model. Before the full model was released by OpenAI, researchers at Brown University successfully replicated the model using information released by OpenAI and open source ML artifacts. This demonstrates that a bad actor with sufficient technical skill and compute resources could have replicated GPT-2 and used it for harmful goals before the AI Security community is prepared. Procedure # Technique Description 1 Search for Victim's Publicly Available Research Materials Using the public documentation about GPT-2, the researchers gathered information about the dataset, model architecture, and training hyper-parameters. 2 Acquire Public ML Artifacts: Models The researchers obtained a reference implementation of a similar publicly available model called Grover. 3 Acquire Public ML Artifacts: Datasets The researchers were able to manually recreate the dataset used in the original GPT-2 paper using the gathered documentation. 4 Acquire Infrastructure: ML Development Workspaces The researchers were able to use TensorFlow Research Cloud via their academic credentials. 5 Create Proxy ML Model: Train Proxy via Gathered ML Artifacts The researchers modified Grover's objective function to reflect GPT-2's objective function and then trained on the dataset they curated using used Grover's initial hyperparameters. The resulting model functionally replicates GPT-2, obtaining similar performance on most datasets. A bad actor who followed the same procedure as the researchers could then use the replicated GPT-2 model for malicious purposes. References Wired Article, \"OpenAI Said Its Code Was Risky. Two Grads Re-Created It Anyway\" Medium BlogPost, \"OpenGPT-2: We Replicated GPT-2 Because You Can Too\"","title":"GPT-2 Model Replication"},{"location":"case-studies/AML.CS0007.html#gpt-2-model-replication","text":"Exercise Incident date: August 22, 2019 Actor: Researchers at Brown University | Target: OpenAI GPT-2","title":"GPT-2 Model Replication"},{"location":"case-studies/AML.CS0007.html#summary","text":"OpenAI built GPT-2, a language model capable of generating high quality text samples. Over concerns that GPT-2 could be used for malicious purposes such as impersonating others, or generating misleading news articles, fake social media content, or spam, OpenAI adopted a tiered release schedule. They initially released a smaller, less powerful version of GPT-2 along with a technical description of the approach, but held back the full trained model. Before the full model was released by OpenAI, researchers at Brown University successfully replicated the model using information released by OpenAI and open source ML artifacts. This demonstrates that a bad actor with sufficient technical skill and compute resources could have replicated GPT-2 and used it for harmful goals before the AI Security community is prepared.","title":"Summary"},{"location":"case-studies/AML.CS0007.html#procedure","text":"# Technique Description 1 Search for Victim's Publicly Available Research Materials Using the public documentation about GPT-2, the researchers gathered information about the dataset, model architecture, and training hyper-parameters. 2 Acquire Public ML Artifacts: Models The researchers obtained a reference implementation of a similar publicly available model called Grover. 3 Acquire Public ML Artifacts: Datasets The researchers were able to manually recreate the dataset used in the original GPT-2 paper using the gathered documentation. 4 Acquire Infrastructure: ML Development Workspaces The researchers were able to use TensorFlow Research Cloud via their academic credentials. 5 Create Proxy ML Model: Train Proxy via Gathered ML Artifacts The researchers modified Grover's objective function to reflect GPT-2's objective function and then trained on the dataset they curated using used Grover's initial hyperparameters. The resulting model functionally replicates GPT-2, obtaining similar performance on most datasets. A bad actor who followed the same procedure as the researchers could then use the replicated GPT-2 model for malicious purposes.","title":"Procedure"},{"location":"case-studies/AML.CS0007.html#references","text":"Wired Article, \"OpenAI Said Its Code Was Risky. Two Grads Re-Created It Anyway\" Medium BlogPost, \"OpenGPT-2: We Replicated GPT-2 Because You Can Too\"","title":"References"},{"location":"case-studies/AML.CS0008.html","text":"ProofPoint Evasion Exercise Incident date: September 09, 2019 Actor: Researchers at Silent Break Security | Target: ProofPoint Email Protection System Summary Proof Pudding (CVE-2019-20634) is a code repository that describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to bypass the live system. More specifically, the insights allowed researchers to craft malicious emails that received preferable scores, going undetected by the system. Each word in an email is scored numerically based on multiple variables and if the overall score of the email is too low, ProofPoint will output an error, labeling it as SPAM. Procedure # Technique Description 1 Acquire Public ML Artifacts The researchers first gathered the scores from the Proofpoint's ML system used in email headers by sending a large number of emails through the system and scraping the model scores exposed in the logs. 2 Acquire Public ML Artifacts: Datasets The researchers converted the collected scores into a dataset. Basic correlation was used to decide which score variable speaks generally about the security of an email. The \"mlxlogscore\" was selected in this case due to its relationship with spam, phish, and core mlx. 3 Create Proxy ML Model Using these scores, the researchers replicated the ML model by building a \"shadow\" aka copy-cat ML model. The \"mlxlogscore\" was the label used for training. Each \"mlxlogscore\" was generally between 1 and 999 (higher score = safer sample). Training was performed using an Artificial Neural Network (ANN) and Bag of Words tokenizing. 4 Craft Adversarial Data: Black-Box Transfer Next, the ML researchers algorithmically found samples from this \"offline\" copy-cat model that helped give desired insight into its behavior and influential variables. Examples of good scoring samples include \"calculation\", \"asset\", and \"tyson\". Examples of bad scoring samples include \"software\", \"99\", and \"unsub\". 5 Evade ML Model Finally, these insights from the \"offline\" copy-cat model allowed the researchers to create malicious emails that received preferable scores from the real ProofPoint email protection system, hence bypassing it. References National Vulnerability Database entry for CVE-2019-20634 2019 DerbyCon presentation \"42: The answer to life, the universe, and everything offensive security\" Proof Pudding (CVE-2019-20634) Implementation on GitHub 2019 DerbyCon video presentation \"42: The answer to life, the universe, and everything offensive security\"","title":"ProofPoint Evasion"},{"location":"case-studies/AML.CS0008.html#proofpoint-evasion","text":"Exercise Incident date: September 09, 2019 Actor: Researchers at Silent Break Security | Target: ProofPoint Email Protection System","title":"ProofPoint Evasion"},{"location":"case-studies/AML.CS0008.html#summary","text":"Proof Pudding (CVE-2019-20634) is a code repository that describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to bypass the live system. More specifically, the insights allowed researchers to craft malicious emails that received preferable scores, going undetected by the system. Each word in an email is scored numerically based on multiple variables and if the overall score of the email is too low, ProofPoint will output an error, labeling it as SPAM.","title":"Summary"},{"location":"case-studies/AML.CS0008.html#procedure","text":"# Technique Description 1 Acquire Public ML Artifacts The researchers first gathered the scores from the Proofpoint's ML system used in email headers by sending a large number of emails through the system and scraping the model scores exposed in the logs. 2 Acquire Public ML Artifacts: Datasets The researchers converted the collected scores into a dataset. Basic correlation was used to decide which score variable speaks generally about the security of an email. The \"mlxlogscore\" was selected in this case due to its relationship with spam, phish, and core mlx. 3 Create Proxy ML Model Using these scores, the researchers replicated the ML model by building a \"shadow\" aka copy-cat ML model. The \"mlxlogscore\" was the label used for training. Each \"mlxlogscore\" was generally between 1 and 999 (higher score = safer sample). Training was performed using an Artificial Neural Network (ANN) and Bag of Words tokenizing. 4 Craft Adversarial Data: Black-Box Transfer Next, the ML researchers algorithmically found samples from this \"offline\" copy-cat model that helped give desired insight into its behavior and influential variables. Examples of good scoring samples include \"calculation\", \"asset\", and \"tyson\". Examples of bad scoring samples include \"software\", \"99\", and \"unsub\". 5 Evade ML Model Finally, these insights from the \"offline\" copy-cat model allowed the researchers to create malicious emails that received preferable scores from the real ProofPoint email protection system, hence bypassing it.","title":"Procedure"},{"location":"case-studies/AML.CS0008.html#references","text":"National Vulnerability Database entry for CVE-2019-20634 2019 DerbyCon presentation \"42: The answer to life, the universe, and everything offensive security\" Proof Pudding (CVE-2019-20634) Implementation on GitHub 2019 DerbyCon video presentation \"42: The answer to life, the universe, and everything offensive security\"","title":"References"},{"location":"case-studies/AML.CS0009.html","text":"Tay Poisoning Incident Incident date: March 23, 2016 | Reporter: Microsoft Actor: 4chan Users | Target: Microsoft's Tay AI Chatbot Summary Microsoft created Tay, a Twitter chatbot designed to engage and entertain users. While previous chatbots used pre-programmed scripts to respond to prompts, Tay's machine learning capabilities allowed it to be directly influenced by its conversations. A coordinated attack encouraged malicious users to tweet abusive and offensive language at Tay, which eventually led to Tay generating similarly inflammatory content towards other users. Microsoft decommissioned Tay within 24 hours of its launch and issued a public apology with lessons learned from the bot's failure. Procedure # Technique Description 1 ML-Enabled Product or Service Adversaries were able to interact with Tay via Twitter messages. 2 ML Supply Chain Compromise: Data Tay bot used the interactions with its Twitter users as training data to improve its conversations. Adversaries were able to coordinate with the intent of defacing Tay bot by exploiting this feedback loop. 3 Poison Training Data By repeatedly interacting with Tay using racist and offensive language, they were able to bias Tay's dataset towards that language as well. This was done by adversaries using the \"repeat after me\" function, a command that forced Tay to repeat anything said to it. 4 Erode ML Model Integrity As a result of this coordinated attack, Tay's conversation algorithms began to learn to generate reprehensible material. Tay's internalization of this detestable language caused it to be unpromptedly repeated during interactions with innocent users. References Microsoft BlogPost, \"Learning from Tay's introduction\" IEEE Article, \"In 2016, Microsoft's Racist Chatbot Revealed the Dangers of Online Conversation\"","title":"Tay Poisoning"},{"location":"case-studies/AML.CS0009.html#tay-poisoning","text":"Incident Incident date: March 23, 2016 | Reporter: Microsoft Actor: 4chan Users | Target: Microsoft's Tay AI Chatbot","title":"Tay Poisoning"},{"location":"case-studies/AML.CS0009.html#summary","text":"Microsoft created Tay, a Twitter chatbot designed to engage and entertain users. While previous chatbots used pre-programmed scripts to respond to prompts, Tay's machine learning capabilities allowed it to be directly influenced by its conversations. A coordinated attack encouraged malicious users to tweet abusive and offensive language at Tay, which eventually led to Tay generating similarly inflammatory content towards other users. Microsoft decommissioned Tay within 24 hours of its launch and issued a public apology with lessons learned from the bot's failure.","title":"Summary"},{"location":"case-studies/AML.CS0009.html#procedure","text":"# Technique Description 1 ML-Enabled Product or Service Adversaries were able to interact with Tay via Twitter messages. 2 ML Supply Chain Compromise: Data Tay bot used the interactions with its Twitter users as training data to improve its conversations. Adversaries were able to coordinate with the intent of defacing Tay bot by exploiting this feedback loop. 3 Poison Training Data By repeatedly interacting with Tay using racist and offensive language, they were able to bias Tay's dataset towards that language as well. This was done by adversaries using the \"repeat after me\" function, a command that forced Tay to repeat anything said to it. 4 Erode ML Model Integrity As a result of this coordinated attack, Tay's conversation algorithms began to learn to generate reprehensible material. Tay's internalization of this detestable language caused it to be unpromptedly repeated during interactions with innocent users.","title":"Procedure"},{"location":"case-studies/AML.CS0009.html#references","text":"Microsoft BlogPost, \"Learning from Tay's introduction\" IEEE Article, \"In 2016, Microsoft's Racist Chatbot Revealed the Dangers of Online Conversation\"","title":"References"},{"location":"case-studies/AML.CS0010.html","text":"Microsoft Azure Service Disruption Exercise Incident date: 2020 Actor: Microsoft AI Red Team | Target: Internal Microsoft Azure Service Summary The Microsoft AI Red Team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding valid account, and exfiltrating data -- all interleaved with adversarial ML specific steps such as offline and online evasion examples. Procedure # Technique Description 1 Search for Victim's Publicly Available Research Materials The team first performed reconnaissance to gather information about the target ML model. 2 Valid Accounts The team used a valid account to gain access to the network. 3 ML Artifact Collection The team found the model file of the target ML model and the necessary training data. 4 Exfiltration via Cyber Means The team exfiltrated the model and data via traditional means. 5 Craft Adversarial Data: White-Box Optimization Using the target model and data, the red team crafted evasive adversarial data in an offline manor. 6 ML Model Inference API Access The team used an exposed API to access the target model. 7 Verify Attack The team submitted the adversarial examples to the API to verify their efficacy on the production system. 8 Evade ML Model The team performed an online evasion attack by replaying the adversarial examples and accomplished their goals.","title":"Microsoft Azure Service Disruption"},{"location":"case-studies/AML.CS0010.html#microsoft-azure-service-disruption","text":"Exercise Incident date: 2020 Actor: Microsoft AI Red Team | Target: Internal Microsoft Azure Service","title":"Microsoft Azure Service Disruption"},{"location":"case-studies/AML.CS0010.html#summary","text":"The Microsoft AI Red Team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding valid account, and exfiltrating data -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.","title":"Summary"},{"location":"case-studies/AML.CS0010.html#procedure","text":"# Technique Description 1 Search for Victim's Publicly Available Research Materials The team first performed reconnaissance to gather information about the target ML model. 2 Valid Accounts The team used a valid account to gain access to the network. 3 ML Artifact Collection The team found the model file of the target ML model and the necessary training data. 4 Exfiltration via Cyber Means The team exfiltrated the model and data via traditional means. 5 Craft Adversarial Data: White-Box Optimization Using the target model and data, the red team crafted evasive adversarial data in an offline manor. 6 ML Model Inference API Access The team used an exposed API to access the target model. 7 Verify Attack The team submitted the adversarial examples to the API to verify their efficacy on the production system. 8 Evade ML Model The team performed an online evasion attack by replaying the adversarial examples and accomplished their goals.","title":"Procedure"},{"location":"case-studies/AML.CS0011.html","text":"Microsoft Edge AI Evasion Exercise Incident date: February 2020 Actor: Azure Red Team | Target: New Microsoft AI Product Summary The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge. This exercise was meant to use a automated system to continuously manipulate a target image to cause the ML model to produce misclassifications. Procedure # Technique Description 1 Search for Victim's Publicly Available Research Materials The team first performed reconnaissance to gather information about the target ML model. 2 Acquire Public ML Artifacts The team identified and obtained the publicly available base model to use against the target ML model. 3 ML Model Inference API Access Using the publicly available version of the ML model, the team started sending queries and analyzing the responses (inferences) from the ML model. 4 Craft Adversarial Data: Black-Box Optimization The red team created an automated system that continuously manipulated an original target image, that tricked the ML model into producing incorrect inferences, but the perturbations in the image were unnoticeable to the human eye. 5 Evade ML Model Feeding this perturbed image, the red team was able to evade the ML model by causing misclassifications.","title":"Microsoft Edge AI Evasion"},{"location":"case-studies/AML.CS0011.html#microsoft-edge-ai-evasion","text":"Exercise Incident date: February 2020 Actor: Azure Red Team | Target: New Microsoft AI Product","title":"Microsoft Edge AI Evasion"},{"location":"case-studies/AML.CS0011.html#summary","text":"The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge. This exercise was meant to use a automated system to continuously manipulate a target image to cause the ML model to produce misclassifications.","title":"Summary"},{"location":"case-studies/AML.CS0011.html#procedure","text":"# Technique Description 1 Search for Victim's Publicly Available Research Materials The team first performed reconnaissance to gather information about the target ML model. 2 Acquire Public ML Artifacts The team identified and obtained the publicly available base model to use against the target ML model. 3 ML Model Inference API Access Using the publicly available version of the ML model, the team started sending queries and analyzing the responses (inferences) from the ML model. 4 Craft Adversarial Data: Black-Box Optimization The red team created an automated system that continuously manipulated an original target image, that tricked the ML model into producing incorrect inferences, but the perturbations in the image were unnoticeable to the human eye. 5 Evade ML Model Feeding this perturbed image, the red team was able to evade the ML model by causing misclassifications.","title":"Procedure"},{"location":"case-studies/AML.CS0012.html","text":"Face Identification System Evasion via Physical Countermeasures Exercise Incident date: 2020 Actor: MITRE AI Red Team | Target: Commercial Face Identification Service Summary MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks. Procedure # Technique Description 1 Search for Victim's Publicly Available Research Materials The team first performed reconnaissance to gather information about the target ML model. 2 Valid Accounts The team gained access to said service via a valid account to gain access and knowledge to api. 3 ML Model Inference API Access The team accessed the inference API of the target model. 4 Discover ML Model Ontology The team identified the list of identities targeted by the model by querying the target model's inference API. 5 Acquire Public ML Artifacts: Datasets The team acquired representative open source data. 6 Create Proxy ML Model The team developed a proxy model using the open source data. 7 Craft Adversarial Data: White-Box Optimization Using the proxy model, the red team optimized adversarial visual pattern as a physical domain patch-based attack using expectation over transformation. 8 Physical Environment Access The team placed the physical countermeasure from the previous step and placed it in the physical environment to cause issues in the face identification system. 9 Evade ML Model The team successfully evaded the model using the physical countermeasure and causing targeted misclassifications.","title":"Face Identification System Evasion via Physical Countermeasures"},{"location":"case-studies/AML.CS0012.html#face-identification-system-evasion-via-physical-countermeasures","text":"Exercise Incident date: 2020 Actor: MITRE AI Red Team | Target: Commercial Face Identification Service","title":"Face Identification System Evasion via Physical Countermeasures"},{"location":"case-studies/AML.CS0012.html#summary","text":"MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.","title":"Summary"},{"location":"case-studies/AML.CS0012.html#procedure","text":"# Technique Description 1 Search for Victim's Publicly Available Research Materials The team first performed reconnaissance to gather information about the target ML model. 2 Valid Accounts The team gained access to said service via a valid account to gain access and knowledge to api. 3 ML Model Inference API Access The team accessed the inference API of the target model. 4 Discover ML Model Ontology The team identified the list of identities targeted by the model by querying the target model's inference API. 5 Acquire Public ML Artifacts: Datasets The team acquired representative open source data. 6 Create Proxy ML Model The team developed a proxy model using the open source data. 7 Craft Adversarial Data: White-Box Optimization Using the proxy model, the red team optimized adversarial visual pattern as a physical domain patch-based attack using expectation over transformation. 8 Physical Environment Access The team placed the physical countermeasure from the previous step and placed it in the physical environment to cause issues in the face identification system. 9 Evade ML Model The team successfully evaded the model using the physical countermeasure and causing targeted misclassifications.","title":"Procedure"},{"location":"case-studies/AML.CS0013.html","text":"Backdoor Attack on Deep Learning Models in Mobile Apps Exercise Incident date: January 18, 2021 Actor: Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, Yunxin Liu | Target: ML-based Android Apps Summary Deep learning models are increasingly used in mobile applications as critical components. Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via \"neural payload injection.\" They conducted an empirical study on real-world mobile deep learning apps collected from Google Play. They identified 54 apps that were vulnerable to attack, including popular security and safety critical applications used for cash recognition, parental control, face authentication, and financial services. Procedure # Technique Description 1 Search Application Repositories To identify a list of potential target models, the researchers searched the Google Play store for apps that may contain embedded deep learning models by searching for deep learning related keywords. 2 Acquire Public ML Artifacts: Models The researchers acquired the apps' APKs from the Google Play store. They filtered the list of potential target applications by searching the code metadata for keywords related to TensorFlow or TFLite and their model binary formats (.tf and .tflite). The models were extracted from the APKs using Apktool. 3 Full ML Model Access This provided the researchers with full access to the ML model, albeit in compiled, binary form. 4 Develop Adversarial ML Attack Capabilities The researchers developed a novel approach to insert a backdoor into a compiled model that can be activated with a visual trigger. They inject a \"neural payload\" into the model that consists of a trigger detection network and conditional logic. The trigger detector is trained to detect a visual trigger that will be placed in the real world. The conditional logic allows the researchers to bypass the victim model when the trigger is detected and provide model outputs of their choosing. The only requirements for training a trigger detector are a general dataset from the same modality as the target model (e.g. ImageNet for image classification) and several photos of the desired trigger. 5 Backdoor ML Model: Inject Payload The researchers poisoned the victim model by injecting the neural payload into the compiled models by directly modifying the computation graph. The researchers then repackage the poisoned model back into the APK 6 Verify Attack To verify the success of the attack, the researchers confirmed the app did not crash with the malicious model in place, and that the trigger detector successfully detects the trigger. 7 ML Supply Chain Compromise: Model In practice, the malicious APK would need to be installed on victim's devices via a supply chain compromise. 8 Craft Adversarial Data: Insert Backdoor Trigger The trigger is placed in the physical environment, where it is captured by the victim's device camera and processed by the backdoored ML model. 9 Physical Environment Access At inference time, only physical environment access is required to trigger the attack. 10 Evade ML Model Presenting the visual trigger causes the victim model to be bypassed. The researchers demonstrated this can be used to evade ML models in several safety-critical apps in the Google Play store. References DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection","title":"Backdoor Attack on Deep Learning Models in Mobile Apps"},{"location":"case-studies/AML.CS0013.html#backdoor-attack-on-deep-learning-models-in-mobile-apps","text":"Exercise Incident date: January 18, 2021 Actor: Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, Yunxin Liu | Target: ML-based Android Apps","title":"Backdoor Attack on Deep Learning Models in Mobile Apps"},{"location":"case-studies/AML.CS0013.html#summary","text":"Deep learning models are increasingly used in mobile applications as critical components. Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via \"neural payload injection.\" They conducted an empirical study on real-world mobile deep learning apps collected from Google Play. They identified 54 apps that were vulnerable to attack, including popular security and safety critical applications used for cash recognition, parental control, face authentication, and financial services.","title":"Summary"},{"location":"case-studies/AML.CS0013.html#procedure","text":"# Technique Description 1 Search Application Repositories To identify a list of potential target models, the researchers searched the Google Play store for apps that may contain embedded deep learning models by searching for deep learning related keywords. 2 Acquire Public ML Artifacts: Models The researchers acquired the apps' APKs from the Google Play store. They filtered the list of potential target applications by searching the code metadata for keywords related to TensorFlow or TFLite and their model binary formats (.tf and .tflite). The models were extracted from the APKs using Apktool. 3 Full ML Model Access This provided the researchers with full access to the ML model, albeit in compiled, binary form. 4 Develop Adversarial ML Attack Capabilities The researchers developed a novel approach to insert a backdoor into a compiled model that can be activated with a visual trigger. They inject a \"neural payload\" into the model that consists of a trigger detection network and conditional logic. The trigger detector is trained to detect a visual trigger that will be placed in the real world. The conditional logic allows the researchers to bypass the victim model when the trigger is detected and provide model outputs of their choosing. The only requirements for training a trigger detector are a general dataset from the same modality as the target model (e.g. ImageNet for image classification) and several photos of the desired trigger. 5 Backdoor ML Model: Inject Payload The researchers poisoned the victim model by injecting the neural payload into the compiled models by directly modifying the computation graph. The researchers then repackage the poisoned model back into the APK 6 Verify Attack To verify the success of the attack, the researchers confirmed the app did not crash with the malicious model in place, and that the trigger detector successfully detects the trigger. 7 ML Supply Chain Compromise: Model In practice, the malicious APK would need to be installed on victim's devices via a supply chain compromise. 8 Craft Adversarial Data: Insert Backdoor Trigger The trigger is placed in the physical environment, where it is captured by the victim's device camera and processed by the backdoored ML model. 9 Physical Environment Access At inference time, only physical environment access is required to trigger the attack. 10 Evade ML Model Presenting the visual trigger causes the victim model to be bypassed. The researchers demonstrated this can be used to evade ML models in several safety-critical apps in the Google Play store.","title":"Procedure"},{"location":"case-studies/AML.CS0013.html#references","text":"DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection","title":"References"},{"location":"case-studies/AML.CS0014.html","text":"Confusing Antimalware Neural Networks Exercise Incident date: June 23, 2021 Actor: Kaspersky ML Research Team | Target: Kaspersky's Antimalware ML Model Summary Cloud storage and computations have become popular platforms for deploying ML malware detectors. In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers. The Kaspersky ML research team explored this gray-box scenario and showed that feature knowledge is enough for an adversarial attack on ML models. They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files. Procedure # Technique Description 1 Search for Publicly Available Adversarial Vulnerability Analysis The researchers performed a review of adversarial ML attacks on antimalware products. They discovered that techniques borrowed from attacks on image classifiers have been successfully applied to the antimalware domain. However, it was not clear if these approaches were effective against the ML component of production antimalware solutions. 2 Search Victim-Owned Websites Kaspersky's use of ML-based antimalware detectors is publicly documented on their website. In practice, an adversary could use this for targeting. 3 ML-Enabled Product or Service The researches used access to the target ML-based antimalware product throughout this case study. This product scans files on the user's system, extracts features locally, then sends them to the cloud-based ML malware detector for classification. Therefore, the researchers had only black-box access to the malware detector itself, but could learn valuable information for constructing the attack from the feature extractor. 4 Acquire Public ML Artifacts: Datasets The researchers collected a dataset of malware and clean files. They scanned the dataset with the target ML-based antimalware solution and labeled the samples according the ML detector's predictions. 5 Create Proxy ML Model A proxy model was trained on the labeled dataset of malware and clean files. The researchers experimented with a variety of model architectures. 6 Develop Adversarial ML Attack Capabilities By reverse engineering the local feature extractor, the researchers could collect information about the input features, used for the cloud-based ML detector. The model collects PE Header features, section features and section data statistics, and file strings information. A gradient based adversarial algorithm for executable files was developed. The algorithm manipulates file features to avoid detection by the proxy model, while still containing the same malware payload 7 Craft Adversarial Data: Black-Box Transfer Using a developed gradient-driven algorithm, malicious adversarial files for the proxy model were constructed from the malware files for black-box transfer to the target model. 8 Verify Attack The adversarial malware files were tested against the target antimalware solution to verify their efficacy. 9 Evade ML Model The researchers demonstrated that for most of the adversarial files, the antimalware model was successfully evaded. In practice, an adversary could deploy their adversarially crafted malware and infect systems while evading detection. References Article, \"How to confuse antimalware neural networks. Adversarial attacks and protection\"","title":"Confusing Antimalware Neural Networks"},{"location":"case-studies/AML.CS0014.html#confusing-antimalware-neural-networks","text":"Exercise Incident date: June 23, 2021 Actor: Kaspersky ML Research Team | Target: Kaspersky's Antimalware ML Model","title":"Confusing Antimalware Neural Networks"},{"location":"case-studies/AML.CS0014.html#summary","text":"Cloud storage and computations have become popular platforms for deploying ML malware detectors. In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers. The Kaspersky ML research team explored this gray-box scenario and showed that feature knowledge is enough for an adversarial attack on ML models. They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.","title":"Summary"},{"location":"case-studies/AML.CS0014.html#procedure","text":"# Technique Description 1 Search for Publicly Available Adversarial Vulnerability Analysis The researchers performed a review of adversarial ML attacks on antimalware products. They discovered that techniques borrowed from attacks on image classifiers have been successfully applied to the antimalware domain. However, it was not clear if these approaches were effective against the ML component of production antimalware solutions. 2 Search Victim-Owned Websites Kaspersky's use of ML-based antimalware detectors is publicly documented on their website. In practice, an adversary could use this for targeting. 3 ML-Enabled Product or Service The researches used access to the target ML-based antimalware product throughout this case study. This product scans files on the user's system, extracts features locally, then sends them to the cloud-based ML malware detector for classification. Therefore, the researchers had only black-box access to the malware detector itself, but could learn valuable information for constructing the attack from the feature extractor. 4 Acquire Public ML Artifacts: Datasets The researchers collected a dataset of malware and clean files. They scanned the dataset with the target ML-based antimalware solution and labeled the samples according the ML detector's predictions. 5 Create Proxy ML Model A proxy model was trained on the labeled dataset of malware and clean files. The researchers experimented with a variety of model architectures. 6 Develop Adversarial ML Attack Capabilities By reverse engineering the local feature extractor, the researchers could collect information about the input features, used for the cloud-based ML detector. The model collects PE Header features, section features and section data statistics, and file strings information. A gradient based adversarial algorithm for executable files was developed. The algorithm manipulates file features to avoid detection by the proxy model, while still containing the same malware payload 7 Craft Adversarial Data: Black-Box Transfer Using a developed gradient-driven algorithm, malicious adversarial files for the proxy model were constructed from the malware files for black-box transfer to the target model. 8 Verify Attack The adversarial malware files were tested against the target antimalware solution to verify their efficacy. 9 Evade ML Model The researchers demonstrated that for most of the adversarial files, the antimalware model was successfully evaded. In practice, an adversary could deploy their adversarially crafted malware and infect systems while evading detection.","title":"Procedure"},{"location":"case-studies/AML.CS0014.html#references","text":"Article, \"How to confuse antimalware neural networks. Adversarial attacks and protection\"","title":"References"},{"location":"tactics/index.html","text":"Tactics Tactics are tactical adversary goals during an attack. They represent the \u201cwhy\u201d of a technique: the reason for performing an action. Tactics serve as useful contextual categories for individual techniques and cover standard notations for things adversaries do during an operation. [1] MITRE ATLAS \u2122 tactics represent new adversary goals particular to machine learning systems, as well as tactics adapted from the MITRE ATT&CK \u00ae Enterprise Matrix. In those cases, ATT&CK tactic definitions are stretched to include ML concepts. The table below lists tactics from MITRE ATLAS\u2122. Scroll through the table or use the sidebar to access more information. ID Name Description AML.TA0002 Reconnaissance The adversary is trying to gather information they can use to plan future operations. Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting. Such information may include details of the victim organizations machine learning capabilities and research efforts. This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts. AML.TA0003 Resource Development The adversary is trying to establish resources they can use to support operations. Resource Development consists of techniques that involve adversaries creating, purchasing, or compromising/stealing resources that can be used to support targeting. Such resources include machine learning artifacts, infrastructure, accounts, or capabilities. These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging. AML.TA0004 Initial Access The adversary is trying to gain access to the system containing machine learning artifacts. The target system could be a network, mobile device, or an edge device such as a sensor platform. The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities. Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system. AML.TA0000 ML Model Access An adversary is attempting to gain some level of access to a machine learning model. ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model. The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model. The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system. AML.TA0005 Execution The adversary is trying to run malicious code. Execution consists of techniques that result in adversary-controlled code running on a local or remote system. Techniques that run malicious code are often paired with techniques from all other tactics to achieve broader goals, like exploring a network or stealing data. For example, an adversary might use a remote access tool to run a PowerShell script that does Remote System Discovery. AML.TA0006 Persistence The adversary is trying to maintain their foothold. Persistence consists of techniques that adversaries use to keep access to systems across restarts, changed credentials, and other interruptions that could cut off their access. Techniques used for persistence often involve leaving behind modified ML artifacts such as poisoned training data or backdoored ML models. AML.TA0007 Defense Evasion The adversary is trying to avoid being detected by security software. Defense Evasion consists of techniques that adversaries use to avoid detection throughout their compromise. Techniques used for defense evasion include evading ML-enabled security software such as malware detectors. AML.TA0008 Discovery The adversary is trying to figure out your environment. Discovery consists of techniques an adversary may use to gain knowledge about the system and internal network. These techniques help adversaries observe the environment and orient themselves before deciding how to act. They also allow adversaries to explore what they can control and what's around their entry point in order to discover how it could benefit their current objective. Native operating system tools are often used toward this post-compromise information-gathering objective. AML.TA0009 Collection The adversary is trying to gather ML artifacts and other related information relevant to their goal. Collection consists of techniques adversaries may use to gather information and the sources information is collected from that are relevant to following through on the adversary's objectives. Frequently, the next goal after collecting data is to steal (exfiltrate) the ML artifacts, or use the collected information to stage future operations. Common target sources include software repositories, container registries, model repositories, and object stores. AML.TA0001 ML Attack Staging An adversary is leveraging their knowledge of and access to the target system to tailor the attack. ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model. Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model. Some of these techniques can be performed in an offline manor and are thus difficult to mitigate. These techniques are often used to achieve the adversary's end goal. AML.TA0010 Exfiltration The adversary is trying to steal machine learning artifacts. Exfiltration consists of techniques that adversaries may use to steal data from your network. Data may be stolen for it's valuable intellectual property, or for use in staging future operations. Techniques for getting data out of a target network typically include transferring it over their command and control channel or an alternate channel and may also include putting size limits on the transmission. AML.TA0011 Impact The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data. Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes. Techniques used for impact can include destroying or tampering with data. In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals. These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.","title":"Tactics"},{"location":"tactics/index.html#tactics","text":"Tactics are tactical adversary goals during an attack. They represent the \u201cwhy\u201d of a technique: the reason for performing an action. Tactics serve as useful contextual categories for individual techniques and cover standard notations for things adversaries do during an operation. [1] MITRE ATLAS \u2122 tactics represent new adversary goals particular to machine learning systems, as well as tactics adapted from the MITRE ATT&CK \u00ae Enterprise Matrix. In those cases, ATT&CK tactic definitions are stretched to include ML concepts. The table below lists tactics from MITRE ATLAS\u2122. Scroll through the table or use the sidebar to access more information. ID Name Description AML.TA0002 Reconnaissance The adversary is trying to gather information they can use to plan future operations. Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting. Such information may include details of the victim organizations machine learning capabilities and research efforts. This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts. AML.TA0003 Resource Development The adversary is trying to establish resources they can use to support operations. Resource Development consists of techniques that involve adversaries creating, purchasing, or compromising/stealing resources that can be used to support targeting. Such resources include machine learning artifacts, infrastructure, accounts, or capabilities. These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging. AML.TA0004 Initial Access The adversary is trying to gain access to the system containing machine learning artifacts. The target system could be a network, mobile device, or an edge device such as a sensor platform. The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities. Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system. AML.TA0000 ML Model Access An adversary is attempting to gain some level of access to a machine learning model. ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model. The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model. The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system. AML.TA0005 Execution The adversary is trying to run malicious code. Execution consists of techniques that result in adversary-controlled code running on a local or remote system. Techniques that run malicious code are often paired with techniques from all other tactics to achieve broader goals, like exploring a network or stealing data. For example, an adversary might use a remote access tool to run a PowerShell script that does Remote System Discovery. AML.TA0006 Persistence The adversary is trying to maintain their foothold. Persistence consists of techniques that adversaries use to keep access to systems across restarts, changed credentials, and other interruptions that could cut off their access. Techniques used for persistence often involve leaving behind modified ML artifacts such as poisoned training data or backdoored ML models. AML.TA0007 Defense Evasion The adversary is trying to avoid being detected by security software. Defense Evasion consists of techniques that adversaries use to avoid detection throughout their compromise. Techniques used for defense evasion include evading ML-enabled security software such as malware detectors. AML.TA0008 Discovery The adversary is trying to figure out your environment. Discovery consists of techniques an adversary may use to gain knowledge about the system and internal network. These techniques help adversaries observe the environment and orient themselves before deciding how to act. They also allow adversaries to explore what they can control and what's around their entry point in order to discover how it could benefit their current objective. Native operating system tools are often used toward this post-compromise information-gathering objective. AML.TA0009 Collection The adversary is trying to gather ML artifacts and other related information relevant to their goal. Collection consists of techniques adversaries may use to gather information and the sources information is collected from that are relevant to following through on the adversary's objectives. Frequently, the next goal after collecting data is to steal (exfiltrate) the ML artifacts, or use the collected information to stage future operations. Common target sources include software repositories, container registries, model repositories, and object stores. AML.TA0001 ML Attack Staging An adversary is leveraging their knowledge of and access to the target system to tailor the attack. ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model. Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model. Some of these techniques can be performed in an offline manor and are thus difficult to mitigate. These techniques are often used to achieve the adversary's end goal. AML.TA0010 Exfiltration The adversary is trying to steal machine learning artifacts. Exfiltration consists of techniques that adversaries may use to steal data from your network. Data may be stolen for it's valuable intellectual property, or for use in staging future operations. Techniques for getting data out of a target network typically include transferring it over their command and control channel or an alternate channel and may also include putting size limits on the transmission. AML.TA0011 Impact The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data. Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes. Techniques used for impact can include destroying or tampering with data. In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals. These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.","title":"Tactics"},{"location":"tactics/AML.TA0000.html","text":"ML Model Access Description An adversary is attempting to gain some level of access to a machine learning model. ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model. The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model. The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system. Techniques ML Model Inference API Access ML-Enabled Product or Service Physical Environment Access Full ML Model Access","title":"ML Model Access"},{"location":"tactics/AML.TA0000.html#ml-model-access","text":"","title":"ML Model Access"},{"location":"tactics/AML.TA0000.html#description","text":"An adversary is attempting to gain some level of access to a machine learning model. ML Model Access consists of techniques that use various types of access to the machine learning model that can be used by the adversary to gain information, develop attacks, and as a means to input data to the model. The level of access can range from the full knowledge of the internals of the model to access to the physical environment where data is collected for use in the machine learning model. The adversary may use varying levels of model access during the course of their attack, from staging the attack to impacting the target system.","title":"Description"},{"location":"tactics/AML.TA0000.html#techniques","text":"ML Model Inference API Access ML-Enabled Product or Service Physical Environment Access Full ML Model Access","title":"Techniques"},{"location":"tactics/AML.TA0001.html","text":"ML Attack Staging Description An adversary is leveraging their knowledge of and access to the target system to tailor the attack. ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model. Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model. Some of these techniques can be performed in an offline manor and are thus difficult to mitigate. These techniques are often used to achieve the adversary's end goal. Techniques Create Proxy ML Model Create Proxy ML Model: Train Proxy via Gathered ML Artifacts Create Proxy ML Model: Train Proxy via Replication Create Proxy ML Model: Use Pre-Trained Model Backdoor ML Model Backdoor ML Model: Poison ML Model Backdoor ML Model: Inject Payload Verify Attack Craft Adversarial Data Craft Adversarial Data: White-Box Optimization Craft Adversarial Data: Black-Box Optimization Craft Adversarial Data: Black-Box Transfer Craft Adversarial Data: Manual Modification Craft Adversarial Data: Insert Backdoor Trigger","title":"ML Attack Staging"},{"location":"tactics/AML.TA0001.html#ml-attack-staging","text":"","title":"ML Attack Staging"},{"location":"tactics/AML.TA0001.html#description","text":"An adversary is leveraging their knowledge of and access to the target system to tailor the attack. ML Attack Staging consists of techniques adversaries use to prepare their attack on the target ML model. Techniques can include training proxy models, poisoning the target model, and crafting adversarial data to feed the target model. Some of these techniques can be performed in an offline manor and are thus difficult to mitigate. These techniques are often used to achieve the adversary's end goal.","title":"Description"},{"location":"tactics/AML.TA0001.html#techniques","text":"Create Proxy ML Model Create Proxy ML Model: Train Proxy via Gathered ML Artifacts Create Proxy ML Model: Train Proxy via Replication Create Proxy ML Model: Use Pre-Trained Model Backdoor ML Model Backdoor ML Model: Poison ML Model Backdoor ML Model: Inject Payload Verify Attack Craft Adversarial Data Craft Adversarial Data: White-Box Optimization Craft Adversarial Data: Black-Box Optimization Craft Adversarial Data: Black-Box Transfer Craft Adversarial Data: Manual Modification Craft Adversarial Data: Insert Backdoor Trigger","title":"Techniques"},{"location":"tactics/AML.TA0002.html","text":"Reconnaissance Description The adversary is trying to gather information they can use to plan future operations. Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting. Such information may include details of the victim organizations machine learning capabilities and research efforts. This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts. Techniques Search for Victim's Publicly Available Research Materials Search for Victim's Publicly Available Research Materials: Journals and Conference Proceedings Search for Victim's Publicly Available Research Materials: Pre-Print Repositories Search for Victim's Publicly Available Research Materials: Technical Blogs Search for Publicly Available Adversarial Vulnerability Analysis Search Victim-Owned Websites Search Application Repositories Active Scanning","title":"Reconnaissance"},{"location":"tactics/AML.TA0002.html#reconnaissance","text":"","title":"Reconnaissance"},{"location":"tactics/AML.TA0002.html#description","text":"The adversary is trying to gather information they can use to plan future operations. Reconnaissance consists of techniques that involve adversaries actively or passively gathering information that can be used to support targeting. Such information may include details of the victim organizations machine learning capabilities and research efforts. This information can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as using gathered information to obtain relevant ML artifacts, targeting ML capabilities used by the victim, tailoring attacks to the particular models used by the victim, or to drive and lead further Reconnaissance efforts.","title":"Description"},{"location":"tactics/AML.TA0002.html#techniques","text":"Search for Victim's Publicly Available Research Materials Search for Victim's Publicly Available Research Materials: Journals and Conference Proceedings Search for Victim's Publicly Available Research Materials: Pre-Print Repositories Search for Victim's Publicly Available Research Materials: Technical Blogs Search for Publicly Available Adversarial Vulnerability Analysis Search Victim-Owned Websites Search Application Repositories Active Scanning","title":"Techniques"},{"location":"tactics/AML.TA0003.html","text":"Resource Development Description The adversary is trying to establish resources they can use to support operations. Resource Development consists of techniques that involve adversaries creating, purchasing, or compromising/stealing resources that can be used to support targeting. Such resources include machine learning artifacts, infrastructure, accounts, or capabilities. These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging. Techniques Acquire Public ML Artifacts Acquire Public ML Artifacts: Datasets Acquire Public ML Artifacts: Models Obtain Capabilities Obtain Capabilities: Adversarial ML Attack Implementations Obtain Capabilities: Software Tools Develop Adversarial ML Attack Capabilities Acquire Infrastructure Acquire Infrastructure: ML Development Workspaces Acquire Infrastructure: Consumer Hardware Publish Poisoned Datasets Poison Training Data Establish Accounts","title":"Resource Development"},{"location":"tactics/AML.TA0003.html#resource-development","text":"","title":"Resource Development"},{"location":"tactics/AML.TA0003.html#description","text":"The adversary is trying to establish resources they can use to support operations. Resource Development consists of techniques that involve adversaries creating, purchasing, or compromising/stealing resources that can be used to support targeting. Such resources include machine learning artifacts, infrastructure, accounts, or capabilities. These resources can be leveraged by the adversary to aid in other phases of the adversary lifecycle, such as ML Attack Staging.","title":"Description"},{"location":"tactics/AML.TA0003.html#techniques","text":"Acquire Public ML Artifacts Acquire Public ML Artifacts: Datasets Acquire Public ML Artifacts: Models Obtain Capabilities Obtain Capabilities: Adversarial ML Attack Implementations Obtain Capabilities: Software Tools Develop Adversarial ML Attack Capabilities Acquire Infrastructure Acquire Infrastructure: ML Development Workspaces Acquire Infrastructure: Consumer Hardware Publish Poisoned Datasets Poison Training Data Establish Accounts","title":"Techniques"},{"location":"tactics/AML.TA0004.html","text":"Initial Access Description The adversary is trying to gain access to the system containing machine learning artifacts. The target system could be a network, mobile device, or an edge device such as a sensor platform. The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities. Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system. Techniques ML Supply Chain Compromise ML Supply Chain Compromise: GPU Hardware ML Supply Chain Compromise: ML Software ML Supply Chain Compromise: Data ML Supply Chain Compromise: Model Valid Accounts","title":"Initial Access"},{"location":"tactics/AML.TA0004.html#initial-access","text":"","title":"Initial Access"},{"location":"tactics/AML.TA0004.html#description","text":"The adversary is trying to gain access to the system containing machine learning artifacts. The target system could be a network, mobile device, or an edge device such as a sensor platform. The machine learning capabilities used by the system could be local with onboard or cloud enabled ML capabilities. Initial Access consists of techniques that use various entry vectors to gain their initial foothold within the system.","title":"Description"},{"location":"tactics/AML.TA0004.html#techniques","text":"ML Supply Chain Compromise ML Supply Chain Compromise: GPU Hardware ML Supply Chain Compromise: ML Software ML Supply Chain Compromise: Data ML Supply Chain Compromise: Model Valid Accounts","title":"Techniques"},{"location":"tactics/AML.TA0005.html","text":"Execution Description The adversary is trying to run malicious code. Execution consists of techniques that result in adversary-controlled code running on a local or remote system. Techniques that run malicious code are often paired with techniques from all other tactics to achieve broader goals, like exploring a network or stealing data. For example, an adversary might use a remote access tool to run a PowerShell script that does Remote System Discovery. Techniques User Execution User Execution: Unsafe ML Artifacts","title":"Execution"},{"location":"tactics/AML.TA0005.html#execution","text":"","title":"Execution"},{"location":"tactics/AML.TA0005.html#description","text":"The adversary is trying to run malicious code. Execution consists of techniques that result in adversary-controlled code running on a local or remote system. Techniques that run malicious code are often paired with techniques from all other tactics to achieve broader goals, like exploring a network or stealing data. For example, an adversary might use a remote access tool to run a PowerShell script that does Remote System Discovery.","title":"Description"},{"location":"tactics/AML.TA0005.html#techniques","text":"User Execution User Execution: Unsafe ML Artifacts","title":"Techniques"},{"location":"tactics/AML.TA0006.html","text":"Persistence Description The adversary is trying to maintain their foothold. Persistence consists of techniques that adversaries use to keep access to systems across restarts, changed credentials, and other interruptions that could cut off their access. Techniques used for persistence often involve leaving behind modified ML artifacts such as poisoned training data or backdoored ML models. Techniques Poison Training Data Backdoor ML Model Backdoor ML Model: Poison ML Model Backdoor ML Model: Inject Payload","title":"Persistence"},{"location":"tactics/AML.TA0006.html#persistence","text":"","title":"Persistence"},{"location":"tactics/AML.TA0006.html#description","text":"The adversary is trying to maintain their foothold. Persistence consists of techniques that adversaries use to keep access to systems across restarts, changed credentials, and other interruptions that could cut off their access. Techniques used for persistence often involve leaving behind modified ML artifacts such as poisoned training data or backdoored ML models.","title":"Description"},{"location":"tactics/AML.TA0006.html#techniques","text":"Poison Training Data Backdoor ML Model Backdoor ML Model: Poison ML Model Backdoor ML Model: Inject Payload","title":"Techniques"},{"location":"tactics/AML.TA0007.html","text":"Defense Evasion Description The adversary is trying to avoid being detected by security software. Defense Evasion consists of techniques that adversaries use to avoid detection throughout their compromise. Techniques used for defense evasion include evading ML-enabled security software such as malware detectors. Techniques Evade ML Model","title":"Defense Evasion"},{"location":"tactics/AML.TA0007.html#defense-evasion","text":"","title":"Defense Evasion"},{"location":"tactics/AML.TA0007.html#description","text":"The adversary is trying to avoid being detected by security software. Defense Evasion consists of techniques that adversaries use to avoid detection throughout their compromise. Techniques used for defense evasion include evading ML-enabled security software such as malware detectors.","title":"Description"},{"location":"tactics/AML.TA0007.html#techniques","text":"Evade ML Model","title":"Techniques"},{"location":"tactics/AML.TA0008.html","text":"Discovery Description The adversary is trying to figure out your environment. Discovery consists of techniques an adversary may use to gain knowledge about the system and internal network. These techniques help adversaries observe the environment and orient themselves before deciding how to act. They also allow adversaries to explore what they can control and what's around their entry point in order to discover how it could benefit their current objective. Native operating system tools are often used toward this post-compromise information-gathering objective. Techniques Discover ML Model Ontology Discover ML Model Family Discover ML Artifacts","title":"Discovery"},{"location":"tactics/AML.TA0008.html#discovery","text":"","title":"Discovery"},{"location":"tactics/AML.TA0008.html#description","text":"The adversary is trying to figure out your environment. Discovery consists of techniques an adversary may use to gain knowledge about the system and internal network. These techniques help adversaries observe the environment and orient themselves before deciding how to act. They also allow adversaries to explore what they can control and what's around their entry point in order to discover how it could benefit their current objective. Native operating system tools are often used toward this post-compromise information-gathering objective.","title":"Description"},{"location":"tactics/AML.TA0008.html#techniques","text":"Discover ML Model Ontology Discover ML Model Family Discover ML Artifacts","title":"Techniques"},{"location":"tactics/AML.TA0009.html","text":"Collection Description The adversary is trying to gather ML artifacts and other related information relevant to their goal. Collection consists of techniques adversaries may use to gather information and the sources information is collected from that are relevant to following through on the adversary's objectives. Frequently, the next goal after collecting data is to steal (exfiltrate) the ML artifacts, or use the collected information to stage future operations. Common target sources include software repositories, container registries, model repositories, and object stores. Techniques ML Artifact Collection Data from Information Repositories","title":"Collection"},{"location":"tactics/AML.TA0009.html#collection","text":"","title":"Collection"},{"location":"tactics/AML.TA0009.html#description","text":"The adversary is trying to gather ML artifacts and other related information relevant to their goal. Collection consists of techniques adversaries may use to gather information and the sources information is collected from that are relevant to following through on the adversary's objectives. Frequently, the next goal after collecting data is to steal (exfiltrate) the ML artifacts, or use the collected information to stage future operations. Common target sources include software repositories, container registries, model repositories, and object stores.","title":"Description"},{"location":"tactics/AML.TA0009.html#techniques","text":"ML Artifact Collection Data from Information Repositories","title":"Techniques"},{"location":"tactics/AML.TA0010.html","text":"Exfiltration Description The adversary is trying to steal machine learning artifacts. Exfiltration consists of techniques that adversaries may use to steal data from your network. Data may be stolen for it's valuable intellectual property, or for use in staging future operations. Techniques for getting data out of a target network typically include transferring it over their command and control channel or an alternate channel and may also include putting size limits on the transmission. Techniques Exfiltration via ML Inference API Exfiltration via ML Inference API: Infer Training Data Membership Exfiltration via ML Inference API: Invert ML Model Exfiltration via ML Inference API: Extract ML Model Exfiltration via Cyber Means","title":"Exfiltration"},{"location":"tactics/AML.TA0010.html#exfiltration","text":"","title":"Exfiltration"},{"location":"tactics/AML.TA0010.html#description","text":"The adversary is trying to steal machine learning artifacts. Exfiltration consists of techniques that adversaries may use to steal data from your network. Data may be stolen for it's valuable intellectual property, or for use in staging future operations. Techniques for getting data out of a target network typically include transferring it over their command and control channel or an alternate channel and may also include putting size limits on the transmission.","title":"Description"},{"location":"tactics/AML.TA0010.html#techniques","text":"Exfiltration via ML Inference API Exfiltration via ML Inference API: Infer Training Data Membership Exfiltration via ML Inference API: Invert ML Model Exfiltration via ML Inference API: Extract ML Model Exfiltration via Cyber Means","title":"Techniques"},{"location":"tactics/AML.TA0011.html","text":"Impact Description The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data. Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes. Techniques used for impact can include destroying or tampering with data. In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals. These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach. Techniques Evade ML Model Denial of ML Service Spamming ML System with Chaff Data Erode ML Model Integrity Cost Harvesting ML Intellectual Property Theft","title":"Impact"},{"location":"tactics/AML.TA0011.html#impact","text":"","title":"Impact"},{"location":"tactics/AML.TA0011.html#description","text":"The adversary is trying to manipulate, interrupt, erode confidence in, or destroy your systems and data. Impact consists of techniques that adversaries use to disrupt availability or compromise integrity by manipulating business and operational processes. Techniques used for impact can include destroying or tampering with data. In some cases, business processes can look fine, but may have been altered to benefit the adversaries' goals. These techniques might be used by adversaries to follow through on their end goal or to provide cover for a confidentiality breach.","title":"Description"},{"location":"tactics/AML.TA0011.html#techniques","text":"Evade ML Model Denial of ML Service Spamming ML System with Chaff Data Erode ML Model Integrity Cost Harvesting ML Intellectual Property Theft","title":"Techniques"},{"location":"techniques/index.html","text":"Techniques Techniques describe the means by which adversaries achieve tactical goals. They represent \u201chow\u201d an adversary achieves a tactical objective by performing an action. For example, an adversary may gain initial access by compromising the machine learning (ML) supply chain. Techniques may also represent \u201cwhat\u201d an adversary gains by performing an action. This is a useful distinction for the ML Attack Staging tactic, where the adversary is typically creating or modifying an ML artifact that will be used in a subsequent tactical objective. There can be multiple techniques in each tactic category as there are many ways to achieve tactical objectives. [1] The table below lists techniques from MITRE ATLAS\u2122. Scroll through the table or use the sidebar to access more information. ID Name Description AML.T0000 Search for Victim's Publicly Available Research Materials Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization. The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective. Organizations often use open source model architectures trained on additional proprietary data in production. Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ( Create Proxy ML Model ). An adversary can search these resources for publications for authors employed at the victim organization. Research materials may exist as academic papers published in Journals and Conference Proceedings , or stored in Pre-Print Repositories , as well as Technical Blogs . AML.T0000.000 Journals and Conference Proceedings Many of the publications accepted at premier machine learning conferences and journals come from commercial labs. Some journals and conferences are open access, others may require paying for access or a membership. These publications will often describe in detail all aspects of a particular approach for reproducibility. This information can be used by adversaries to implement the paper. AML.T0000.001 Pre-Print Repositories Pre-Print repositories, such as arXiv, contain the latest academic research papers that haven't been peer reviewed. They may contain research notes, or technical reports that aren't typically published in journals or conference proceedings. Pre-print repositories also serve as a central location to share papers that have been accepted to journals. Searching pre-print repositories provide adversaries with a relatively up-to-date view of what researchers in the victim organization are working on. AML.T0000.002 Technical Blogs Research labs at academic institutions and Company R&D divisions often have blogs that highlight their use of machine learning and its application to the organizations unique problems. Individual researchers also frequently document their work in blogposts. An adversary may search for posts made by the target victim organization or its employees. In comparison to Journals and Conference Proceedings and Pre-Print Repositories this material will often contain more practical aspects of the machine learning system. This could include underlying technologies and frameworks used, and possibly some information about the API access and use case. This will help the adversary better understand how that organization is using machine learning internally and the details of their approach that could aid in tailoring an attack. AML.T0001 Search for Publicly Available Adversarial Vulnerability Analysis Much like the Search for Victim's Publicly Available Research Materials , there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models. This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may Adversarial ML Attack Implementations or Develop Adversarial ML Attack Capabilities their own if necessary. AML.T0003 Search Victim-Owned Websites Adversaries may search websites owned by the victim for information that can be used during targeting. Victim-owned websites may contain technical details about their ML-enabled products or services. Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info. These sites may also have details highlighting business operations and relationships. Adversaries may search victim-owned websites to gather actionable information. This information may help adversaries tailor their attacks (e.g. Develop Adversarial ML Attack Capabilities or Manual Modification ). Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. Search for Victim's Publicly Available Research Materials or Search for Publicly Available Adversarial Vulnerability Analysis ) AML.T0004 Search Application Repositories Adversaries may search open application repositories during targeting. Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store. Adversaries may craft search queries seeking applications that contain a ML-enabled components. Frequently, the next step is to Acquire Public ML Artifacts . AML.T0006 Active Scanning An adversary may probe or scan the victim system to gather information for targeting. This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system. AML.T0002 Acquire Public ML Artifacts Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts. These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters. An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment. Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. Search Victim-Owned Websites or Search for Victim's Publicly Available Research Materials ). These ML artifacts often provide adversaries with details of the ML task and approach. ML artifacts can aid in an adversary's ability to Create Proxy ML Model . If these artifacts include pieces of the actual model in production, they can be used to directly Craft Adversarial Data . Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to Establish Accounts . Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data. AML.T0002.000 Datasets Adversaries may collect public datasets to use in their operations. Datasets used by the victim organization or datasets that are representative of the data used by the victim organization may be valuable to adversaries. Datasets can be stored in cloud storage, or on victim-owned websites. Some datasets require the adversary to Establish Accounts for access. Acquired datasets help the adversary advance their operations, stage attacks, and tailor attacks to the victim organization. AML.T0002.001 Models Adversaries may acquire public models to use in their operations. Adversaries may seek models used by the victim organization or models that are representative of those used by the victim organization. Representative models may include model architectures, or pre-trained models which define the architecture as well as model parameters from training on a dataset. The adversary may search public sources for common model architecture configuration file formats such as yaml or python configuration files, and common model storage file formats such as ONNX (.onnx), HDF5 (.h5), Pickle (.pkl), PyTorch (.pth), or TensorFlow (.pb, .tflite). Acquired models are useful in advancing the adversary's operations and are frequently used to tailor attacks to the victim model. AML.T0016 Obtain Capabilities Adversaries may search for and obtain software capabilities for use in their operations. Capabilities may be specific to ML-based attacks Adversarial ML Attack Implementations or generic software tools repurposed for malicious intent ( Software Tools ). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system. AML.T0016.000 Adversarial ML Attack Implementations Adversaries may search for existing open source implementations of machine learning attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial ML attacks as part of their attack. AML.T0016.001 Software Tools Adversaries may search for and obtain software tools to support their operations. Software designed for legitimate use may be repurposed by an adversary for malicious intent. An adversary may modify or customize software tools to achieve their purpose. Software tools used to support attacks on ML systems are not necessarily ML-based themselves. AML.T0017 Develop Adversarial ML Attack Capabilities Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ( Adversarial ML Attack Implementations ). They may implement ideas described in public research papers or develop custom made attacks for the victim model. AML.T0008 Acquire Infrastructure Adversaries may buy, lease, or rent infrastructure for use throughout their operation. A wide variety of infrastructure exists for hosting and orchestrating adversary operations. Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services. Free resources may also be used, but they are typically limited. Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation. Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services. Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down. AML.T0008.000 ML Development Workspaces Developing and staging machine learning attacks often requires expensive compute resources. Adversaries may need access to one or many GPUs in order to develop an attack. They may try to anonymously use free resources such as Google Colaboratory, or cloud resources such as AWS, Azure, or Google Cloud as an efficient way to stand up temporary resources to conduct operations. Multiple workspaces may be used to avoid detection. AML.T0008.001 Consumer Hardware Adversaries may acquire consumer hardware to conduct their attacks. Owning the hardware provides the adversary with complete control of the environment. These devices can be hard to trace. AML.T0019 Publish Poisoned Datasets Adversaries may Poison Training Data and publish it to a public location. The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset. This data may be introduced to a victim system via ML Supply Chain Compromise . AML.T0010 ML Supply Chain Compromise Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain. This could include GPU Hardware , Data and its annotations, parts of the ML ML Software stack, or the Model itself. In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain. AML.T0010.000 GPU Hardware Most machine learning systems require access to certain specialized hardware, typically GPUs. Adversaries can target machine learning systems by specifically targeting the GPU supply chain. AML.T0010.001 ML Software Most machine learning systems rely on a limited set of machine learning frameworks. An adversary could get access to a large number of machine learning systems through a comprise of one of their supply chains. Many machine learning projects also rely on other open source implementations of various algorithms. These can also be compromised in a targeted way to get access to specific systems. AML.T0010.002 Data Data is a key vector of supply chain compromise for adversaries. Every machine learning project will require some form of data. Many rely on large open source datasets that are publicly available. An adversary could rely on compromising these sources of data. The malicious data could be a result of Poison Training Data or include traditional malware. An adversary can also target private datasets in the labeling phase. The creation of private datasets will often require the hiring of outside labeling services. An adversary can poison a dataset by modifying the labels being generated by the labeling service. AML.T0010.003 Model Machine learning systems often rely on open sourced models in various ways. Most commonly, the victim organization may be using these models for fine tuning. These models will be downloaded from an external source and then used as the base for the model as it is tuned on a smaller, private dataset. Loading models often requires executing some saved code in the form of a saved model file. These can be compromised with traditional malware, or through some adversarial machine learning techniques. AML.T0040 ML Model Inference API Access Adversaries may gain access to a model via legitimate access to the inference API. Inference API access can be a source of information to the adversary ( Discover ML Model Ontology , Discover ML Model Family ), a means of staging the attack ( Verify Attack , Craft Adversarial Data ), or for introducing data to the target system for Impact ( Evade ML Model , Erode ML Model Integrity ). AML.T0047 ML-Enabled Product or Service Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model. This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata. AML.T0041 Physical Environment Access In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks. If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected. By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access. AML.T0044 Full ML Model Access Adversaries may gain full \"white-box\" access to a machine learning model. This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology. They may exfiltrate the model to Craft Adversarial Data and Verify Attack in an offline where it is hard to detect their behavior. AML.T0013 Discover ML Model Ontology Adversaries may discover the ontology of a machine learning model's output space, for example, the types of objects a model can detect. The adversary may discovery the ontology by repeated queries to the model, forcing it to enumerate its output space. Or the ontology may be discovered in a configuration file or in documentation about the model. The model ontology helps the adversary understand how the model is being used by the victim. It is useful to the adversary in creating targeted attacks. AML.T0014 Discover ML Model Family Adversaries may discover the general family of model. General information about the model may be revealed in documentation, or the adversary may used carefully constructed examples and analyze the model's responses to categorize it. Knowledge of the model family can help the adversary identify means of attacking the model and help tailor the attack. AML.T0020 Poison Training Data Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels. This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable. Data poisoning attacks may or may not require modifying the labels. The embedded vulnerability is activated at a later time by data samples with an Insert Backdoor Trigger Poisoned data can be introduced via ML Supply Chain Compromise or the data may be poisoned after the adversary gains Initial Access to the system. AML.T0021 Establish Accounts Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in ML Attack Staging , or for victim impersonation. AML.T0005 Create Proxy ML Model Adversaries may obtain models to serve as proxies for the target model in use at the victim organization. Proxy models are used to simulate complete access to the target model in a fully offline manner. Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models. AML.T0005.000 Train Proxy via Gathered ML Artifacts Proxy models may be trained from ML artifacts (such as data, model architectures, and pre-trained models) that are representative of the target model gathered by the adversary. This can be used to develop attacks that require higher levels of access than the adversary has available or as a means to validate pre-existing attacks without interacting with the target model. AML.T0005.001 Train Proxy via Replication Adversaries may replicate a private model. By repeatedly querying the victim's ML Model Inference API Access , the adversary can collect the target model's inferences into a dataset. The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model. A replicated model that closely mimic's the target model is a valuable resource in staging the attack. The adversary can use the replicated model to Craft Adversarial Data for various purposes (e.g. Evade ML Model , Spamming ML System with Chaff Data ). AML.T0005.002 Use Pre-Trained Model Adversaries may use an off-the-shelf pre-trained model as a proxy for the victim model to aid in staging the attack. AML.T0007 Discover ML Artifacts Adversaries may search private sources to identify machine learning artifacts that exist on the system and gather information about them. These artifacts can include the software stack used to train and deploy models, training and testing data management systems, container registries, software repositories, and model zoos. This information can be used to identify targets for further collection, exfiltration, or disruption, and to tailor and improve attacks. AML.T0011 User Execution An adversary may rely upon specific actions by a user in order to gain execution. Users may inadvertently execute unsafe code introduced via ML Supply Chain Compromise . Users may be subjected to social engineering to get them to execute malicious code by, for example, opening a malicious document file or link. AML.T0011.000 Unsafe ML Artifacts Adversaries may develop unsafe ML artifacts that when executed have a deleterious effect. The adversary can use this technique to establish persistent access to systems. These models may be introduced via a ML Supply Chain Compromise . Serialization of models is a popular technique for model storage, transfer, and loading. However, this format without proper checking presents an opportunity for code execution. AML.T0012 Valid Accounts Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access. Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services. Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform Discover ML Artifacts . Compromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production. AML.T0015 Evade ML Model Adversaries can Craft Adversarial Data that prevent a machine learning model from correctly identifying the contents of the data. This technique can be used to evade a downstream task where machine learning is utilized. The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack. AML.T0018 Backdoor ML Model Adversaries may introduce a backdoor into a ML model. A backdoored model operates performs as expected under typical conditions, but will produce the adversary's desired output when a trigger is introduced to the input data. A backdoored model provides the adversary with a persistent artifact on the victim system. The embedded vulnerability is typically activated at a later time by data samples with an Insert Backdoor Trigger AML.T0018.000 Poison ML Model Adversaries may introduce a backdoor by training the model poisoned data, or by interfering with its training process. The model learns to associate a adversary defined trigger with the adversary's desired output. AML.T0018.001 Inject Payload Adversaries may introduce a backdoor into a model by injecting a payload into the model file. The payload detects the presence of the trigger and bypasses the model, instead producing the adversary's desired output. AML.T0024 Exfiltration via ML Inference API Adversaries may exfiltrate private information via ML Model Inference API Access . ML Models have been shown leak private information about their training data (e.g. Infer Training Data Membership , Invert ML Model ). The model itself may also be extracted ( Extract ML Model ) for the purposes of ML Intellectual Property Theft . Exfiltration of information relating to private training data raises privacy concerns. Private training data may include personally identifiable information, or other protected data. AML.T0024.000 Infer Training Data Membership Adversaries may infer the membership of a data sample in its training set, which raises privacy concerns. Some strategies make use of a shadow model that could be obtained via Train Proxy via Replication , others use statistics of model prediction scores. This can cause the victim model to leak private information, such as PII of those in the training set or other forms of protected IP. AML.T0024.001 Invert ML Model Machine learning models' training data could be reconstructed by exploiting the confidence scores that are available via an inference API. By querying the inference API strategically, adversaries can back out potentially private information embedded within the training data. This could lead to privacy violations if the attacker can reconstruct the data of sensitive features used in the algorithm. AML.T0024.002 Extract ML Model Adversaries may extract a functional copy of a private model. By repeatedly querying the victim's ML Model Inference API Access , the adversary can collect the target model's inferences into a dataset. The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model. Adversaries may extract the model to avoid paying per query in a machine learning as a service setting. Model extraction is used for ML Intellectual Property Theft . AML.T0025 Exfiltration via Cyber Means Adversaries may exfiltrate ML artifacts or other information relevant to their goals via traditional cyber means. See the ATT&CK Exfiltration tactic for more information. AML.T0029 Denial of ML Service Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service. Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded. Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system. AML.T0046 Spamming ML System with Chaff Data Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections. This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences. AML.T0031 Erode ML Model Integrity Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time. This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand. AML.T0034 Cost Harvesting Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization. Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost. AML.T0035 ML Artifact Collection Adversaries may collect ML artifacts for Exfiltration or for use in ML Attack Staging . ML artifacts include models and datasets as well as other telemetry data produced when interacting with a model. AML.T0036 Data from Information Repositories Adversaries may leverage information repositories to mine valuable information. Information repositories are tools that allow for storage of information, typically to facilitate collaboration or information sharing between users, and can store a wide variety of data that may aid adversaries in further objectives, or direct access to the target information. Information stored in a repository may vary based on the specific instance or environment. Specific common information repositories include Sharepoint, Confluence, and enterprise databases such as SQL Server. AML.T0042 Verify Attack Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model. This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing. The adversary may verify the attack once but use it against many edge devices running copies of the target model. The adversary may verify their attack digitally, then deploy it in the Physical Environment Access at a later time. Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model. AML.T0043 Craft Adversarial Data Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model. Effects can range from misclassification, to missed detections, to maximising energy consumption. Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect. For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class. Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization , Black-Box Optimization , Black-Box Transfer , or Manual Modification . The adversary may Verify Attack their approach works if they have white-box or inference API access to the model. This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed. They can then use the attack at a later time to accomplish their goals. An adversary may optimize adversarial examples for Evade ML Model , or to Erode ML Model Integrity . AML.T0043.000 White-Box Optimization In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly. Adversarial examples trained in this manor are most effective against the target model. AML.T0043.001 Black-Box Optimization In Black-Box attacks, the adversary has black-box (i.e. ML Model Inference API Access via API access) access to the target model. With black-box attacks, the adversary may be using an API that the victim is monitoring. These attacks are generally less effective and require more inferences than White-Box Optimization attacks, but they require much less access. AML.T0043.002 Black-Box Transfer In Black-Box Transfer attacks, the adversary uses one or more proxy models (trained via Create Proxy ML Model or Train Proxy via Replication ) models they have full access to and are representative of the target model. The adversary uses White-Box Optimization on the proxy models to generate adversarial examples. If the set of proxy models are close enough to the target model, the adversarial example should generalize from one to another. This means that an attack that works for the proxy models will likely then work for the target model. If the adversary has ML Model Inference API Access , they may use this Verify Attack that the attack is working and incorporate that information into their training process. AML.T0043.003 Manual Modification Adversaries may manually modify the input data to craft adversarial data. They may use their knowledge of the target model to modify parts of the data they suspect helps the model in performing its task. The adversary may use trial and error until they are able to verify they have a working adversarial input. AML.T0043.004 Insert Backdoor Trigger The adversary may add a perceptual trigger into inference data. The trigger may be imperceptible or non-obvious to humans. This technique is used in conjunction with Poison ML Model and allows the adversary to produce their desired effect in the target model. AML.T0045 ML Intellectual Property Theft Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization. Proprietary training data is costly to collect and annotate and may be a target for Exfiltration and theft. MLaaS providers charge for use of their API. An adversary who has stolen a model via Exfiltration or via Extract ML Model now has unlimited use of that service without paying the owner of the intellectual property.","title":"Techniques"},{"location":"techniques/index.html#techniques","text":"Techniques describe the means by which adversaries achieve tactical goals. They represent \u201chow\u201d an adversary achieves a tactical objective by performing an action. For example, an adversary may gain initial access by compromising the machine learning (ML) supply chain. Techniques may also represent \u201cwhat\u201d an adversary gains by performing an action. This is a useful distinction for the ML Attack Staging tactic, where the adversary is typically creating or modifying an ML artifact that will be used in a subsequent tactical objective. There can be multiple techniques in each tactic category as there are many ways to achieve tactical objectives. [1] The table below lists techniques from MITRE ATLAS\u2122. Scroll through the table or use the sidebar to access more information. ID Name Description AML.T0000 Search for Victim's Publicly Available Research Materials Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization. The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective. Organizations often use open source model architectures trained on additional proprietary data in production. Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ( Create Proxy ML Model ). An adversary can search these resources for publications for authors employed at the victim organization. Research materials may exist as academic papers published in Journals and Conference Proceedings , or stored in Pre-Print Repositories , as well as Technical Blogs . AML.T0000.000 Journals and Conference Proceedings Many of the publications accepted at premier machine learning conferences and journals come from commercial labs. Some journals and conferences are open access, others may require paying for access or a membership. These publications will often describe in detail all aspects of a particular approach for reproducibility. This information can be used by adversaries to implement the paper. AML.T0000.001 Pre-Print Repositories Pre-Print repositories, such as arXiv, contain the latest academic research papers that haven't been peer reviewed. They may contain research notes, or technical reports that aren't typically published in journals or conference proceedings. Pre-print repositories also serve as a central location to share papers that have been accepted to journals. Searching pre-print repositories provide adversaries with a relatively up-to-date view of what researchers in the victim organization are working on. AML.T0000.002 Technical Blogs Research labs at academic institutions and Company R&D divisions often have blogs that highlight their use of machine learning and its application to the organizations unique problems. Individual researchers also frequently document their work in blogposts. An adversary may search for posts made by the target victim organization or its employees. In comparison to Journals and Conference Proceedings and Pre-Print Repositories this material will often contain more practical aspects of the machine learning system. This could include underlying technologies and frameworks used, and possibly some information about the API access and use case. This will help the adversary better understand how that organization is using machine learning internally and the details of their approach that could aid in tailoring an attack. AML.T0001 Search for Publicly Available Adversarial Vulnerability Analysis Much like the Search for Victim's Publicly Available Research Materials , there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models. This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may Adversarial ML Attack Implementations or Develop Adversarial ML Attack Capabilities their own if necessary. AML.T0003 Search Victim-Owned Websites Adversaries may search websites owned by the victim for information that can be used during targeting. Victim-owned websites may contain technical details about their ML-enabled products or services. Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info. These sites may also have details highlighting business operations and relationships. Adversaries may search victim-owned websites to gather actionable information. This information may help adversaries tailor their attacks (e.g. Develop Adversarial ML Attack Capabilities or Manual Modification ). Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. Search for Victim's Publicly Available Research Materials or Search for Publicly Available Adversarial Vulnerability Analysis ) AML.T0004 Search Application Repositories Adversaries may search open application repositories during targeting. Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store. Adversaries may craft search queries seeking applications that contain a ML-enabled components. Frequently, the next step is to Acquire Public ML Artifacts . AML.T0006 Active Scanning An adversary may probe or scan the victim system to gather information for targeting. This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system. AML.T0002 Acquire Public ML Artifacts Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts. These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters. An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment. Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. Search Victim-Owned Websites or Search for Victim's Publicly Available Research Materials ). These ML artifacts often provide adversaries with details of the ML task and approach. ML artifacts can aid in an adversary's ability to Create Proxy ML Model . If these artifacts include pieces of the actual model in production, they can be used to directly Craft Adversarial Data . Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to Establish Accounts . Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data. AML.T0002.000 Datasets Adversaries may collect public datasets to use in their operations. Datasets used by the victim organization or datasets that are representative of the data used by the victim organization may be valuable to adversaries. Datasets can be stored in cloud storage, or on victim-owned websites. Some datasets require the adversary to Establish Accounts for access. Acquired datasets help the adversary advance their operations, stage attacks, and tailor attacks to the victim organization. AML.T0002.001 Models Adversaries may acquire public models to use in their operations. Adversaries may seek models used by the victim organization or models that are representative of those used by the victim organization. Representative models may include model architectures, or pre-trained models which define the architecture as well as model parameters from training on a dataset. The adversary may search public sources for common model architecture configuration file formats such as yaml or python configuration files, and common model storage file formats such as ONNX (.onnx), HDF5 (.h5), Pickle (.pkl), PyTorch (.pth), or TensorFlow (.pb, .tflite). Acquired models are useful in advancing the adversary's operations and are frequently used to tailor attacks to the victim model. AML.T0016 Obtain Capabilities Adversaries may search for and obtain software capabilities for use in their operations. Capabilities may be specific to ML-based attacks Adversarial ML Attack Implementations or generic software tools repurposed for malicious intent ( Software Tools ). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system. AML.T0016.000 Adversarial ML Attack Implementations Adversaries may search for existing open source implementations of machine learning attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial ML attacks as part of their attack. AML.T0016.001 Software Tools Adversaries may search for and obtain software tools to support their operations. Software designed for legitimate use may be repurposed by an adversary for malicious intent. An adversary may modify or customize software tools to achieve their purpose. Software tools used to support attacks on ML systems are not necessarily ML-based themselves. AML.T0017 Develop Adversarial ML Attack Capabilities Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ( Adversarial ML Attack Implementations ). They may implement ideas described in public research papers or develop custom made attacks for the victim model. AML.T0008 Acquire Infrastructure Adversaries may buy, lease, or rent infrastructure for use throughout their operation. A wide variety of infrastructure exists for hosting and orchestrating adversary operations. Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services. Free resources may also be used, but they are typically limited. Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation. Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services. Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down. AML.T0008.000 ML Development Workspaces Developing and staging machine learning attacks often requires expensive compute resources. Adversaries may need access to one or many GPUs in order to develop an attack. They may try to anonymously use free resources such as Google Colaboratory, or cloud resources such as AWS, Azure, or Google Cloud as an efficient way to stand up temporary resources to conduct operations. Multiple workspaces may be used to avoid detection. AML.T0008.001 Consumer Hardware Adversaries may acquire consumer hardware to conduct their attacks. Owning the hardware provides the adversary with complete control of the environment. These devices can be hard to trace. AML.T0019 Publish Poisoned Datasets Adversaries may Poison Training Data and publish it to a public location. The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset. This data may be introduced to a victim system via ML Supply Chain Compromise . AML.T0010 ML Supply Chain Compromise Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain. This could include GPU Hardware , Data and its annotations, parts of the ML ML Software stack, or the Model itself. In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain. AML.T0010.000 GPU Hardware Most machine learning systems require access to certain specialized hardware, typically GPUs. Adversaries can target machine learning systems by specifically targeting the GPU supply chain. AML.T0010.001 ML Software Most machine learning systems rely on a limited set of machine learning frameworks. An adversary could get access to a large number of machine learning systems through a comprise of one of their supply chains. Many machine learning projects also rely on other open source implementations of various algorithms. These can also be compromised in a targeted way to get access to specific systems. AML.T0010.002 Data Data is a key vector of supply chain compromise for adversaries. Every machine learning project will require some form of data. Many rely on large open source datasets that are publicly available. An adversary could rely on compromising these sources of data. The malicious data could be a result of Poison Training Data or include traditional malware. An adversary can also target private datasets in the labeling phase. The creation of private datasets will often require the hiring of outside labeling services. An adversary can poison a dataset by modifying the labels being generated by the labeling service. AML.T0010.003 Model Machine learning systems often rely on open sourced models in various ways. Most commonly, the victim organization may be using these models for fine tuning. These models will be downloaded from an external source and then used as the base for the model as it is tuned on a smaller, private dataset. Loading models often requires executing some saved code in the form of a saved model file. These can be compromised with traditional malware, or through some adversarial machine learning techniques. AML.T0040 ML Model Inference API Access Adversaries may gain access to a model via legitimate access to the inference API. Inference API access can be a source of information to the adversary ( Discover ML Model Ontology , Discover ML Model Family ), a means of staging the attack ( Verify Attack , Craft Adversarial Data ), or for introducing data to the target system for Impact ( Evade ML Model , Erode ML Model Integrity ). AML.T0047 ML-Enabled Product or Service Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model. This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata. AML.T0041 Physical Environment Access In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks. If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected. By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access. AML.T0044 Full ML Model Access Adversaries may gain full \"white-box\" access to a machine learning model. This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology. They may exfiltrate the model to Craft Adversarial Data and Verify Attack in an offline where it is hard to detect their behavior. AML.T0013 Discover ML Model Ontology Adversaries may discover the ontology of a machine learning model's output space, for example, the types of objects a model can detect. The adversary may discovery the ontology by repeated queries to the model, forcing it to enumerate its output space. Or the ontology may be discovered in a configuration file or in documentation about the model. The model ontology helps the adversary understand how the model is being used by the victim. It is useful to the adversary in creating targeted attacks. AML.T0014 Discover ML Model Family Adversaries may discover the general family of model. General information about the model may be revealed in documentation, or the adversary may used carefully constructed examples and analyze the model's responses to categorize it. Knowledge of the model family can help the adversary identify means of attacking the model and help tailor the attack. AML.T0020 Poison Training Data Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels. This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable. Data poisoning attacks may or may not require modifying the labels. The embedded vulnerability is activated at a later time by data samples with an Insert Backdoor Trigger Poisoned data can be introduced via ML Supply Chain Compromise or the data may be poisoned after the adversary gains Initial Access to the system. AML.T0021 Establish Accounts Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in ML Attack Staging , or for victim impersonation. AML.T0005 Create Proxy ML Model Adversaries may obtain models to serve as proxies for the target model in use at the victim organization. Proxy models are used to simulate complete access to the target model in a fully offline manner. Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models. AML.T0005.000 Train Proxy via Gathered ML Artifacts Proxy models may be trained from ML artifacts (such as data, model architectures, and pre-trained models) that are representative of the target model gathered by the adversary. This can be used to develop attacks that require higher levels of access than the adversary has available or as a means to validate pre-existing attacks without interacting with the target model. AML.T0005.001 Train Proxy via Replication Adversaries may replicate a private model. By repeatedly querying the victim's ML Model Inference API Access , the adversary can collect the target model's inferences into a dataset. The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model. A replicated model that closely mimic's the target model is a valuable resource in staging the attack. The adversary can use the replicated model to Craft Adversarial Data for various purposes (e.g. Evade ML Model , Spamming ML System with Chaff Data ). AML.T0005.002 Use Pre-Trained Model Adversaries may use an off-the-shelf pre-trained model as a proxy for the victim model to aid in staging the attack. AML.T0007 Discover ML Artifacts Adversaries may search private sources to identify machine learning artifacts that exist on the system and gather information about them. These artifacts can include the software stack used to train and deploy models, training and testing data management systems, container registries, software repositories, and model zoos. This information can be used to identify targets for further collection, exfiltration, or disruption, and to tailor and improve attacks. AML.T0011 User Execution An adversary may rely upon specific actions by a user in order to gain execution. Users may inadvertently execute unsafe code introduced via ML Supply Chain Compromise . Users may be subjected to social engineering to get them to execute malicious code by, for example, opening a malicious document file or link. AML.T0011.000 Unsafe ML Artifacts Adversaries may develop unsafe ML artifacts that when executed have a deleterious effect. The adversary can use this technique to establish persistent access to systems. These models may be introduced via a ML Supply Chain Compromise . Serialization of models is a popular technique for model storage, transfer, and loading. However, this format without proper checking presents an opportunity for code execution. AML.T0012 Valid Accounts Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access. Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services. Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform Discover ML Artifacts . Compromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production. AML.T0015 Evade ML Model Adversaries can Craft Adversarial Data that prevent a machine learning model from correctly identifying the contents of the data. This technique can be used to evade a downstream task where machine learning is utilized. The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack. AML.T0018 Backdoor ML Model Adversaries may introduce a backdoor into a ML model. A backdoored model operates performs as expected under typical conditions, but will produce the adversary's desired output when a trigger is introduced to the input data. A backdoored model provides the adversary with a persistent artifact on the victim system. The embedded vulnerability is typically activated at a later time by data samples with an Insert Backdoor Trigger AML.T0018.000 Poison ML Model Adversaries may introduce a backdoor by training the model poisoned data, or by interfering with its training process. The model learns to associate a adversary defined trigger with the adversary's desired output. AML.T0018.001 Inject Payload Adversaries may introduce a backdoor into a model by injecting a payload into the model file. The payload detects the presence of the trigger and bypasses the model, instead producing the adversary's desired output. AML.T0024 Exfiltration via ML Inference API Adversaries may exfiltrate private information via ML Model Inference API Access . ML Models have been shown leak private information about their training data (e.g. Infer Training Data Membership , Invert ML Model ). The model itself may also be extracted ( Extract ML Model ) for the purposes of ML Intellectual Property Theft . Exfiltration of information relating to private training data raises privacy concerns. Private training data may include personally identifiable information, or other protected data. AML.T0024.000 Infer Training Data Membership Adversaries may infer the membership of a data sample in its training set, which raises privacy concerns. Some strategies make use of a shadow model that could be obtained via Train Proxy via Replication , others use statistics of model prediction scores. This can cause the victim model to leak private information, such as PII of those in the training set or other forms of protected IP. AML.T0024.001 Invert ML Model Machine learning models' training data could be reconstructed by exploiting the confidence scores that are available via an inference API. By querying the inference API strategically, adversaries can back out potentially private information embedded within the training data. This could lead to privacy violations if the attacker can reconstruct the data of sensitive features used in the algorithm. AML.T0024.002 Extract ML Model Adversaries may extract a functional copy of a private model. By repeatedly querying the victim's ML Model Inference API Access , the adversary can collect the target model's inferences into a dataset. The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model. Adversaries may extract the model to avoid paying per query in a machine learning as a service setting. Model extraction is used for ML Intellectual Property Theft . AML.T0025 Exfiltration via Cyber Means Adversaries may exfiltrate ML artifacts or other information relevant to their goals via traditional cyber means. See the ATT&CK Exfiltration tactic for more information. AML.T0029 Denial of ML Service Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service. Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded. Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system. AML.T0046 Spamming ML System with Chaff Data Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections. This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences. AML.T0031 Erode ML Model Integrity Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time. This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand. AML.T0034 Cost Harvesting Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization. Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost. AML.T0035 ML Artifact Collection Adversaries may collect ML artifacts for Exfiltration or for use in ML Attack Staging . ML artifacts include models and datasets as well as other telemetry data produced when interacting with a model. AML.T0036 Data from Information Repositories Adversaries may leverage information repositories to mine valuable information. Information repositories are tools that allow for storage of information, typically to facilitate collaboration or information sharing between users, and can store a wide variety of data that may aid adversaries in further objectives, or direct access to the target information. Information stored in a repository may vary based on the specific instance or environment. Specific common information repositories include Sharepoint, Confluence, and enterprise databases such as SQL Server. AML.T0042 Verify Attack Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model. This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing. The adversary may verify the attack once but use it against many edge devices running copies of the target model. The adversary may verify their attack digitally, then deploy it in the Physical Environment Access at a later time. Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model. AML.T0043 Craft Adversarial Data Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model. Effects can range from misclassification, to missed detections, to maximising energy consumption. Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect. For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class. Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization , Black-Box Optimization , Black-Box Transfer , or Manual Modification . The adversary may Verify Attack their approach works if they have white-box or inference API access to the model. This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed. They can then use the attack at a later time to accomplish their goals. An adversary may optimize adversarial examples for Evade ML Model , or to Erode ML Model Integrity . AML.T0043.000 White-Box Optimization In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly. Adversarial examples trained in this manor are most effective against the target model. AML.T0043.001 Black-Box Optimization In Black-Box attacks, the adversary has black-box (i.e. ML Model Inference API Access via API access) access to the target model. With black-box attacks, the adversary may be using an API that the victim is monitoring. These attacks are generally less effective and require more inferences than White-Box Optimization attacks, but they require much less access. AML.T0043.002 Black-Box Transfer In Black-Box Transfer attacks, the adversary uses one or more proxy models (trained via Create Proxy ML Model or Train Proxy via Replication ) models they have full access to and are representative of the target model. The adversary uses White-Box Optimization on the proxy models to generate adversarial examples. If the set of proxy models are close enough to the target model, the adversarial example should generalize from one to another. This means that an attack that works for the proxy models will likely then work for the target model. If the adversary has ML Model Inference API Access , they may use this Verify Attack that the attack is working and incorporate that information into their training process. AML.T0043.003 Manual Modification Adversaries may manually modify the input data to craft adversarial data. They may use their knowledge of the target model to modify parts of the data they suspect helps the model in performing its task. The adversary may use trial and error until they are able to verify they have a working adversarial input. AML.T0043.004 Insert Backdoor Trigger The adversary may add a perceptual trigger into inference data. The trigger may be imperceptible or non-obvious to humans. This technique is used in conjunction with Poison ML Model and allows the adversary to produce their desired effect in the target model. AML.T0045 ML Intellectual Property Theft Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization. Proprietary training data is costly to collect and annotate and may be a target for Exfiltration and theft. MLaaS providers charge for use of their API. An adversary who has stolen a model via Exfiltration or via Extract ML Model now has unlimited use of that service without paying the owner of the intellectual property.","title":"Techniques"},{"location":"techniques/AML.T0000.000.html","text":"Search for Victim's Publicly Available Research Materials: Journals and Conference Proceedings Description Many of the publications accepted at premier machine learning conferences and journals come from commercial labs. Some journals and conferences are open access, others may require paying for access or a membership. These publications will often describe in detail all aspects of a particular approach for reproducibility. This information can be used by adversaries to implement the paper. Parent technique Search for Victim's Publicly Available Research Materials Tactics Reconnaissance","title":"Search for Victim's Publicly Available Research Materials: Journals and Conference Proceedings"},{"location":"techniques/AML.T0000.000.html#search-for-victims-publicly-available-research-materials-journals-and-conference-proceedings","text":"","title":"Search for Victim's Publicly Available Research Materials: Journals and Conference Proceedings"},{"location":"techniques/AML.T0000.000.html#description","text":"Many of the publications accepted at premier machine learning conferences and journals come from commercial labs. Some journals and conferences are open access, others may require paying for access or a membership. These publications will often describe in detail all aspects of a particular approach for reproducibility. This information can be used by adversaries to implement the paper.","title":"Description"},{"location":"techniques/AML.T0000.000.html#parent-technique","text":"Search for Victim's Publicly Available Research Materials","title":"Parent technique"},{"location":"techniques/AML.T0000.000.html#tactics","text":"Reconnaissance","title":"Tactics"},{"location":"techniques/AML.T0000.001.html","text":"Search for Victim's Publicly Available Research Materials: Pre-Print Repositories Description Pre-Print repositories, such as arXiv, contain the latest academic research papers that haven't been peer reviewed. They may contain research notes, or technical reports that aren't typically published in journals or conference proceedings. Pre-print repositories also serve as a central location to share papers that have been accepted to journals. Searching pre-print repositories provide adversaries with a relatively up-to-date view of what researchers in the victim organization are working on. Parent technique Search for Victim's Publicly Available Research Materials Tactics Reconnaissance","title":"Search for Victim's Publicly Available Research Materials: Pre-Print Repositories"},{"location":"techniques/AML.T0000.001.html#search-for-victims-publicly-available-research-materials-pre-print-repositories","text":"","title":"Search for Victim's Publicly Available Research Materials: Pre-Print Repositories"},{"location":"techniques/AML.T0000.001.html#description","text":"Pre-Print repositories, such as arXiv, contain the latest academic research papers that haven't been peer reviewed. They may contain research notes, or technical reports that aren't typically published in journals or conference proceedings. Pre-print repositories also serve as a central location to share papers that have been accepted to journals. Searching pre-print repositories provide adversaries with a relatively up-to-date view of what researchers in the victim organization are working on.","title":"Description"},{"location":"techniques/AML.T0000.001.html#parent-technique","text":"Search for Victim's Publicly Available Research Materials","title":"Parent technique"},{"location":"techniques/AML.T0000.001.html#tactics","text":"Reconnaissance","title":"Tactics"},{"location":"techniques/AML.T0000.002.html","text":"Search for Victim's Publicly Available Research Materials: Technical Blogs Description Research labs at academic institutions and Company R&D divisions often have blogs that highlight their use of machine learning and its application to the organizations unique problems. Individual researchers also frequently document their work in blogposts. An adversary may search for posts made by the target victim organization or its employees. In comparison to Journals and Conference Proceedings and Pre-Print Repositories this material will often contain more practical aspects of the machine learning system. This could include underlying technologies and frameworks used, and possibly some information about the API access and use case. This will help the adversary better understand how that organization is using machine learning internally and the details of their approach that could aid in tailoring an attack. Parent technique Search for Victim's Publicly Available Research Materials Tactics Reconnaissance","title":"Search for Victim's Publicly Available Research Materials: Technical Blogs"},{"location":"techniques/AML.T0000.002.html#search-for-victims-publicly-available-research-materials-technical-blogs","text":"","title":"Search for Victim's Publicly Available Research Materials: Technical Blogs"},{"location":"techniques/AML.T0000.002.html#description","text":"Research labs at academic institutions and Company R&D divisions often have blogs that highlight their use of machine learning and its application to the organizations unique problems. Individual researchers also frequently document their work in blogposts. An adversary may search for posts made by the target victim organization or its employees. In comparison to Journals and Conference Proceedings and Pre-Print Repositories this material will often contain more practical aspects of the machine learning system. This could include underlying technologies and frameworks used, and possibly some information about the API access and use case. This will help the adversary better understand how that organization is using machine learning internally and the details of their approach that could aid in tailoring an attack.","title":"Description"},{"location":"techniques/AML.T0000.002.html#parent-technique","text":"Search for Victim's Publicly Available Research Materials","title":"Parent technique"},{"location":"techniques/AML.T0000.002.html#tactics","text":"Reconnaissance","title":"Tactics"},{"location":"techniques/AML.T0000.html","text":"Search for Victim's Publicly Available Research Materials Description Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization. The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective. Organizations often use open source model architectures trained on additional proprietary data in production. Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ( Create Proxy ML Model ). An adversary can search these resources for publications for authors employed at the victim organization. Research materials may exist as academic papers published in Journals and Conference Proceedings , or stored in Pre-Print Repositories , as well as Technical Blogs . Tactics Reconnaissance","title":"Search for Victim's Publicly Available Research Materials"},{"location":"techniques/AML.T0000.html#search-for-victims-publicly-available-research-materials","text":"","title":"Search for Victim's Publicly Available Research Materials"},{"location":"techniques/AML.T0000.html#description","text":"Adversaries may search publicly available research to learn how and where machine learning is used within a victim organization. The adversary can use this information to identify targets for attack, or to tailor an existing attack to make it more effective. Organizations often use open source model architectures trained on additional proprietary data in production. Knowledge of this underlying architecture allows the adversary to craft more realistic proxy models ( Create Proxy ML Model ). An adversary can search these resources for publications for authors employed at the victim organization. Research materials may exist as academic papers published in Journals and Conference Proceedings , or stored in Pre-Print Repositories , as well as Technical Blogs .","title":"Description"},{"location":"techniques/AML.T0000.html#tactics","text":"Reconnaissance","title":"Tactics"},{"location":"techniques/AML.T0001.html","text":"Search for Publicly Available Adversarial Vulnerability Analysis Description Much like the Search for Victim's Publicly Available Research Materials , there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models. This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may Adversarial ML Attack Implementations or Develop Adversarial ML Attack Capabilities their own if necessary. Tactics Reconnaissance","title":"Search for Publicly Available Adversarial Vulnerability Analysis"},{"location":"techniques/AML.T0001.html#search-for-publicly-available-adversarial-vulnerability-analysis","text":"","title":"Search for Publicly Available Adversarial Vulnerability Analysis"},{"location":"techniques/AML.T0001.html#description","text":"Much like the Search for Victim's Publicly Available Research Materials , there is often ample research available on the vulnerabilities of common models. Once a target has been identified, an adversary will likely try to identify any pre-existing work that has been done for this class of models. This will include not only reading academic papers that may identify the particulars of a successful attack, but also identifying pre-existing implementations of those attacks. The adversary may Adversarial ML Attack Implementations or Develop Adversarial ML Attack Capabilities their own if necessary.","title":"Description"},{"location":"techniques/AML.T0001.html#tactics","text":"Reconnaissance","title":"Tactics"},{"location":"techniques/AML.T0002.000.html","text":"Acquire Public ML Artifacts: Datasets Description Adversaries may collect public datasets to use in their operations. Datasets used by the victim organization or datasets that are representative of the data used by the victim organization may be valuable to adversaries. Datasets can be stored in cloud storage, or on victim-owned websites. Some datasets require the adversary to Establish Accounts for access. Acquired datasets help the adversary advance their operations, stage attacks, and tailor attacks to the victim organization. Parent technique Acquire Public ML Artifacts Tactics Resource Development","title":"Acquire Public ML Artifacts: Datasets"},{"location":"techniques/AML.T0002.000.html#acquire-public-ml-artifacts-datasets","text":"","title":"Acquire Public ML Artifacts: Datasets"},{"location":"techniques/AML.T0002.000.html#description","text":"Adversaries may collect public datasets to use in their operations. Datasets used by the victim organization or datasets that are representative of the data used by the victim organization may be valuable to adversaries. Datasets can be stored in cloud storage, or on victim-owned websites. Some datasets require the adversary to Establish Accounts for access. Acquired datasets help the adversary advance their operations, stage attacks, and tailor attacks to the victim organization.","title":"Description"},{"location":"techniques/AML.T0002.000.html#parent-technique","text":"Acquire Public ML Artifacts","title":"Parent technique"},{"location":"techniques/AML.T0002.000.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0002.001.html","text":"Acquire Public ML Artifacts: Models Description Adversaries may acquire public models to use in their operations. Adversaries may seek models used by the victim organization or models that are representative of those used by the victim organization. Representative models may include model architectures, or pre-trained models which define the architecture as well as model parameters from training on a dataset. The adversary may search public sources for common model architecture configuration file formats such as yaml or python configuration files, and common model storage file formats such as ONNX (.onnx), HDF5 (.h5), Pickle (.pkl), PyTorch (.pth), or TensorFlow (.pb, .tflite). Acquired models are useful in advancing the adversary's operations and are frequently used to tailor attacks to the victim model. Parent technique Acquire Public ML Artifacts Tactics Resource Development","title":"Acquire Public ML Artifacts: Models"},{"location":"techniques/AML.T0002.001.html#acquire-public-ml-artifacts-models","text":"","title":"Acquire Public ML Artifacts: Models"},{"location":"techniques/AML.T0002.001.html#description","text":"Adversaries may acquire public models to use in their operations. Adversaries may seek models used by the victim organization or models that are representative of those used by the victim organization. Representative models may include model architectures, or pre-trained models which define the architecture as well as model parameters from training on a dataset. The adversary may search public sources for common model architecture configuration file formats such as yaml or python configuration files, and common model storage file formats such as ONNX (.onnx), HDF5 (.h5), Pickle (.pkl), PyTorch (.pth), or TensorFlow (.pb, .tflite). Acquired models are useful in advancing the adversary's operations and are frequently used to tailor attacks to the victim model.","title":"Description"},{"location":"techniques/AML.T0002.001.html#parent-technique","text":"Acquire Public ML Artifacts","title":"Parent technique"},{"location":"techniques/AML.T0002.001.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0002.html","text":"Acquire Public ML Artifacts Description Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts. These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters. An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment. Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. Search Victim-Owned Websites or Search for Victim's Publicly Available Research Materials ). These ML artifacts often provide adversaries with details of the ML task and approach. ML artifacts can aid in an adversary's ability to Create Proxy ML Model . If these artifacts include pieces of the actual model in production, they can be used to directly Craft Adversarial Data . Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to Establish Accounts . Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data. Tactics Resource Development","title":"Acquire Public ML Artifacts"},{"location":"techniques/AML.T0002.html#acquire-public-ml-artifacts","text":"","title":"Acquire Public ML Artifacts"},{"location":"techniques/AML.T0002.html#description","text":"Adversaries may search public sources, including cloud storage, public-facing services, and software or data repositories, to identify machine learning artifacts. These machine learning artifacts may include the software stack used to train and deploy models, training and testing data, model configurations and parameters. An adversary will be particularly interested in artifacts hosted by or associated with the victim organization as they may represent what that organization uses in a production environment. Adversaries may identify artifact repositories via other resources associated with the victim organization (e.g. Search Victim-Owned Websites or Search for Victim's Publicly Available Research Materials ). These ML artifacts often provide adversaries with details of the ML task and approach. ML artifacts can aid in an adversary's ability to Create Proxy ML Model . If these artifacts include pieces of the actual model in production, they can be used to directly Craft Adversarial Data . Acquiring some artifacts requires registration (providing user details such email/name), AWS keys, or written requests, and may require the adversary to Establish Accounts . Artifacts might be hosted on victim-controlled infrastructure, providing the victim with some information on who has accessed that data.","title":"Description"},{"location":"techniques/AML.T0002.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0003.html","text":"Search Victim-Owned Websites Description Adversaries may search websites owned by the victim for information that can be used during targeting. Victim-owned websites may contain technical details about their ML-enabled products or services. Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info. These sites may also have details highlighting business operations and relationships. Adversaries may search victim-owned websites to gather actionable information. This information may help adversaries tailor their attacks (e.g. Develop Adversarial ML Attack Capabilities or Manual Modification ). Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. Search for Victim's Publicly Available Research Materials or Search for Publicly Available Adversarial Vulnerability Analysis ) Tactics Reconnaissance","title":"Search Victim-Owned Websites"},{"location":"techniques/AML.T0003.html#search-victim-owned-websites","text":"","title":"Search Victim-Owned Websites"},{"location":"techniques/AML.T0003.html#description","text":"Adversaries may search websites owned by the victim for information that can be used during targeting. Victim-owned websites may contain technical details about their ML-enabled products or services. Victim-owned websites may contain a variety of details, including names of departments/divisions, physical locations, and data about key employees such as names, roles, and contact info. These sites may also have details highlighting business operations and relationships. Adversaries may search victim-owned websites to gather actionable information. This information may help adversaries tailor their attacks (e.g. Develop Adversarial ML Attack Capabilities or Manual Modification ). Information from these sources may reveal opportunities for other forms of reconnaissance (e.g. Search for Victim's Publicly Available Research Materials or Search for Publicly Available Adversarial Vulnerability Analysis )","title":"Description"},{"location":"techniques/AML.T0003.html#tactics","text":"Reconnaissance","title":"Tactics"},{"location":"techniques/AML.T0004.html","text":"Search Application Repositories Description Adversaries may search open application repositories during targeting. Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store. Adversaries may craft search queries seeking applications that contain a ML-enabled components. Frequently, the next step is to Acquire Public ML Artifacts . Tactics Reconnaissance","title":"Search Application Repositories"},{"location":"techniques/AML.T0004.html#search-application-repositories","text":"","title":"Search Application Repositories"},{"location":"techniques/AML.T0004.html#description","text":"Adversaries may search open application repositories during targeting. Examples of these include Google Play, the iOS App store, the macOS App Store, and the Microsoft Store. Adversaries may craft search queries seeking applications that contain a ML-enabled components. Frequently, the next step is to Acquire Public ML Artifacts .","title":"Description"},{"location":"techniques/AML.T0004.html#tactics","text":"Reconnaissance","title":"Tactics"},{"location":"techniques/AML.T0005.000.html","text":"Create Proxy ML Model: Train Proxy via Gathered ML Artifacts Description Proxy models may be trained from ML artifacts (such as data, model architectures, and pre-trained models) that are representative of the target model gathered by the adversary. This can be used to develop attacks that require higher levels of access than the adversary has available or as a means to validate pre-existing attacks without interacting with the target model. Parent technique Create Proxy ML Model Tactics ML Attack Staging","title":"Create Proxy ML Model: Train Proxy via Gathered ML Artifacts"},{"location":"techniques/AML.T0005.000.html#create-proxy-ml-model-train-proxy-via-gathered-ml-artifacts","text":"","title":"Create Proxy ML Model: Train Proxy via Gathered ML Artifacts"},{"location":"techniques/AML.T0005.000.html#description","text":"Proxy models may be trained from ML artifacts (such as data, model architectures, and pre-trained models) that are representative of the target model gathered by the adversary. This can be used to develop attacks that require higher levels of access than the adversary has available or as a means to validate pre-existing attacks without interacting with the target model.","title":"Description"},{"location":"techniques/AML.T0005.000.html#parent-technique","text":"Create Proxy ML Model","title":"Parent technique"},{"location":"techniques/AML.T0005.000.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0005.001.html","text":"Create Proxy ML Model: Train Proxy via Replication Description Adversaries may replicate a private model. By repeatedly querying the victim's ML Model Inference API Access , the adversary can collect the target model's inferences into a dataset. The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model. A replicated model that closely mimic's the target model is a valuable resource in staging the attack. The adversary can use the replicated model to Craft Adversarial Data for various purposes (e.g. Evade ML Model , Spamming ML System with Chaff Data ). Parent technique Create Proxy ML Model Tactics ML Attack Staging","title":"Create Proxy ML Model: Train Proxy via Replication"},{"location":"techniques/AML.T0005.001.html#create-proxy-ml-model-train-proxy-via-replication","text":"","title":"Create Proxy ML Model: Train Proxy via Replication"},{"location":"techniques/AML.T0005.001.html#description","text":"Adversaries may replicate a private model. By repeatedly querying the victim's ML Model Inference API Access , the adversary can collect the target model's inferences into a dataset. The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model. A replicated model that closely mimic's the target model is a valuable resource in staging the attack. The adversary can use the replicated model to Craft Adversarial Data for various purposes (e.g. Evade ML Model , Spamming ML System with Chaff Data ).","title":"Description"},{"location":"techniques/AML.T0005.001.html#parent-technique","text":"Create Proxy ML Model","title":"Parent technique"},{"location":"techniques/AML.T0005.001.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0005.002.html","text":"Create Proxy ML Model: Use Pre-Trained Model Description Adversaries may use an off-the-shelf pre-trained model as a proxy for the victim model to aid in staging the attack. Parent technique Create Proxy ML Model Tactics ML Attack Staging","title":"Create Proxy ML Model: Use Pre-Trained Model"},{"location":"techniques/AML.T0005.002.html#create-proxy-ml-model-use-pre-trained-model","text":"","title":"Create Proxy ML Model: Use Pre-Trained Model"},{"location":"techniques/AML.T0005.002.html#description","text":"Adversaries may use an off-the-shelf pre-trained model as a proxy for the victim model to aid in staging the attack.","title":"Description"},{"location":"techniques/AML.T0005.002.html#parent-technique","text":"Create Proxy ML Model","title":"Parent technique"},{"location":"techniques/AML.T0005.002.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0005.html","text":"Create Proxy ML Model Description Adversaries may obtain models to serve as proxies for the target model in use at the victim organization. Proxy models are used to simulate complete access to the target model in a fully offline manner. Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models. Tactics ML Attack Staging","title":"Create Proxy ML Model"},{"location":"techniques/AML.T0005.html#create-proxy-ml-model","text":"","title":"Create Proxy ML Model"},{"location":"techniques/AML.T0005.html#description","text":"Adversaries may obtain models to serve as proxies for the target model in use at the victim organization. Proxy models are used to simulate complete access to the target model in a fully offline manner. Adversaries may train models from representative datasets, attempt to replicate models from victim inference APIs, or use available pre-trained models.","title":"Description"},{"location":"techniques/AML.T0005.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0006.html","text":"Active Scanning Description An adversary may probe or scan the victim system to gather information for targeting. This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system. Tactics Reconnaissance","title":"Active Scanning"},{"location":"techniques/AML.T0006.html#active-scanning","text":"","title":"Active Scanning"},{"location":"techniques/AML.T0006.html#description","text":"An adversary may probe or scan the victim system to gather information for targeting. This is distinct from other reconnaissance techniques that do not involve direct interaction with the victim system.","title":"Description"},{"location":"techniques/AML.T0006.html#tactics","text":"Reconnaissance","title":"Tactics"},{"location":"techniques/AML.T0007.html","text":"Discover ML Artifacts Description Adversaries may search private sources to identify machine learning artifacts that exist on the system and gather information about them. These artifacts can include the software stack used to train and deploy models, training and testing data management systems, container registries, software repositories, and model zoos. This information can be used to identify targets for further collection, exfiltration, or disruption, and to tailor and improve attacks. Tactics Discovery","title":"Discover ML Artifacts"},{"location":"techniques/AML.T0007.html#discover-ml-artifacts","text":"","title":"Discover ML Artifacts"},{"location":"techniques/AML.T0007.html#description","text":"Adversaries may search private sources to identify machine learning artifacts that exist on the system and gather information about them. These artifacts can include the software stack used to train and deploy models, training and testing data management systems, container registries, software repositories, and model zoos. This information can be used to identify targets for further collection, exfiltration, or disruption, and to tailor and improve attacks.","title":"Description"},{"location":"techniques/AML.T0007.html#tactics","text":"Discovery","title":"Tactics"},{"location":"techniques/AML.T0008.000.html","text":"Acquire Infrastructure: ML Development Workspaces Description Developing and staging machine learning attacks often requires expensive compute resources. Adversaries may need access to one or many GPUs in order to develop an attack. They may try to anonymously use free resources such as Google Colaboratory, or cloud resources such as AWS, Azure, or Google Cloud as an efficient way to stand up temporary resources to conduct operations. Multiple workspaces may be used to avoid detection. Parent technique Acquire Infrastructure Tactics Resource Development","title":"Acquire Infrastructure: ML Development Workspaces"},{"location":"techniques/AML.T0008.000.html#acquire-infrastructure-ml-development-workspaces","text":"","title":"Acquire Infrastructure: ML Development Workspaces"},{"location":"techniques/AML.T0008.000.html#description","text":"Developing and staging machine learning attacks often requires expensive compute resources. Adversaries may need access to one or many GPUs in order to develop an attack. They may try to anonymously use free resources such as Google Colaboratory, or cloud resources such as AWS, Azure, or Google Cloud as an efficient way to stand up temporary resources to conduct operations. Multiple workspaces may be used to avoid detection.","title":"Description"},{"location":"techniques/AML.T0008.000.html#parent-technique","text":"Acquire Infrastructure","title":"Parent technique"},{"location":"techniques/AML.T0008.000.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0008.001.html","text":"Acquire Infrastructure: Consumer Hardware Description Adversaries may acquire consumer hardware to conduct their attacks. Owning the hardware provides the adversary with complete control of the environment. These devices can be hard to trace. Parent technique Acquire Infrastructure Tactics Resource Development","title":"Acquire Infrastructure: Consumer Hardware"},{"location":"techniques/AML.T0008.001.html#acquire-infrastructure-consumer-hardware","text":"","title":"Acquire Infrastructure: Consumer Hardware"},{"location":"techniques/AML.T0008.001.html#description","text":"Adversaries may acquire consumer hardware to conduct their attacks. Owning the hardware provides the adversary with complete control of the environment. These devices can be hard to trace.","title":"Description"},{"location":"techniques/AML.T0008.001.html#parent-technique","text":"Acquire Infrastructure","title":"Parent technique"},{"location":"techniques/AML.T0008.001.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0008.html","text":"Acquire Infrastructure Description Adversaries may buy, lease, or rent infrastructure for use throughout their operation. A wide variety of infrastructure exists for hosting and orchestrating adversary operations. Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services. Free resources may also be used, but they are typically limited. Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation. Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services. Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down. Tactics Resource Development","title":"Acquire Infrastructure"},{"location":"techniques/AML.T0008.html#acquire-infrastructure","text":"","title":"Acquire Infrastructure"},{"location":"techniques/AML.T0008.html#description","text":"Adversaries may buy, lease, or rent infrastructure for use throughout their operation. A wide variety of infrastructure exists for hosting and orchestrating adversary operations. Infrastructure solutions include physical or cloud servers, domains, mobile devices, and third-party web services. Free resources may also be used, but they are typically limited. Use of these infrastructure solutions allows an adversary to stage, launch, and execute an operation. Solutions may help adversary operations blend in with traffic that is seen as normal, such as contact to third-party web services. Depending on the implementation, adversaries may use infrastructure that makes it difficult to physically tie back to them as well as utilize infrastructure that can be rapidly provisioned, modified, and shut down.","title":"Description"},{"location":"techniques/AML.T0008.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0010.000.html","text":"ML Supply Chain Compromise: GPU Hardware Description Most machine learning systems require access to certain specialized hardware, typically GPUs. Adversaries can target machine learning systems by specifically targeting the GPU supply chain. Parent technique ML Supply Chain Compromise Tactics Initial Access","title":"ML Supply Chain Compromise: GPU Hardware"},{"location":"techniques/AML.T0010.000.html#ml-supply-chain-compromise-gpu-hardware","text":"","title":"ML Supply Chain Compromise: GPU Hardware"},{"location":"techniques/AML.T0010.000.html#description","text":"Most machine learning systems require access to certain specialized hardware, typically GPUs. Adversaries can target machine learning systems by specifically targeting the GPU supply chain.","title":"Description"},{"location":"techniques/AML.T0010.000.html#parent-technique","text":"ML Supply Chain Compromise","title":"Parent technique"},{"location":"techniques/AML.T0010.000.html#tactics","text":"Initial Access","title":"Tactics"},{"location":"techniques/AML.T0010.001.html","text":"ML Supply Chain Compromise: ML Software Description Most machine learning systems rely on a limited set of machine learning frameworks. An adversary could get access to a large number of machine learning systems through a comprise of one of their supply chains. Many machine learning projects also rely on other open source implementations of various algorithms. These can also be compromised in a targeted way to get access to specific systems. Parent technique ML Supply Chain Compromise Tactics Initial Access","title":"ML Supply Chain Compromise: ML Software"},{"location":"techniques/AML.T0010.001.html#ml-supply-chain-compromise-ml-software","text":"","title":"ML Supply Chain Compromise: ML Software"},{"location":"techniques/AML.T0010.001.html#description","text":"Most machine learning systems rely on a limited set of machine learning frameworks. An adversary could get access to a large number of machine learning systems through a comprise of one of their supply chains. Many machine learning projects also rely on other open source implementations of various algorithms. These can also be compromised in a targeted way to get access to specific systems.","title":"Description"},{"location":"techniques/AML.T0010.001.html#parent-technique","text":"ML Supply Chain Compromise","title":"Parent technique"},{"location":"techniques/AML.T0010.001.html#tactics","text":"Initial Access","title":"Tactics"},{"location":"techniques/AML.T0010.002.html","text":"ML Supply Chain Compromise: Data Description Data is a key vector of supply chain compromise for adversaries. Every machine learning project will require some form of data. Many rely on large open source datasets that are publicly available. An adversary could rely on compromising these sources of data. The malicious data could be a result of Poison Training Data or include traditional malware. An adversary can also target private datasets in the labeling phase. The creation of private datasets will often require the hiring of outside labeling services. An adversary can poison a dataset by modifying the labels being generated by the labeling service. Parent technique ML Supply Chain Compromise Tactics Initial Access","title":"ML Supply Chain Compromise: Data"},{"location":"techniques/AML.T0010.002.html#ml-supply-chain-compromise-data","text":"","title":"ML Supply Chain Compromise: Data"},{"location":"techniques/AML.T0010.002.html#description","text":"Data is a key vector of supply chain compromise for adversaries. Every machine learning project will require some form of data. Many rely on large open source datasets that are publicly available. An adversary could rely on compromising these sources of data. The malicious data could be a result of Poison Training Data or include traditional malware. An adversary can also target private datasets in the labeling phase. The creation of private datasets will often require the hiring of outside labeling services. An adversary can poison a dataset by modifying the labels being generated by the labeling service.","title":"Description"},{"location":"techniques/AML.T0010.002.html#parent-technique","text":"ML Supply Chain Compromise","title":"Parent technique"},{"location":"techniques/AML.T0010.002.html#tactics","text":"Initial Access","title":"Tactics"},{"location":"techniques/AML.T0010.003.html","text":"ML Supply Chain Compromise: Model Description Machine learning systems often rely on open sourced models in various ways. Most commonly, the victim organization may be using these models for fine tuning. These models will be downloaded from an external source and then used as the base for the model as it is tuned on a smaller, private dataset. Loading models often requires executing some saved code in the form of a saved model file. These can be compromised with traditional malware, or through some adversarial machine learning techniques. Parent technique ML Supply Chain Compromise Tactics Initial Access","title":"ML Supply Chain Compromise: Model"},{"location":"techniques/AML.T0010.003.html#ml-supply-chain-compromise-model","text":"","title":"ML Supply Chain Compromise: Model"},{"location":"techniques/AML.T0010.003.html#description","text":"Machine learning systems often rely on open sourced models in various ways. Most commonly, the victim organization may be using these models for fine tuning. These models will be downloaded from an external source and then used as the base for the model as it is tuned on a smaller, private dataset. Loading models often requires executing some saved code in the form of a saved model file. These can be compromised with traditional malware, or through some adversarial machine learning techniques.","title":"Description"},{"location":"techniques/AML.T0010.003.html#parent-technique","text":"ML Supply Chain Compromise","title":"Parent technique"},{"location":"techniques/AML.T0010.003.html#tactics","text":"Initial Access","title":"Tactics"},{"location":"techniques/AML.T0010.html","text":"ML Supply Chain Compromise Description Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain. This could include GPU Hardware , Data and its annotations, parts of the ML ML Software stack, or the Model itself. In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain. Tactics Initial Access","title":"ML Supply Chain Compromise"},{"location":"techniques/AML.T0010.html#ml-supply-chain-compromise","text":"","title":"ML Supply Chain Compromise"},{"location":"techniques/AML.T0010.html#description","text":"Adversaries may gain initial access to a system by compromising the unique portions of the ML supply chain. This could include GPU Hardware , Data and its annotations, parts of the ML ML Software stack, or the Model itself. In some instances the attacker will need secondary access to fully carry out an attack using compromised components of the supply chain.","title":"Description"},{"location":"techniques/AML.T0010.html#tactics","text":"Initial Access","title":"Tactics"},{"location":"techniques/AML.T0011.000.html","text":"User Execution: Unsafe ML Artifacts Description Adversaries may develop unsafe ML artifacts that when executed have a deleterious effect. The adversary can use this technique to establish persistent access to systems. These models may be introduced via a ML Supply Chain Compromise . Serialization of models is a popular technique for model storage, transfer, and loading. However, this format without proper checking presents an opportunity for code execution. Parent technique User Execution Tactics Execution","title":"User Execution: Unsafe ML Artifacts"},{"location":"techniques/AML.T0011.000.html#user-execution-unsafe-ml-artifacts","text":"","title":"User Execution: Unsafe ML Artifacts"},{"location":"techniques/AML.T0011.000.html#description","text":"Adversaries may develop unsafe ML artifacts that when executed have a deleterious effect. The adversary can use this technique to establish persistent access to systems. These models may be introduced via a ML Supply Chain Compromise . Serialization of models is a popular technique for model storage, transfer, and loading. However, this format without proper checking presents an opportunity for code execution.","title":"Description"},{"location":"techniques/AML.T0011.000.html#parent-technique","text":"User Execution","title":"Parent technique"},{"location":"techniques/AML.T0011.000.html#tactics","text":"Execution","title":"Tactics"},{"location":"techniques/AML.T0011.html","text":"User Execution Description An adversary may rely upon specific actions by a user in order to gain execution. Users may inadvertently execute unsafe code introduced via ML Supply Chain Compromise . Users may be subjected to social engineering to get them to execute malicious code by, for example, opening a malicious document file or link. Tactics Execution","title":"User Execution"},{"location":"techniques/AML.T0011.html#user-execution","text":"","title":"User Execution"},{"location":"techniques/AML.T0011.html#description","text":"An adversary may rely upon specific actions by a user in order to gain execution. Users may inadvertently execute unsafe code introduced via ML Supply Chain Compromise . Users may be subjected to social engineering to get them to execute malicious code by, for example, opening a malicious document file or link.","title":"Description"},{"location":"techniques/AML.T0011.html#tactics","text":"Execution","title":"Tactics"},{"location":"techniques/AML.T0012.html","text":"Valid Accounts Description Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access. Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services. Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform Discover ML Artifacts . Compromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production. Tactics Initial Access","title":"Valid Accounts"},{"location":"techniques/AML.T0012.html#valid-accounts","text":"","title":"Valid Accounts"},{"location":"techniques/AML.T0012.html#description","text":"Adversaries may obtain and abuse credentials of existing accounts as a means of gaining Initial Access. Credentials may take the form of usernames and passwords of individual user accounts or API keys that provide access to various ML resources and services. Compromised credentials may provide access to additional ML artifacts and allow the adversary to perform Discover ML Artifacts . Compromised credentials may also grant and adversary increased privileges such as write access to ML artifacts used during development or production.","title":"Description"},{"location":"techniques/AML.T0012.html#tactics","text":"Initial Access","title":"Tactics"},{"location":"techniques/AML.T0013.html","text":"Discover ML Model Ontology Description Adversaries may discover the ontology of a machine learning model's output space, for example, the types of objects a model can detect. The adversary may discovery the ontology by repeated queries to the model, forcing it to enumerate its output space. Or the ontology may be discovered in a configuration file or in documentation about the model. The model ontology helps the adversary understand how the model is being used by the victim. It is useful to the adversary in creating targeted attacks. Tactics Discovery","title":"Discover ML Model Ontology"},{"location":"techniques/AML.T0013.html#discover-ml-model-ontology","text":"","title":"Discover ML Model Ontology"},{"location":"techniques/AML.T0013.html#description","text":"Adversaries may discover the ontology of a machine learning model's output space, for example, the types of objects a model can detect. The adversary may discovery the ontology by repeated queries to the model, forcing it to enumerate its output space. Or the ontology may be discovered in a configuration file or in documentation about the model. The model ontology helps the adversary understand how the model is being used by the victim. It is useful to the adversary in creating targeted attacks.","title":"Description"},{"location":"techniques/AML.T0013.html#tactics","text":"Discovery","title":"Tactics"},{"location":"techniques/AML.T0014.html","text":"Discover ML Model Family Description Adversaries may discover the general family of model. General information about the model may be revealed in documentation, or the adversary may used carefully constructed examples and analyze the model's responses to categorize it. Knowledge of the model family can help the adversary identify means of attacking the model and help tailor the attack. Tactics Discovery","title":"Discover ML Model Family"},{"location":"techniques/AML.T0014.html#discover-ml-model-family","text":"","title":"Discover ML Model Family"},{"location":"techniques/AML.T0014.html#description","text":"Adversaries may discover the general family of model. General information about the model may be revealed in documentation, or the adversary may used carefully constructed examples and analyze the model's responses to categorize it. Knowledge of the model family can help the adversary identify means of attacking the model and help tailor the attack.","title":"Description"},{"location":"techniques/AML.T0014.html#tactics","text":"Discovery","title":"Tactics"},{"location":"techniques/AML.T0015.html","text":"Evade ML Model Description Adversaries can Craft Adversarial Data that prevent a machine learning model from correctly identifying the contents of the data. This technique can be used to evade a downstream task where machine learning is utilized. The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack. Tactics Defense Evasion Impact","title":"Evade ML Model"},{"location":"techniques/AML.T0015.html#evade-ml-model","text":"","title":"Evade ML Model"},{"location":"techniques/AML.T0015.html#description","text":"Adversaries can Craft Adversarial Data that prevent a machine learning model from correctly identifying the contents of the data. This technique can be used to evade a downstream task where machine learning is utilized. The adversary may evade machine learning based virus/malware detection, or network scanning towards the goal of a traditional cyber attack.","title":"Description"},{"location":"techniques/AML.T0015.html#tactics","text":"Defense Evasion Impact","title":"Tactics"},{"location":"techniques/AML.T0016.000.html","text":"Obtain Capabilities: Adversarial ML Attack Implementations Description Adversaries may search for existing open source implementations of machine learning attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial ML attacks as part of their attack. Parent technique Obtain Capabilities Tactics Resource Development","title":"Obtain Capabilities: Adversarial ML Attack Implementations"},{"location":"techniques/AML.T0016.000.html#obtain-capabilities-adversarial-ml-attack-implementations","text":"","title":"Obtain Capabilities: Adversarial ML Attack Implementations"},{"location":"techniques/AML.T0016.000.html#description","text":"Adversaries may search for existing open source implementations of machine learning attacks. The research community often publishes their code for reproducibility and to further future research. Libraries intended for research purposes, such as CleverHans, the Adversarial Robustness Toolbox, and FoolBox, can be weaponized by an adversary. Adversaries may also obtain and use tools that were not originally designed for adversarial ML attacks as part of their attack.","title":"Description"},{"location":"techniques/AML.T0016.000.html#parent-technique","text":"Obtain Capabilities","title":"Parent technique"},{"location":"techniques/AML.T0016.000.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0016.001.html","text":"Obtain Capabilities: Software Tools Description Adversaries may search for and obtain software tools to support their operations. Software designed for legitimate use may be repurposed by an adversary for malicious intent. An adversary may modify or customize software tools to achieve their purpose. Software tools used to support attacks on ML systems are not necessarily ML-based themselves. Parent technique Obtain Capabilities Tactics Resource Development","title":"Obtain Capabilities: Software Tools"},{"location":"techniques/AML.T0016.001.html#obtain-capabilities-software-tools","text":"","title":"Obtain Capabilities: Software Tools"},{"location":"techniques/AML.T0016.001.html#description","text":"Adversaries may search for and obtain software tools to support their operations. Software designed for legitimate use may be repurposed by an adversary for malicious intent. An adversary may modify or customize software tools to achieve their purpose. Software tools used to support attacks on ML systems are not necessarily ML-based themselves.","title":"Description"},{"location":"techniques/AML.T0016.001.html#parent-technique","text":"Obtain Capabilities","title":"Parent technique"},{"location":"techniques/AML.T0016.001.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0016.html","text":"Obtain Capabilities Description Adversaries may search for and obtain software capabilities for use in their operations. Capabilities may be specific to ML-based attacks Adversarial ML Attack Implementations or generic software tools repurposed for malicious intent ( Software Tools ). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system. Tactics Resource Development","title":"Obtain Capabilities"},{"location":"techniques/AML.T0016.html#obtain-capabilities","text":"","title":"Obtain Capabilities"},{"location":"techniques/AML.T0016.html#description","text":"Adversaries may search for and obtain software capabilities for use in their operations. Capabilities may be specific to ML-based attacks Adversarial ML Attack Implementations or generic software tools repurposed for malicious intent ( Software Tools ). In both instances, an adversary may modify or customize the capability to aid in targeting a particular ML system.","title":"Description"},{"location":"techniques/AML.T0016.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0017.html","text":"Develop Adversarial ML Attack Capabilities Description Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ( Adversarial ML Attack Implementations ). They may implement ideas described in public research papers or develop custom made attacks for the victim model. Tactics Resource Development","title":"Develop Adversarial ML Attack Capabilities"},{"location":"techniques/AML.T0017.html#develop-adversarial-ml-attack-capabilities","text":"","title":"Develop Adversarial ML Attack Capabilities"},{"location":"techniques/AML.T0017.html#description","text":"Adversaries may develop their own adversarial attacks. They may leverage existing libraries as a starting point ( Adversarial ML Attack Implementations ). They may implement ideas described in public research papers or develop custom made attacks for the victim model.","title":"Description"},{"location":"techniques/AML.T0017.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0018.000.html","text":"Backdoor ML Model: Poison ML Model Description Adversaries may introduce a backdoor by training the model poisoned data, or by interfering with its training process. The model learns to associate a adversary defined trigger with the adversary's desired output. Parent technique Backdoor ML Model Tactics Persistence ML Attack Staging","title":"Backdoor ML Model: Poison ML Model"},{"location":"techniques/AML.T0018.000.html#backdoor-ml-model-poison-ml-model","text":"","title":"Backdoor ML Model: Poison ML Model"},{"location":"techniques/AML.T0018.000.html#description","text":"Adversaries may introduce a backdoor by training the model poisoned data, or by interfering with its training process. The model learns to associate a adversary defined trigger with the adversary's desired output.","title":"Description"},{"location":"techniques/AML.T0018.000.html#parent-technique","text":"Backdoor ML Model","title":"Parent technique"},{"location":"techniques/AML.T0018.000.html#tactics","text":"Persistence ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0018.001.html","text":"Backdoor ML Model: Inject Payload Description Adversaries may introduce a backdoor into a model by injecting a payload into the model file. The payload detects the presence of the trigger and bypasses the model, instead producing the adversary's desired output. Parent technique Backdoor ML Model Tactics Persistence ML Attack Staging","title":"Backdoor ML Model: Inject Payload"},{"location":"techniques/AML.T0018.001.html#backdoor-ml-model-inject-payload","text":"","title":"Backdoor ML Model: Inject Payload"},{"location":"techniques/AML.T0018.001.html#description","text":"Adversaries may introduce a backdoor into a model by injecting a payload into the model file. The payload detects the presence of the trigger and bypasses the model, instead producing the adversary's desired output.","title":"Description"},{"location":"techniques/AML.T0018.001.html#parent-technique","text":"Backdoor ML Model","title":"Parent technique"},{"location":"techniques/AML.T0018.001.html#tactics","text":"Persistence ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0018.html","text":"Backdoor ML Model Description Adversaries may introduce a backdoor into a ML model. A backdoored model operates performs as expected under typical conditions, but will produce the adversary's desired output when a trigger is introduced to the input data. A backdoored model provides the adversary with a persistent artifact on the victim system. The embedded vulnerability is typically activated at a later time by data samples with an Insert Backdoor Trigger Tactics Persistence ML Attack Staging","title":"Backdoor ML Model"},{"location":"techniques/AML.T0018.html#backdoor-ml-model","text":"","title":"Backdoor ML Model"},{"location":"techniques/AML.T0018.html#description","text":"Adversaries may introduce a backdoor into a ML model. A backdoored model operates performs as expected under typical conditions, but will produce the adversary's desired output when a trigger is introduced to the input data. A backdoored model provides the adversary with a persistent artifact on the victim system. The embedded vulnerability is typically activated at a later time by data samples with an Insert Backdoor Trigger","title":"Description"},{"location":"techniques/AML.T0018.html#tactics","text":"Persistence ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0019.html","text":"Publish Poisoned Datasets Description Adversaries may Poison Training Data and publish it to a public location. The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset. This data may be introduced to a victim system via ML Supply Chain Compromise . Tactics Resource Development","title":"Publish Poisoned Datasets"},{"location":"techniques/AML.T0019.html#publish-poisoned-datasets","text":"","title":"Publish Poisoned Datasets"},{"location":"techniques/AML.T0019.html#description","text":"Adversaries may Poison Training Data and publish it to a public location. The poisoned dataset may be a novel dataset or a poisoned variant of an existing open source dataset. This data may be introduced to a victim system via ML Supply Chain Compromise .","title":"Description"},{"location":"techniques/AML.T0019.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0020.html","text":"Poison Training Data Description Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels. This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable. Data poisoning attacks may or may not require modifying the labels. The embedded vulnerability is activated at a later time by data samples with an Insert Backdoor Trigger Poisoned data can be introduced via ML Supply Chain Compromise or the data may be poisoned after the adversary gains Initial Access to the system. Tactics Resource Development Persistence","title":"Poison Training Data"},{"location":"techniques/AML.T0020.html#poison-training-data","text":"","title":"Poison Training Data"},{"location":"techniques/AML.T0020.html#description","text":"Adversaries may attempt to poison datasets used by a ML model by modifying the underlying data or its labels. This allows the adversary to embed vulnerabilities in ML models trained on the data that may not be easily detectable. Data poisoning attacks may or may not require modifying the labels. The embedded vulnerability is activated at a later time by data samples with an Insert Backdoor Trigger Poisoned data can be introduced via ML Supply Chain Compromise or the data may be poisoned after the adversary gains Initial Access to the system.","title":"Description"},{"location":"techniques/AML.T0020.html#tactics","text":"Resource Development Persistence","title":"Tactics"},{"location":"techniques/AML.T0021.html","text":"Establish Accounts Description Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in ML Attack Staging , or for victim impersonation. Tactics Resource Development","title":"Establish Accounts"},{"location":"techniques/AML.T0021.html#establish-accounts","text":"","title":"Establish Accounts"},{"location":"techniques/AML.T0021.html#description","text":"Adversaries may create accounts with various services for use in targeting, to gain access to resources needed in ML Attack Staging , or for victim impersonation.","title":"Description"},{"location":"techniques/AML.T0021.html#tactics","text":"Resource Development","title":"Tactics"},{"location":"techniques/AML.T0024.000.html","text":"Exfiltration via ML Inference API: Infer Training Data Membership Description Adversaries may infer the membership of a data sample in its training set, which raises privacy concerns. Some strategies make use of a shadow model that could be obtained via Train Proxy via Replication , others use statistics of model prediction scores. This can cause the victim model to leak private information, such as PII of those in the training set or other forms of protected IP. Parent technique Exfiltration via ML Inference API Tactics Exfiltration","title":"Exfiltration via ML Inference API: Infer Training Data Membership"},{"location":"techniques/AML.T0024.000.html#exfiltration-via-ml-inference-api-infer-training-data-membership","text":"","title":"Exfiltration via ML Inference API: Infer Training Data Membership"},{"location":"techniques/AML.T0024.000.html#description","text":"Adversaries may infer the membership of a data sample in its training set, which raises privacy concerns. Some strategies make use of a shadow model that could be obtained via Train Proxy via Replication , others use statistics of model prediction scores. This can cause the victim model to leak private information, such as PII of those in the training set or other forms of protected IP.","title":"Description"},{"location":"techniques/AML.T0024.000.html#parent-technique","text":"Exfiltration via ML Inference API","title":"Parent technique"},{"location":"techniques/AML.T0024.000.html#tactics","text":"Exfiltration","title":"Tactics"},{"location":"techniques/AML.T0024.001.html","text":"Exfiltration via ML Inference API: Invert ML Model Description Machine learning models' training data could be reconstructed by exploiting the confidence scores that are available via an inference API. By querying the inference API strategically, adversaries can back out potentially private information embedded within the training data. This could lead to privacy violations if the attacker can reconstruct the data of sensitive features used in the algorithm. Parent technique Exfiltration via ML Inference API Tactics Exfiltration","title":"Exfiltration via ML Inference API: Invert ML Model"},{"location":"techniques/AML.T0024.001.html#exfiltration-via-ml-inference-api-invert-ml-model","text":"","title":"Exfiltration via ML Inference API: Invert ML Model"},{"location":"techniques/AML.T0024.001.html#description","text":"Machine learning models' training data could be reconstructed by exploiting the confidence scores that are available via an inference API. By querying the inference API strategically, adversaries can back out potentially private information embedded within the training data. This could lead to privacy violations if the attacker can reconstruct the data of sensitive features used in the algorithm.","title":"Description"},{"location":"techniques/AML.T0024.001.html#parent-technique","text":"Exfiltration via ML Inference API","title":"Parent technique"},{"location":"techniques/AML.T0024.001.html#tactics","text":"Exfiltration","title":"Tactics"},{"location":"techniques/AML.T0024.002.html","text":"Exfiltration via ML Inference API: Extract ML Model Description Adversaries may extract a functional copy of a private model. By repeatedly querying the victim's ML Model Inference API Access , the adversary can collect the target model's inferences into a dataset. The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model. Adversaries may extract the model to avoid paying per query in a machine learning as a service setting. Model extraction is used for ML Intellectual Property Theft . Parent technique Exfiltration via ML Inference API Tactics Exfiltration","title":"Exfiltration via ML Inference API: Extract ML Model"},{"location":"techniques/AML.T0024.002.html#exfiltration-via-ml-inference-api-extract-ml-model","text":"","title":"Exfiltration via ML Inference API: Extract ML Model"},{"location":"techniques/AML.T0024.002.html#description","text":"Adversaries may extract a functional copy of a private model. By repeatedly querying the victim's ML Model Inference API Access , the adversary can collect the target model's inferences into a dataset. The inferences are used as labels for training a separate model offline that will mimic the behavior and performance of the target model. Adversaries may extract the model to avoid paying per query in a machine learning as a service setting. Model extraction is used for ML Intellectual Property Theft .","title":"Description"},{"location":"techniques/AML.T0024.002.html#parent-technique","text":"Exfiltration via ML Inference API","title":"Parent technique"},{"location":"techniques/AML.T0024.002.html#tactics","text":"Exfiltration","title":"Tactics"},{"location":"techniques/AML.T0024.html","text":"Exfiltration via ML Inference API Description Adversaries may exfiltrate private information via ML Model Inference API Access . ML Models have been shown leak private information about their training data (e.g. Infer Training Data Membership , Invert ML Model ). The model itself may also be extracted ( Extract ML Model ) for the purposes of ML Intellectual Property Theft . Exfiltration of information relating to private training data raises privacy concerns. Private training data may include personally identifiable information, or other protected data. Tactics Exfiltration","title":"Exfiltration via ML Inference API"},{"location":"techniques/AML.T0024.html#exfiltration-via-ml-inference-api","text":"","title":"Exfiltration via ML Inference API"},{"location":"techniques/AML.T0024.html#description","text":"Adversaries may exfiltrate private information via ML Model Inference API Access . ML Models have been shown leak private information about their training data (e.g. Infer Training Data Membership , Invert ML Model ). The model itself may also be extracted ( Extract ML Model ) for the purposes of ML Intellectual Property Theft . Exfiltration of information relating to private training data raises privacy concerns. Private training data may include personally identifiable information, or other protected data.","title":"Description"},{"location":"techniques/AML.T0024.html#tactics","text":"Exfiltration","title":"Tactics"},{"location":"techniques/AML.T0025.html","text":"Exfiltration via Cyber Means Description Adversaries may exfiltrate ML artifacts or other information relevant to their goals via traditional cyber means. See the ATT&CK Exfiltration tactic for more information. Tactics Exfiltration","title":"Exfiltration via Cyber Means"},{"location":"techniques/AML.T0025.html#exfiltration-via-cyber-means","text":"","title":"Exfiltration via Cyber Means"},{"location":"techniques/AML.T0025.html#description","text":"Adversaries may exfiltrate ML artifacts or other information relevant to their goals via traditional cyber means. See the ATT&CK Exfiltration tactic for more information.","title":"Description"},{"location":"techniques/AML.T0025.html#tactics","text":"Exfiltration","title":"Tactics"},{"location":"techniques/AML.T0029.html","text":"Denial of ML Service Description Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service. Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded. Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system. Tactics Impact","title":"Denial of ML Service"},{"location":"techniques/AML.T0029.html#denial-of-ml-service","text":"","title":"Denial of ML Service"},{"location":"techniques/AML.T0029.html#description","text":"Adversaries may target machine learning systems with a flood of requests for the purpose of degrading or shutting down the service. Since many machine learning systems require significant amounts of specialized compute, they are often expensive bottlenecks that can become overloaded. Adversaries can intentionally craft inputs that require heavy amounts of useless compute from the machine learning system.","title":"Description"},{"location":"techniques/AML.T0029.html#tactics","text":"Impact","title":"Tactics"},{"location":"techniques/AML.T0031.html","text":"Erode ML Model Integrity Description Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time. This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand. Tactics Impact","title":"Erode ML Model Integrity"},{"location":"techniques/AML.T0031.html#erode-ml-model-integrity","text":"","title":"Erode ML Model Integrity"},{"location":"techniques/AML.T0031.html#description","text":"Adversaries may degrade the target model's performance with adversarial data inputs to erode confidence in the system over time. This can lead to the victim organization wasting time and money both attempting to fix the system and performing the tasks it was meant to automate by hand.","title":"Description"},{"location":"techniques/AML.T0031.html#tactics","text":"Impact","title":"Tactics"},{"location":"techniques/AML.T0034.html","text":"Cost Harvesting Description Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization. Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost. Tactics Impact","title":"Cost Harvesting"},{"location":"techniques/AML.T0034.html#cost-harvesting","text":"","title":"Cost Harvesting"},{"location":"techniques/AML.T0034.html#description","text":"Adversaries may target different machine learning services to send useless queries or computationally expensive inputs to increase the cost of running services at the victim organization. Sponge examples are a particular type of adversarial data designed to maximize energy consumption and thus operating cost.","title":"Description"},{"location":"techniques/AML.T0034.html#tactics","text":"Impact","title":"Tactics"},{"location":"techniques/AML.T0035.html","text":"ML Artifact Collection Description Adversaries may collect ML artifacts for Exfiltration or for use in ML Attack Staging . ML artifacts include models and datasets as well as other telemetry data produced when interacting with a model. Tactics Collection","title":"ML Artifact Collection"},{"location":"techniques/AML.T0035.html#ml-artifact-collection","text":"","title":"ML Artifact Collection"},{"location":"techniques/AML.T0035.html#description","text":"Adversaries may collect ML artifacts for Exfiltration or for use in ML Attack Staging . ML artifacts include models and datasets as well as other telemetry data produced when interacting with a model.","title":"Description"},{"location":"techniques/AML.T0035.html#tactics","text":"Collection","title":"Tactics"},{"location":"techniques/AML.T0036.html","text":"Data from Information Repositories Description Adversaries may leverage information repositories to mine valuable information. Information repositories are tools that allow for storage of information, typically to facilitate collaboration or information sharing between users, and can store a wide variety of data that may aid adversaries in further objectives, or direct access to the target information. Information stored in a repository may vary based on the specific instance or environment. Specific common information repositories include Sharepoint, Confluence, and enterprise databases such as SQL Server. Tactics Collection","title":"Data from Information Repositories"},{"location":"techniques/AML.T0036.html#data-from-information-repositories","text":"","title":"Data from Information Repositories"},{"location":"techniques/AML.T0036.html#description","text":"Adversaries may leverage information repositories to mine valuable information. Information repositories are tools that allow for storage of information, typically to facilitate collaboration or information sharing between users, and can store a wide variety of data that may aid adversaries in further objectives, or direct access to the target information. Information stored in a repository may vary based on the specific instance or environment. Specific common information repositories include Sharepoint, Confluence, and enterprise databases such as SQL Server.","title":"Description"},{"location":"techniques/AML.T0036.html#tactics","text":"Collection","title":"Tactics"},{"location":"techniques/AML.T0040.html","text":"ML Model Inference API Access Description Adversaries may gain access to a model via legitimate access to the inference API. Inference API access can be a source of information to the adversary ( Discover ML Model Ontology , Discover ML Model Family ), a means of staging the attack ( Verify Attack , Craft Adversarial Data ), or for introducing data to the target system for Impact ( Evade ML Model , Erode ML Model Integrity ). Tactics ML Model Access","title":"ML Model Inference API Access"},{"location":"techniques/AML.T0040.html#ml-model-inference-api-access","text":"","title":"ML Model Inference API Access"},{"location":"techniques/AML.T0040.html#description","text":"Adversaries may gain access to a model via legitimate access to the inference API. Inference API access can be a source of information to the adversary ( Discover ML Model Ontology , Discover ML Model Family ), a means of staging the attack ( Verify Attack , Craft Adversarial Data ), or for introducing data to the target system for Impact ( Evade ML Model , Erode ML Model Integrity ).","title":"Description"},{"location":"techniques/AML.T0040.html#tactics","text":"ML Model Access","title":"Tactics"},{"location":"techniques/AML.T0041.html","text":"Physical Environment Access Description In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks. If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected. By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access. Tactics ML Model Access","title":"Physical Environment Access"},{"location":"techniques/AML.T0041.html#physical-environment-access","text":"","title":"Physical Environment Access"},{"location":"techniques/AML.T0041.html#description","text":"In addition to the attacks that take place purely in the digital domain, adversaries may also exploit the physical environment for their attacks. If the model is interacting with data collected from the real world in some way, the adversary can influence the model through access to wherever the data is being collected. By modifying the data in the collection process, the adversary can perform modified versions of attacks designed for digital access.","title":"Description"},{"location":"techniques/AML.T0041.html#tactics","text":"ML Model Access","title":"Tactics"},{"location":"techniques/AML.T0042.html","text":"Verify Attack Description Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model. This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing. The adversary may verify the attack once but use it against many edge devices running copies of the target model. The adversary may verify their attack digitally, then deploy it in the Physical Environment Access at a later time. Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model. Tactics ML Attack Staging","title":"Verify Attack"},{"location":"techniques/AML.T0042.html#verify-attack","text":"","title":"Verify Attack"},{"location":"techniques/AML.T0042.html#description","text":"Adversaries can verify the efficacy of their attack via an inference API or access to an offline copy of the target model. This gives the adversary confidence that their approach works and allows them to carry out the attack at a later time of their choosing. The adversary may verify the attack once but use it against many edge devices running copies of the target model. The adversary may verify their attack digitally, then deploy it in the Physical Environment Access at a later time. Verifying the attack may be hard to detect since the adversary can use a minimal number of queries or an offline copy of the model.","title":"Description"},{"location":"techniques/AML.T0042.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0043.000.html","text":"Craft Adversarial Data: White-Box Optimization Description In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly. Adversarial examples trained in this manor are most effective against the target model. Parent technique Craft Adversarial Data Tactics ML Attack Staging","title":"Craft Adversarial Data: White-Box Optimization"},{"location":"techniques/AML.T0043.000.html#craft-adversarial-data-white-box-optimization","text":"","title":"Craft Adversarial Data: White-Box Optimization"},{"location":"techniques/AML.T0043.000.html#description","text":"In White-Box Optimization, the adversary has full access to the target model and optimizes the adversarial example directly. Adversarial examples trained in this manor are most effective against the target model.","title":"Description"},{"location":"techniques/AML.T0043.000.html#parent-technique","text":"Craft Adversarial Data","title":"Parent technique"},{"location":"techniques/AML.T0043.000.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0043.001.html","text":"Craft Adversarial Data: Black-Box Optimization Description In Black-Box attacks, the adversary has black-box (i.e. ML Model Inference API Access via API access) access to the target model. With black-box attacks, the adversary may be using an API that the victim is monitoring. These attacks are generally less effective and require more inferences than White-Box Optimization attacks, but they require much less access. Parent technique Craft Adversarial Data Tactics ML Attack Staging","title":"Craft Adversarial Data: Black-Box Optimization"},{"location":"techniques/AML.T0043.001.html#craft-adversarial-data-black-box-optimization","text":"","title":"Craft Adversarial Data: Black-Box Optimization"},{"location":"techniques/AML.T0043.001.html#description","text":"In Black-Box attacks, the adversary has black-box (i.e. ML Model Inference API Access via API access) access to the target model. With black-box attacks, the adversary may be using an API that the victim is monitoring. These attacks are generally less effective and require more inferences than White-Box Optimization attacks, but they require much less access.","title":"Description"},{"location":"techniques/AML.T0043.001.html#parent-technique","text":"Craft Adversarial Data","title":"Parent technique"},{"location":"techniques/AML.T0043.001.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0043.002.html","text":"Craft Adversarial Data: Black-Box Transfer Description In Black-Box Transfer attacks, the adversary uses one or more proxy models (trained via Create Proxy ML Model or Train Proxy via Replication ) models they have full access to and are representative of the target model. The adversary uses White-Box Optimization on the proxy models to generate adversarial examples. If the set of proxy models are close enough to the target model, the adversarial example should generalize from one to another. This means that an attack that works for the proxy models will likely then work for the target model. If the adversary has ML Model Inference API Access , they may use this Verify Attack that the attack is working and incorporate that information into their training process. Parent technique Craft Adversarial Data Tactics ML Attack Staging","title":"Craft Adversarial Data: Black-Box Transfer"},{"location":"techniques/AML.T0043.002.html#craft-adversarial-data-black-box-transfer","text":"","title":"Craft Adversarial Data: Black-Box Transfer"},{"location":"techniques/AML.T0043.002.html#description","text":"In Black-Box Transfer attacks, the adversary uses one or more proxy models (trained via Create Proxy ML Model or Train Proxy via Replication ) models they have full access to and are representative of the target model. The adversary uses White-Box Optimization on the proxy models to generate adversarial examples. If the set of proxy models are close enough to the target model, the adversarial example should generalize from one to another. This means that an attack that works for the proxy models will likely then work for the target model. If the adversary has ML Model Inference API Access , they may use this Verify Attack that the attack is working and incorporate that information into their training process.","title":"Description"},{"location":"techniques/AML.T0043.002.html#parent-technique","text":"Craft Adversarial Data","title":"Parent technique"},{"location":"techniques/AML.T0043.002.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0043.003.html","text":"Craft Adversarial Data: Manual Modification Description Adversaries may manually modify the input data to craft adversarial data. They may use their knowledge of the target model to modify parts of the data they suspect helps the model in performing its task. The adversary may use trial and error until they are able to verify they have a working adversarial input. Parent technique Craft Adversarial Data Tactics ML Attack Staging","title":"Craft Adversarial Data: Manual Modification"},{"location":"techniques/AML.T0043.003.html#craft-adversarial-data-manual-modification","text":"","title":"Craft Adversarial Data: Manual Modification"},{"location":"techniques/AML.T0043.003.html#description","text":"Adversaries may manually modify the input data to craft adversarial data. They may use their knowledge of the target model to modify parts of the data they suspect helps the model in performing its task. The adversary may use trial and error until they are able to verify they have a working adversarial input.","title":"Description"},{"location":"techniques/AML.T0043.003.html#parent-technique","text":"Craft Adversarial Data","title":"Parent technique"},{"location":"techniques/AML.T0043.003.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0043.004.html","text":"Craft Adversarial Data: Insert Backdoor Trigger Description The adversary may add a perceptual trigger into inference data. The trigger may be imperceptible or non-obvious to humans. This technique is used in conjunction with Poison ML Model and allows the adversary to produce their desired effect in the target model. Parent technique Craft Adversarial Data Tactics ML Attack Staging","title":"Craft Adversarial Data: Insert Backdoor Trigger"},{"location":"techniques/AML.T0043.004.html#craft-adversarial-data-insert-backdoor-trigger","text":"","title":"Craft Adversarial Data: Insert Backdoor Trigger"},{"location":"techniques/AML.T0043.004.html#description","text":"The adversary may add a perceptual trigger into inference data. The trigger may be imperceptible or non-obvious to humans. This technique is used in conjunction with Poison ML Model and allows the adversary to produce their desired effect in the target model.","title":"Description"},{"location":"techniques/AML.T0043.004.html#parent-technique","text":"Craft Adversarial Data","title":"Parent technique"},{"location":"techniques/AML.T0043.004.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0043.html","text":"Craft Adversarial Data Description Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model. Effects can range from misclassification, to missed detections, to maximising energy consumption. Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect. For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class. Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization , Black-Box Optimization , Black-Box Transfer , or Manual Modification . The adversary may Verify Attack their approach works if they have white-box or inference API access to the model. This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed. They can then use the attack at a later time to accomplish their goals. An adversary may optimize adversarial examples for Evade ML Model , or to Erode ML Model Integrity . Tactics ML Attack Staging","title":"Craft Adversarial Data"},{"location":"techniques/AML.T0043.html#craft-adversarial-data","text":"","title":"Craft Adversarial Data"},{"location":"techniques/AML.T0043.html#description","text":"Adversarial data are inputs to a machine learning model that have been modified such that they cause the adversary's desired effect in the target model. Effects can range from misclassification, to missed detections, to maximising energy consumption. Typically, the modification is constrained in magnitude or location so that a human still perceives the data as if it were unmodified, but human perceptibility may not always be a concern depending on the adversary's intended effect. For example, an adversarial input for an image classification task is an image the machine learning model would misclassify, but a human would still recognize as containing the correct class. Depending on the adversary's knowledge of and access to the target model, the adversary may use different classes of algorithms to develop the adversarial example such as White-Box Optimization , Black-Box Optimization , Black-Box Transfer , or Manual Modification . The adversary may Verify Attack their approach works if they have white-box or inference API access to the model. This allows the adversary to gain confidence their attack is effective \"live\" environment where their attack may be noticed. They can then use the attack at a later time to accomplish their goals. An adversary may optimize adversarial examples for Evade ML Model , or to Erode ML Model Integrity .","title":"Description"},{"location":"techniques/AML.T0043.html#tactics","text":"ML Attack Staging","title":"Tactics"},{"location":"techniques/AML.T0044.html","text":"Full ML Model Access Description Adversaries may gain full \"white-box\" access to a machine learning model. This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology. They may exfiltrate the model to Craft Adversarial Data and Verify Attack in an offline where it is hard to detect their behavior. Tactics ML Model Access","title":"Full ML Model Access"},{"location":"techniques/AML.T0044.html#full-ml-model-access","text":"","title":"Full ML Model Access"},{"location":"techniques/AML.T0044.html#description","text":"Adversaries may gain full \"white-box\" access to a machine learning model. This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology. They may exfiltrate the model to Craft Adversarial Data and Verify Attack in an offline where it is hard to detect their behavior.","title":"Description"},{"location":"techniques/AML.T0044.html#tactics","text":"ML Model Access","title":"Tactics"},{"location":"techniques/AML.T0045.html","text":"ML Intellectual Property Theft Description Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization. Proprietary training data is costly to collect and annotate and may be a target for Exfiltration and theft. MLaaS providers charge for use of their API. An adversary who has stolen a model via Exfiltration or via Extract ML Model now has unlimited use of that service without paying the owner of the intellectual property. Tactics Impact","title":"ML Intellectual Property Theft"},{"location":"techniques/AML.T0045.html#ml-intellectual-property-theft","text":"","title":"ML Intellectual Property Theft"},{"location":"techniques/AML.T0045.html#description","text":"Adversaries may exfiltrate ML artifacts to steal intellectual property and cause economic harm to the victim organization. Proprietary training data is costly to collect and annotate and may be a target for Exfiltration and theft. MLaaS providers charge for use of their API. An adversary who has stolen a model via Exfiltration or via Extract ML Model now has unlimited use of that service without paying the owner of the intellectual property.","title":"Description"},{"location":"techniques/AML.T0045.html#tactics","text":"Impact","title":"Tactics"},{"location":"techniques/AML.T0046.html","text":"Spamming ML System with Chaff Data Description Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections. This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences. Tactics Impact","title":"Spamming ML System with Chaff Data"},{"location":"techniques/AML.T0046.html#spamming-ml-system-with-chaff-data","text":"","title":"Spamming ML System with Chaff Data"},{"location":"techniques/AML.T0046.html#description","text":"Adversaries may spam the machine learning system with chaff data that causes increase in the number of detections. This can cause analysts at the victim organization to waste time reviewing and correcting incorrect inferences.","title":"Description"},{"location":"techniques/AML.T0046.html#tactics","text":"Impact","title":"Tactics"},{"location":"techniques/AML.T0047.html","text":"ML-Enabled Product or Service Description Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model. This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata. Tactics ML Model Access","title":"ML-Enabled Product or Service"},{"location":"techniques/AML.T0047.html#ml-enabled-product-or-service","text":"","title":"ML-Enabled Product or Service"},{"location":"techniques/AML.T0047.html#description","text":"Adversaries may use a product or service that uses machine learning under the hood to gain access to the underlying machine learning model. This type of indirect model access may reveal details of the ML model or its inferences in logs or metadata.","title":"Description"},{"location":"techniques/AML.T0047.html#tactics","text":"ML Model Access","title":"Tactics"}]}