
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../images/favicon.ico">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.2.11">
    
    
      
        <title>Case Studies - ATLAS | MITRE</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c5ef100.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.9647289d.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="grey" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#atlas-case-studies" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="ATLAS | MITRE" class="md-header__button md-logo" aria-label="ATLAS | MITRE" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 11c0 5.55-3.84 10.74-9 12-5.16-1.26-9-6.45-9-12V5l9-4 9 4v6m-9 10c3.75-1 7-5.46 7-9.78V6.3l-7-3.12V21Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ATLAS | MITRE
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Case Studies
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="" data-md-color-primary="grey" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_3" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9h-1.9M20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69m-9.15 3.96h2.3L12 9l-1.15 3.65Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="grey" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="grey" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_3">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="ATLAS | MITRE" class="md-nav__button md-logo" aria-label="ATLAS | MITRE" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 11c0 5.55-3.84 10.74-9 12-5.16-1.26-9-6.45-9-12V5l9-4 9 4v6m-9 10c3.75-1 7-5.46 7-9.78V6.3l-7-3.12V21Z"/></svg>

    </a>
    ATLAS | MITRE
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../Tactics/" class="md-nav__link">
        Tactics
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../Techniques/" class="md-nav__link">
        Techniques
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Case Studies
      </a>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


<h1 id="atlas-case-studies">ATLAS Case Studies</h1>
<h1 id="evasion-of-deep-learning-detector-for-malware-cc-traffic">Evasion of Deep Learning Detector for Malware C&amp;C Traffic</h1>
<p>AML.CS0000</p>
<p>Case study type: exercise</p>
<p>Summary: The Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&amp;C) traffic detection in HTTP traffic.
Based on the publicly available <a href="https://arxiv.org/abs/1802.03162">paper by Le et al.</a>, we built a model that was trained on a similar dataset as our production model and had similar performance.
Then we crafted adversarial samples, queried the model, and adjusted the adversarial sample accordingly until the model was evaded.</p>
<p>Date: 2020-01-01</p>
<p>Actor: Palo Alto Networks AI Research Team</p>
<p>Target: Palo Alto Networks malware detection system</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0000.001</td>
<td align="center">We identified a machine learning based approach to malicious URL detection as a representative approach and potential target from the paper <a href="https://arxiv.org/abs/1802.03162">URLNet: Learning a URL representation with deep learning for malicious URL detection</a>, which was found on arXiv (a pre-print repository).</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002.000</td>
<td align="center">We acquired a command and control HTTP traffic  dataset consisting of approximately 33 million benign and 27 million malicious HTTP packet headers.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0005</td>
<td align="center">We trained a model on the HTTP traffic dataset to use as a proxy for the target model.</td>
</tr>
<tr>
<td align="center">Evaluation showed a true positive rate of ~ 99% and false positive rate of ~ 0.01%, on average.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">Testing the model with a HTTP packet header from known malware command and control traffic samples was detected as malicious with high confidence (&gt; 99%).</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.003</td>
<td align="center">We crafted evasion samples by removing fields from packet header which are typically not used for C&amp;C communication (e.g. cache-control, connection, etc.).</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0042</td>
<td align="center">We queried the model with our adversarial examples and adjusted them until the model was evaded.</td>
</tr>
<tr>
<td align="center">AML.TA0007</td>
<td align="center">AML.T0015</td>
<td align="center">With the crafted samples, we performed online evasion of the ML-based spyware detection model.</td>
</tr>
<tr>
<td align="center">The crafted packets were identified as benign with &gt; 80% confidence.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">This evaluation demonstrates that adversaries are able to bypass advanced ML detection techniques, by crafting samples that are misclassified by an ML model.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<h1 id="botnet-domain-generation-algorithm-dga-detection-evasion">Botnet Domain Generation Algorithm (DGA) Detection Evasion</h1>
<p>AML.CS0001</p>
<p>Case study type: exercise</p>
<p>Summary: The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network based botnet Domain Generation Algorithm (DGA) detector using a generic domain name mutation technique.
It is a generic domain mutation technique which can evade most ML-based DGA detection modules.
The generic mutation technique evades most ML-based DGA detection modules DGA and can be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before they is deployed to the production environment.</p>
<p>Date: 2020-01-01</p>
<p>Actor: Palo Alto Networks AI Research Team</p>
<p>Target: Palo Alto Networks ML-based DGA detection module</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0000</td>
<td align="center">DGA detection is a widely used technique to detect botnets in academia and industry.</td>
</tr>
<tr>
<td align="center">The research team searched for research papers related to DGA detection.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002</td>
<td align="center">The researchers acquired a publicly available CNN-based DGA detection model and tested it against a well-known DGA generated domain name data sets, which includes ~50 million domain names from 64 botnet DGA families.</td>
</tr>
<tr>
<td align="center">The CNN-based DGA detection model shows more than 70% detection accuracy on 16 (~25%) botnet DGA families.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0017</td>
<td align="center">The researchers developed a generic mutation technique that requires a minimal number of iterations.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.001</td>
<td align="center">The researchers used the mutation technique to generate evasive domain names.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0042</td>
<td align="center">The experiment results show that the detection rate of all 16 botnet DGA families drop to less than 25% after only one string is inserted once to the DGA generated domain names.</td>
</tr>
<tr>
<td align="center">AML.TA0007</td>
<td align="center">AML.T0015</td>
<td align="center">The DGA generated domain names mutated with this technique successfully evade the target DGA Detection model, allowing an adversary to continue communication with their <a href="https://attack.mitre.org/tactics/TA0011/">Command and Control</a> servers.</td>
</tr>
</tbody>
</table>
<h1 id="virustotal-poisoning">VirusTotal Poisoning</h1>
<p>AML.CS0002</p>
<p>Case study type: incident</p>
<p>Summary: McAfee Advanced Threat Research noticed an increase in reports of a certain ransomware family that was out of the ordinary. Case investigation revealed that many samples of that particular ransomware family were submitted through a popular virus-sharing platform within a short amount of time. Further investigation revealed that based on string similarity the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar. Interestingly enough, the compile time was the same for all the samples. After more digging, researchers discovered that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants. The variants would not always be executable, but are still classified as the same ransomware family.</p>
<p>Date: 2020-01-01</p>
<p>Actor: Unknown</p>
<p>Target: VirusTotal</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0016.000</td>
<td align="center">The actor obtained <a href="https://github.com/a0rtega/metame">metame</a>, a simple metamorphic code engine for arbitrary executables.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043</td>
<td align="center">The actor used a malware sample from a prevalent ransomware family as a start to create "mutant" variants.</td>
</tr>
<tr>
<td align="center">AML.TA0004</td>
<td align="center">AML.T0010.002</td>
<td align="center">The actor uploaded "mutant" samples to the platform.</td>
</tr>
<tr>
<td align="center">AML.TA0006</td>
<td align="center">AML.T0020</td>
<td align="center">Several vendors started to classify the files as the ransomware family even though most of them won't run.</td>
</tr>
<tr>
<td align="center">The "mutant" samples poisoned the dataset the ML model(s) use to identify and classify this ransomware family.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<h1 id="bypassing-cylances-ai-malware-detection">Bypassing Cylance's AI Malware Detection</h1>
<p>AML.CS0003</p>
<p>Case study type: exercise</p>
<p>Summary: Researchers at Skylight were able to create a universal bypass string that evades detection by Cylance's AI Malware detector when appended to a malicious file.</p>
<p>Date: 2019-09-07</p>
<p>Actor: Skylight Cyber</p>
<p>Target: CylancePROTECT, Cylance Smart Antivirus</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0000</td>
<td align="center">The researchers read publicly available information about Cylance's AI Malware detector. They gathered this information from various sources such as public talks as well as patent submissions by Cylance.</td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0047</td>
<td align="center">The researchers used Cylance's AI Malware detector and enabled verbose logging to understand the inner workings of the ML model, particularly around reputation scoring and model ensembling.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0017</td>
<td align="center">The researchers used the reputation scoring information to reverse engineer which attributes provided what level of positive or negative reputation.</td>
</tr>
<tr>
<td align="center">Along the way, they discovered a secondary model which was an override for the first model.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">Positive assessments from the second model overrode the decision of the core ML model.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.003</td>
<td align="center">Using this knowledge, the researchers fused attributes of known good files with malware to manually create adversarial malware.</td>
</tr>
<tr>
<td align="center">AML.TA0007</td>
<td align="center">AML.T0015</td>
<td align="center">Due to the secondary model overriding the primary, the researchers were effectively able to bypass the ML model.</td>
</tr>
</tbody>
</table>
<h1 id="camera-hijack-attack-on-facial-recognition-system">Camera Hijack Attack on Facial Recognition System</h1>
<p>AML.CS0004</p>
<p>Case study type: incident</p>
<p>Summary: This type of camera hijack attack can evade the traditional live facial recognition authentication model and enable access to privileged systems and victim impersonation.</p>
<p>Two individuals in China used this attack to gain access to the local government's tax system. They created a fake shell company and sent invoices via tax system to supposed clients. The individuals started this scheme in 2018 and were able to fraudulently collect $77 million.</p>
<p>Date: 2020-01-01</p>
<p>Actor: Two individuals</p>
<p>Target: Shanghai government tax office's facial recognition service</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0008.001</td>
<td align="center">The attackers bought customized low-end mobile phones.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0016.001</td>
<td align="center">The attackers obtained customized Android ROMs and a virtual camera application.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0016.000</td>
<td align="center">The attackers obtained software that turns static photos into videos, adding realistic effects such as blinking eyes.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0021</td>
<td align="center">The attackers collected user identity information and high definition face photos from an online black market and used the victim's information to register accounts.</td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0047</td>
<td align="center">The attackers used the virtual camera app to present the generated video to the ML-based facial recognition service used for user verification.</td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0015</td>
<td align="center">The attackers successfully evaded the face recognition system. This allowed the attackers to impersonate the victim and verify their their identity in the tax system.</td>
</tr>
</tbody>
</table>
<h1 id="attack-on-machine-translation-service-google-translate-bing-translator-and-systran-translate">Attack on Machine Translation Service - Google Translate, Bing Translator, and Systran Translate</h1>
<p>AML.CS0005</p>
<p>Case study type: exercise</p>
<p>Summary: Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.
A research group at UC Berkeley utilized these public endpoints to create a replicated model with near-production state-of-the-art translation quality.
Beyond demonstrating that IP can be functionally stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.
These adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.</p>
<p>Date: 2020-04-30</p>
<p>Actor: Berkeley Artificial Intelligence Research</p>
<p>Target: Google Translate, Bing Translator, Systran Translate</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0000</td>
<td align="center">The researchers used published research papers to identify the datasets and model architectures used by the target translation services.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002.000</td>
<td align="center">The researchers gathered similar datasets that the target translation services used.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002.001</td>
<td align="center">The researchers gathered similar model architectures that the target translation services used.</td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0040</td>
<td align="center">They abused a public facing application to query the model and produced machine translated sentence pairs as training data.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0005.001</td>
<td align="center">Using these translated sentence pairs, the researchers trained a model that replicates the behavior of the target model.</td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0045</td>
<td align="center">By replicating the model with high fidelity, the researchers demonstrated that an adversary could steal a model and violate the victim's intellectual property rights.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.002</td>
<td align="center">The replicated models were used to generate adversarial examples that successfully transferred to the black-box translation services.</td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0015</td>
<td align="center">The adversarial examples were used to evade the machine translation services by a variety of means. This included targeted word flips, vulgar outputs, and dropped sentences.</td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0031</td>
<td align="center">Adversarial attacks can cause errors that cause reputational damage to the company of the translation service and decrease user trust in AI-powered services.</td>
</tr>
</tbody>
</table>
<h1 id="clearviewai-misconfiguration">ClearviewAI Misconfiguration</h1>
<p>AML.CS0006</p>
<p>Case study type: incident</p>
<p>Summary: Clearview AI makes a facial recognition tool that searches publicly available photos for matches.  This tool has been used for investigative purposes by law enforcement agencies and other parties.</p>
<p>Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account.
This allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens.
With access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model.
These kinds of attacks illustrate that any attempt to secure ML system should be on top of "traditional" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing.</p>
<p>Date: 2020-04-16</p>
<p>Actor: Researchers at spiderSilk</p>
<p>Target: Clearview AI facial recognition tool</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0021</td>
<td align="center">A security researcher gained initial access to Clearview AI's private code repository via a misconfigured server setting that allowed an arbitrary user to register a valid account.</td>
</tr>
<tr>
<td align="center">AML.TA0009</td>
<td align="center">AML.T0036</td>
<td align="center">The private code repository contained credentials which were used to access AWS S3 cloud storage buckets, leading to the discovery of assets for the facial recognition tool, including:</td>
</tr>
<tr>
<td align="center">- Released desktop and mobile applications</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">- Pre-release applications featuring new capabilities</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">- Slack access tokens</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">- Raw videos and other data</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002</td>
<td align="center">Adversaries could have downloaded training data and gleaned details about software, models, and capabilities from the source code and decompiled application binaries.</td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0031</td>
<td align="center">As a result, future application releases could have been compromised, causing degraded or malicious facial recognition capabilities.</td>
</tr>
</tbody>
</table>
<h1 id="gpt-2-model-replication">GPT-2 Model Replication</h1>
<p>AML.CS0007</p>
<p>Case study type: exercise</p>
<p>Summary: OpenAI built GPT-2, a language model capable of generating high quality text samples. Over concerns that GPT-2 could be used for malicious purposes such as impersonating others, or generating misleading news articles, fake social media content, or spam, OpenAI adopted a tiered release schedule. They initially released a smaller, less powerful version of GPT-2 along with a technical description of the approach, but held back the full trained model.</p>
<p>Before the full model was released by OpenAI, researchers at Brown University successfully replicated the model using information released by OpenAI and open source ML artifacts. This demonstrates that a bad actor with sufficient technical skill and compute resources could have replicated GPT-2 and used it for harmful goals before the AI Security community is prepared.</p>
<p>Date: 2019-08-22</p>
<p>Actor: Researchers at Brown University</p>
<p>Target: OpenAI GPT-2</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0000</td>
<td align="center">Using the public documentation about GPT-2, the researchers gathered information about the dataset, model architecture, and training hyper-parameters.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002.001</td>
<td align="center">The researchers obtained a reference implementation of a similar publicly available model called Grover.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002.000</td>
<td align="center">The researchers were able to manually recreate the dataset used in the original GPT-2 paper using the gathered documentation.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0008.000</td>
<td align="center">The researchers were able to use TensorFlow Research Cloud via their academic credentials.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0005.000</td>
<td align="center">The researchers modified Grover's objective function to reflect GPT-2's objective function and then trained on the dataset they curated using used Grover's initial hyperparameters. The resulting model functionally replicates GPT-2, obtaining similar performance on most datasets.</td>
</tr>
<tr>
<td align="center">A bad actor who followed the same procedure as the researchers could then use the replicated GPT-2 model for malicious purposes.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<h1 id="proofpoint-evasion">ProofPoint Evasion</h1>
<p>AML.CS0008</p>
<p>Case study type: exercise</p>
<p>Summary: Proof Pudding (CVE-2019-20634) is a code repository that describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to bypass the live system. More specifically, the insights allowed researchers to craft malicious emails that received preferable scores, going undetected by the system. Each word in an email is scored numerically based on multiple variables and if the overall score of the email is too low, ProofPoint will output an error, labeling it as SPAM.</p>
<p>Date: 2019-09-09</p>
<p>Actor: Researchers at Silent Break Security</p>
<p>Target: ProofPoint Email Protection System</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002</td>
<td align="center">The researchers first gathered the scores from the Proofpoint's ML system used in email headers by sending a large number of emails through the system and scraping the model scores exposed in the logs.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002.000</td>
<td align="center">The researchers converted the collected scores into a dataset. Basic correlation was used to decide which score variable speaks generally about the security of an email. The "mlxlogscore" was selected in this case due to its relationship with spam, phish, and core mlx.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0005</td>
<td align="center">Using these scores, the researchers replicated the ML model by building a "shadow" aka copy-cat ML model. The "mlxlogscore" was the label used for training. Each "mlxlogscore" was generally between 1 and 999 (higher score = safer sample). Training was performed using an Artificial Neural Network (ANN) and Bag of Words tokenizing.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.002</td>
<td align="center">Next, the ML researchers algorithmically found samples from this "offline" copy-cat model that helped give desired insight into its behavior and influential variables.</td>
</tr>
</tbody>
</table>
<p>Examples of good scoring samples include "calculation", "asset", and "tyson".
Examples of bad scoring samples include "software", "99", and "unsub".|
|AML.TA0011|AML.T0015|Finally, these insights from the "offline" copy-cat model allowed the researchers to create malicious emails that received preferable scores from the real ProofPoint email protection system, hence bypassing it.|</p>
<h1 id="tay-poisoning">Tay Poisoning</h1>
<p>AML.CS0009</p>
<p>Case study type: incident</p>
<p>Summary: Microsoft created Tay, a Twitter chatbot designed to engage and entertain users.
While previous chatbots used pre-programmed scripts
to respond to prompts, Tay's machine learning capabilities allowed it to be
directly influenced by its conversations.</p>
<p>A coordinated attack encouraged malicious users to tweet abusive and offensive language at Tay,
which eventually led to Tay generating similarly inflammatory content towards other users.</p>
<p>Microsoft decommissioned Tay within 24 hours of its launch and issued a public apology
with lessons learned from the bot's failure.</p>
<p>Date: 2016-03-23</p>
<p>Actor: 4chan Users</p>
<p>Target: Microsoft's Tay AI Chatbot</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0047</td>
<td align="center">Adversaries were able to interact with Tay via Twitter messages.</td>
</tr>
<tr>
<td align="center">AML.TA0004</td>
<td align="center">AML.T0010.002</td>
<td align="center">Tay bot used the interactions with its Twitter users as training data to improve its conversations.</td>
</tr>
<tr>
<td align="center">Adversaries were able to coordinate with the intent of defacing Tay bot by exploiting this feedback loop.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0006</td>
<td align="center">AML.T0020</td>
<td align="center">By repeatedly interacting with Tay using racist and offensive language, they were able to bias Tay's dataset towards that language as well. This was done by adversaries using the "repeat after me" function, a command that forced Tay to repeat anything said to it.</td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0031</td>
<td align="center">As a result of this coordinated attack, Tay's conversation algorithms began to learn to generate reprehensible material. Tay's internalization of this detestable language caused it to be unpromptedly repeated during interactions with innocent users.</td>
</tr>
</tbody>
</table>
<h1 id="microsoft-azure-service-disruption">Microsoft Azure Service Disruption</h1>
<p>AML.CS0010</p>
<p>Case study type: exercise</p>
<p>Summary: The Microsoft AI Red Team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&amp;CK enterprise techniques such as finding valid account, and exfiltrating data -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.</p>
<p>Date: 2020-01-01</p>
<p>Actor: Microsoft AI Red Team</p>
<p>Target: Internal Microsoft Azure Service</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0000</td>
<td align="center">The team first performed reconnaissance to gather information about the target ML model.</td>
</tr>
<tr>
<td align="center">AML.TA0004</td>
<td align="center">AML.T0012</td>
<td align="center">The team used a valid account to gain access to the network.</td>
</tr>
<tr>
<td align="center">AML.TA0009</td>
<td align="center">AML.T0035</td>
<td align="center">The team found the model file of the target ML model and the necessary training data.</td>
</tr>
<tr>
<td align="center">AML.TA0010</td>
<td align="center">AML.T0025</td>
<td align="center">The team exfiltrated the model and data via traditional means.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.000</td>
<td align="center">Using the target model and data, the red team crafted evasive adversarial data in an offline manor.</td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0040</td>
<td align="center">The team used an exposed API to access the target model.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0042</td>
<td align="center">The team submitted the adversarial examples to the API to verify their efficacy on the production system.</td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0015</td>
<td align="center">The team performed an online evasion attack by replaying the adversarial examples and accomplished their goals.</td>
</tr>
</tbody>
</table>
<h1 id="microsoft-edge-ai-evasion">Microsoft Edge AI Evasion</h1>
<p>AML.CS0011</p>
<p>Case study type: exercise</p>
<p>Summary: The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge. This exercise was meant to use a automated system to continuously manipulate a target image to cause the ML model to produce misclassifications.</p>
<p>Date: 2020-02-01</p>
<p>Actor: Azure Red Team</p>
<p>Target: New Microsoft AI Product</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0000</td>
<td align="center">The team first performed reconnaissance to gather information about the target ML model.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002</td>
<td align="center">The team identified and obtained the publicly available base model to use against the target ML model.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0040</td>
<td align="center">Using the publicly available version of the ML model, the team started sending queries and analyzing the responses (inferences) from the ML model.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.001</td>
<td align="center">The red team created an automated system that continuously manipulated an original target image, that tricked the ML model into producing incorrect inferences, but the perturbations in the image were unnoticeable to the human eye.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0015</td>
<td align="center">Feeding this perturbed image, the red team was able to evade the ML model by causing misclassifications.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<h1 id="face-identification-system-evasion-via-physical-countermeasures">Face Identification System Evasion via Physical Countermeasures</h1>
<p>AML.CS0012</p>
<p>Case study type: exercise</p>
<p>Summary: MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.
This operation had a combination of traditional ATT&amp;CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.</p>
<p>Date: 2020-01-01</p>
<p>Actor: MITRE AI Red Team</p>
<p>Target: Commercial Face Identification Service</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0000</td>
<td align="center">The team first performed reconnaissance to gather information about the target ML model.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0004</td>
<td align="center">AML.T0012</td>
<td align="center">The team gained access to said service via a valid account to gain access and knowledge to api.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0040</td>
<td align="center">The team accessed the inference API of the target model.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0008</td>
<td align="center">AML.T0013</td>
<td align="center">The team identified the list of identities targeted by the model by querying the target model's inference API.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002.000</td>
<td align="center">The team acquired representative open source data.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0005</td>
<td align="center">The team developed a proxy model using the open source data.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.000</td>
<td align="center">Using the proxy model, the red team optimized adversarial visual pattern as a physical domain patch-based attack using expectation over transformation.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0041</td>
<td align="center">The team placed the physical countermeasure from the previous step and placed it in the physical environment to cause issues in the face identification system.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0015</td>
<td align="center">The team successfully evaded the model using the physical countermeasure and causing targeted misclassifications.</td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<h1 id="backdoor-attack-on-deep-learning-models-in-mobile-apps">Backdoor Attack on Deep Learning Models in Mobile Apps</h1>
<p>AML.CS0013</p>
<p>Case study type: exercise</p>
<p>Summary: Deep learning models are increasingly used in mobile applications as critical components.
Researchers from Microsoft Research demonstrated that many deep learning models deployed in mobile apps are vulnerable to backdoor attacks via "neural payload injection."
They conducted an empirical study on real-world mobile deep learning apps collected from Google Play. They identified 54 apps that were vulnerable to attack, including popular security and safety critical applications used for cash recognition, parental control, face authentication, and financial services.</p>
<p>Date: 2021-01-18</p>
<p>Actor: Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, Yunxin Liu</p>
<p>Target: ML-based Android Apps</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0004</td>
<td align="center">To identify a list of potential target models, the researchers searched the Google Play store for apps that may contain embedded deep learning models by searching for deep learning related keywords.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002.001</td>
<td align="center">The researchers acquired the apps' APKs from the Google Play store.</td>
</tr>
<tr>
<td align="center">They filtered the list of potential target applications by searching the code metadata for keywords related to TensorFlow or TFLite and their model binary formats (.tf and .tflite).</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">The models were extracted from the APKs using Apktool.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0044</td>
<td align="center">This provided the researchers with full access to the ML model, albeit in compiled, binary form.</td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0017</td>
<td align="center">The researchers developed a novel approach to insert a backdoor into a compiled model that can be activated with a visual trigger.  They inject a "neural payload" into the model that consists of a trigger detection network and conditional logic.</td>
</tr>
<tr>
<td align="center">The trigger detector is trained to detect a visual trigger that will be placed in the real world.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">The conditional logic allows the researchers to bypass the victim model when the trigger is detected and provide model outputs of their choosing.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">The only requirements for training a trigger detector are a general</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">dataset from the same modality as the target model (e.g. ImageNet for image classification) and several photos of the desired trigger.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0006</td>
<td align="center">AML.T0018.001</td>
<td align="center">The researchers poisoned the victim model by injecting the neural</td>
</tr>
<tr>
<td align="center">payload into the compiled models by directly modifying the computation</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">graph.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">The researchers then repackage the poisoned model back into the APK</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0042</td>
<td align="center">To verify the success of the attack, the researchers confirmed the app did not crash with the malicious model in place, and that the trigger detector successfully detects the trigger.</td>
</tr>
<tr>
<td align="center">AML.TA0004</td>
<td align="center">AML.T0010.003</td>
<td align="center">In practice, the malicious APK would need to be installed on victim's devices via a supply chain compromise.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.004</td>
<td align="center">The trigger is placed in the physical environment, where it is captured by the victim's device camera and processed by the backdoored ML model.</td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0041</td>
<td align="center">At inference time, only physical environment access is required to trigger the attack.</td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0015</td>
<td align="center">Presenting the visual trigger causes the victim model to be bypassed.</td>
</tr>
<tr>
<td align="center">The researchers demonstrated this can be used to evade ML models in</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">several safety-critical apps in the Google Play store.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<h1 id="confusing-antimalware-neural-networks">Confusing Antimalware Neural Networks</h1>
<p>AML.CS0014</p>
<p>Case study type: exercise</p>
<p>Summary: Cloud storage and computations have become popular platforms for deploying ML malware detectors.
In such cases, the features for models are built on users' systems and then sent to cybersecurity company servers.
The Kaspersky ML research team explored this gray-box scenario and showed that feature knowledge is enough for an adversarial attack on ML models.</p>
<p>They attacked one of Kaspersky's antimalware ML models without white-box access to it and successfully evaded detection for most of the adversarially modified malware files.</p>
<p>Date: 2021-06-23</p>
<p>Actor: Kaspersky ML Research Team</p>
<p>Target: Kaspersky's Antimalware ML Model</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0001</td>
<td align="center">The researchers performed a review of adversarial ML attacks on antimalware products.</td>
</tr>
<tr>
<td align="center">They discovered that techniques borrowed from attacks on image classifiers have been successfully applied to the antimalware domain.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">However, it was not clear if these approaches were effective against the ML component of production antimalware solutions.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0002</td>
<td align="center">AML.T0003</td>
<td align="center">Kaspersky's use of ML-based antimalware detectors is publicly documented on their website. In practice, an adversary could use this for targeting.</td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0047</td>
<td align="center">The researches used access to the target ML-based antimalware product throughout this case study.</td>
</tr>
<tr>
<td align="center">This product scans files on the user's system, extracts features locally, then sends them to the cloud-based ML malware detector for classification.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">Therefore, the researchers had only black-box access to the malware detector itself, but could learn valuable information for constructing the attack from the feature extractor.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0002.000</td>
<td align="center">The researchers collected a dataset of malware and clean files.</td>
</tr>
<tr>
<td align="center">They scanned the dataset with the target ML-based antimalware solution and labeled the samples according the ML detector's predictions.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0005</td>
<td align="center">A proxy model was trained on the labeled dataset of malware and clean files.</td>
</tr>
<tr>
<td align="center">The researchers experimented with a variety of model architectures.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0003</td>
<td align="center">AML.T0017</td>
<td align="center">By reverse engineering the local feature extractor, the researchers could collect information about the input features, used for the cloud-based ML detector.</td>
</tr>
<tr>
<td align="center">The model collects PE Header features, section features and section data statistics, and file strings information.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">A gradient based adversarial algorithm for executable files was developed.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">The algorithm manipulates file features to avoid detection by the proxy model, while still containing the same malware payload</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.002</td>
<td align="center">Using a developed gradient-driven algorithm, malicious adversarial files for the proxy model were constructed from the malware files for black-box transfer to the target model.</td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0042</td>
<td align="center">The adversarial malware files were tested against the target antimalware solution to verify their efficacy.</td>
</tr>
<tr>
<td align="center">AML.TA0007</td>
<td align="center">AML.T0015</td>
<td align="center">The researchers demonstrated that for most of the adversarial files, the antimalware model was successfully evaded.</td>
</tr>
<tr>
<td align="center">In practice, an adversary could deploy their adversarially crafted malware and infect systems while evading detection.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>
<h1 id="tesla-auto-wiper-and-enhanced-autopilot-attack">Tesla Auto Wiper and Enhanced Autopilot Attack</h1>
<p>AML.CS0015</p>
<p>Case study type: exercise</p>
<p>Summary: Tesla Auto Wipers and Enhanced Autopilot driving mode both make use of computer
vision machine learning models to determine the vehicle's corresponding functions.
These functions can be exploited by physical adversarial machine learning attacks
that affect the operation and the safety of the vehicle. While exploits to gain
root access to the Tesla firmware had since been patched, the vulnerabilities to
the underlying machine learning systems discovered by this research were still exploitable.</p>
<p>Date: 2019-03-01</p>
<p>Actor: Tencent Keen Security Lab</p>
<p>Target: Tesla Auto Wiper and Enhanced Autopilot</p>
<table>
<thead>
<tr>
<th align="center">Tactic</th>
<th align="center">Technique</th>
<th align="center">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">AML.TA0004</td>
<td align="center">AML.T0012</td>
<td align="center">By having physical access to their Tesla, the researchers executed a root attack chain to uncover internal processes</td>
</tr>
<tr>
<td align="center">of data-flow (camera(s), pre-processing, algorithm(s), post-processing, hardware, etc.).</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0008</td>
<td align="center">AML.T0013</td>
<td align="center">Researchers discovered the neural network architecture of the</td>
</tr>
<tr>
<td align="center">"rain classifier" and the corresponding outputs that predict the probability</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">of moisture on the windshield.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0047</td>
<td align="center">Researchers found that the fish-eye camera is used to capture images</td>
</tr>
<tr>
<td align="center">of the windshield and is the main input to the ML model used to control the wipers.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0040</td>
<td align="center">Images are fed into a neural network after pre-processing and output</td>
</tr>
<tr>
<td align="center">is a float between [0, 1] for probability of moisture.  The neural network</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">is outlined in <code>fisheye.prototxt</code> which was uncovered from remote root attack</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">chain.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.001</td>
<td align="center">Experimented with generating digital adversarial examples based on</td>
</tr>
<tr>
<td align="center">some optimization with the vehicle in the training loop.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0000</td>
<td align="center">AML.T0041</td>
<td align="center">A television display with adversarial images can be placed anywhere</td>
</tr>
<tr>
<td align="center">that the fisheye sensor can capture images.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0042</td>
<td align="center">Experimented with Salt and Pepper noise (gray and RGB) and Worley noise</td>
</tr>
<tr>
<td align="center">for procedural textures to create effective physical adversarial patches that</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">are streamed onto the television display.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0015</td>
<td align="center">Using most of the same techniques as above, researchers were able to</td>
</tr>
<tr>
<td align="center">get the Tesla autopilot to drive into the opposite lane by hallucinating a fake</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">lane. Details in the research report are sparse.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0043.001</td>
<td align="center">Using most of the same techniques as above to gain access to the Tesla,</td>
</tr>
<tr>
<td align="center">researchers generated additional digital adversarial examples to cause the Tesla</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">Enhanced Autopilot system to hallucinate fake lanes.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0001</td>
<td align="center">AML.T0042</td>
<td align="center">Researchers verified the effectiveness of the adversarial attacks by</td>
</tr>
<tr>
<td align="center">placing small white squares as triggers in the road to cause the Tesla to hallucinate</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">lanes. Details in the research report are sparse on this specific attack.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">AML.TA0011</td>
<td align="center">AML.T0015</td>
<td align="center">Researchers were able to get the Tesla autopilot to drive into the</td>
</tr>
<tr>
<td align="center">opposite lane by hallucinating a fake lane.</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../Techniques/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Techniques" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Techniques
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 The MITRE Corporation
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    <a href="https://github.com/mitre-atlas" target="_blank" rel="noopener" title="MITRE | ATLAS GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    <a href="https://join.slack.com/t/mitreatlas/shared_invite/zt-10i6ka9xw-~dc70mXWrlbN9dfFNKyyzQ" target="_blank" rel="noopener" title="MITRE | ATLAS Slack" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>
    </a>
  
    
    
    <a href="mailto:atlas@mitre.org" target="_blank" rel="noopener" title="Email atlas@mitre.org" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2Z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.748e2769.min.js"></script>
      
    
  </body>
</html>